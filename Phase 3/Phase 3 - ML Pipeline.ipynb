{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8b69068-1d55-4f26-aa17-658806e3af64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Phase 3 - End-to-end ML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d67377e-fefd-4547-b38f-5b31d739b18c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Phase 3 ML Pipeline Flow Chart](https://i.imgur.com/3FF1OYx.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b68855d-7ef9-4f61-92b5-65f1f2cc6006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2e0b0d8-0393-4e74-abac-4d4a957c8931",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import gc\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import reduce\n",
    "\n",
    "from pyspark import keyword_only\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, RobustScaler, Imputer\n",
    "from pyspark.ml import PipelineModel, Pipeline, Transformer, PipelineModel, Estimator, Model\n",
    "from pyspark.sql.functions import col, when, isnan, lit, expr, row_number\n",
    "from pyspark.ml.functions import vector_to_array, array_to_vector\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.stat import Correlation\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from pyspark.ml.param.shared import Param, Params, TypeConverters, HasParallelism, HasInputCols, HasOutputCols\n",
    "from pyspark.ml.tuning import CrossValidatorModel, ParamGridBuilder\n",
    "from pyspark.ml.util import MLReadable, MLWritable, DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier, OneVsRest, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "# Set pandas settings\n",
    "pd.set_option('display.max_rows', None)  \n",
    "pd.set_option('display.max_columns', None)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d00f12fb-51e8-4989-8da2-bc5eedc333ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Set Spark Config (for large dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d415eeae-85ab-44af-b719-f23bc3e209cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# These configurations help Spark better handle large datasets\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")  # Enable adaptive query execution\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "# Updated values based on your specific cluster\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")  # Balanced for your core count\n",
    "spark.conf.set(\"spark.default.parallelism\", \"200\")\n",
    "# Additional configuration for shuffle stability\n",
    "spark.conf.set(\"spark.shuffle.io.maxRetries\", \"10\")\n",
    "spark.conf.set(\"spark.shuffle.io.retryWait\", \"30s\")\n",
    "spark.conf.set(\"spark.storage.blockManagerSlaveTimeoutMs\", \"300000\")\n",
    "spark.conf.set(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\")\n",
    "\n",
    "# checkpoint initial cleaned data\n",
    "# Create folder\n",
    "section = \"03\"\n",
    "number = \"01\"\n",
    "folder_path = f\"dbfs:/student-groups/Group_{section}_{number}/pipeline_checkpoints\"\n",
    "spark.sparkContext.setCheckpointDir(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07fc5f02-85f3-4eb8-bc0a-e1a5f0d316b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "385635a2-d5f9-42ee-afb8-ec95484404eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to find missing values\n",
    "def find_missing_values(df):\n",
    "    \"\"\"Take a dataframe and count up the number of missing values in each column as well as the percent of missing values.\n",
    "    Output as a pandas dataframe.\"\"\"\n",
    "    df_len = df.count() # find length of df\n",
    "\n",
    "    # count missing values and convert to pandas df\n",
    "    df_missing = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).toPandas().transpose()\n",
    "\n",
    "    # find percent missing and sort the columns by the percent missing\n",
    "    df_missing = df_missing.rename(columns={0: 'num_missing'})\n",
    "    df_missing['percent_missing'] = df_missing['num_missing'] / df_len * 100\n",
    "    df_missing = df_missing.sort_values('percent_missing', ascending=False)\n",
    "\n",
    "    return df_missing\n",
    "\n",
    "# Helper function to check for null and NaN values\n",
    "def get_null_nan_percentages(df):\n",
    "    \"\"\"Get percentage of null and NaN values in each column of the dataframe\"\"\"\n",
    "    from pyspark.sql.functions import col, sum, when, isnan\n",
    "    from pyspark.sql.types import NumericType\n",
    "\n",
    "    data_len = df.count()\n",
    "\n",
    "    # Identify numeric and non-numeric columns\n",
    "    numeric_cols = [c for c, dtype in df.dtypes if isinstance(df.schema[c].dataType, NumericType)]\n",
    "    string_cols = [c for c in df.columns if c not in numeric_cols]\n",
    "\n",
    "    # Handle NULL values for all columns + NaN values only for numeric columns\n",
    "    null_nan_counts_df = df.select([\n",
    "        sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in string_cols\n",
    "    ] + [\n",
    "        sum(when(col(c).isNull() | isnan(col(c)), 1).otherwise(0)).alias(c) for c in numeric_cols\n",
    "    ])\n",
    "\n",
    "    # Convert result to a Python dictionary\n",
    "    null_nan_counts = null_nan_counts_df.toPandas().to_dict(orient=\"records\")[0]\n",
    "\n",
    "    null_nan_percentage = {key: value / data_len for key, value in null_nan_counts.items()}\n",
    "\n",
    "    return null_nan_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae259840-e927-41b0-aa9c-9ca9a3475173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper method to generate sequential train-test splits from dataset\n",
    "def train_test_split_seq(df):\n",
    "    # Add a sequential row index using a window function\n",
    "    windowSpec = Window.orderBy('YEAR', 'MONTH', 'DAY_OF_MONTH', 'sched_depart_date_time_UTC')\n",
    "\n",
    "    df = df.withColumn(\"row_index\", row_number().over(windowSpec))\n",
    "\n",
    "    # Compute the boundary index for an 80/20 split\n",
    "    train_rows = int(df.count() * 0.8)\n",
    "\n",
    "    # Split the data into training and test sets based on the row index\n",
    "    train_df = df.filter(df.row_index <= train_rows)\n",
    "    test_df = df.filter(df.row_index > train_rows)\n",
    "\n",
    "    # Drop Row Index Column\n",
    "    train_df = train_df.drop(\"row_index\")\n",
    "    test_df = test_df.drop(\"row_index\")\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f8c105b-fab6-432b-bf02-6dc46d659be0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Sampling Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11385fa4-0c83-4398-a7fc-0c26fb830af9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample with time-based windows\n",
    "def sample_with_time_windows(df, sample_size):\n",
    "    # Count total rows\n",
    "    total = df.count()\n",
    "    # Calculate approximately how many rows to take per month\n",
    "    rows_per_month = sample_size / 12  # Assuming data spans 12 months\n",
    "    \n",
    "    # Sample from each month\n",
    "    samples = []\n",
    "    for month in range(1, 13):\n",
    "        month_df = df.filter(F.col(\"MONTH\") == month)\n",
    "        month_count = month_df.count()\n",
    "        if month_count > 0:\n",
    "            fraction = min(1.0, rows_per_month / month_count)\n",
    "            month_sample = month_df.sample(fraction=fraction, seed=42)\n",
    "            samples.append(month_sample)\n",
    "    \n",
    "    # Combine all samples\n",
    "    if samples:\n",
    "        return reduce(DataFrame.unionAll, samples)\n",
    "    return df.limit(sample_size)  # Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "533fdcb4-3673-43f3-8127-da0080d6b5b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SMOTE-like function w/full row preservation\n",
    "def fast_pseudo_smote_preserve_all(df: DataFrame, label_col: str, feature_col: str, target_count: int = None) -> DataFrame:\n",
    "    class_counts = df.groupBy(label_col).count().collect()\n",
    "    class_sizes = {row[label_col]: row['count'] for row in class_counts}\n",
    "    max_size = target_count if target_count else max(class_sizes.values())\n",
    "\n",
    "    balanced_dfs = []\n",
    "\n",
    "    for cls, count in class_sizes.items():\n",
    "        df_cls = df.filter(col(label_col) == cls)\n",
    "        balanced_dfs.append(df_cls)\n",
    "\n",
    "        if count >= max_size:\n",
    "            continue\n",
    "\n",
    "        repeat_factor = int(max_size / count)\n",
    "        remainder = max_size - (repeat_factor * count)\n",
    "\n",
    "        # Repeat original rows\n",
    "        for _ in range(repeat_factor - 1):\n",
    "            balanced_dfs.append(df_cls)\n",
    "\n",
    "        # Add synthetic noise to feature vector\n",
    "        df_array = df_cls.withColumn(\"feature_arr\", vector_to_array(col(feature_col)))\n",
    "        df_noisy = df_array.withColumn(\n",
    "            \"feature_arr\", F.expr(\"transform(feature_arr, x -> x + randn() * 0.01)\")\n",
    "        ).withColumn(\n",
    "            feature_col, array_to_vector(\"feature_arr\")\n",
    "        ).drop(\"feature_arr\").limit(remainder)\n",
    "\n",
    "        balanced_dfs.append(df_noisy)\n",
    "\n",
    "    return reduce(DataFrame.unionByName, balanced_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59206011-c672-4655-9d0c-f33c5f01f203",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def upsampling(df):\n",
    "#     # Filter the DataFrame by each class label for DEP_DELAY_BINNED:\n",
    "#     df_class0 = df.filter(col(\"DEP_DELAY_BINNED\") == 0)\n",
    "#     df_class1 = df.filter(col(\"DEP_DELAY_BINNED\") == 1)\n",
    "#     df_class2 = df.filter(col(\"DEP_DELAY_BINNED\") == 2)\n",
    "\n",
    "#     # Count the number of rows (samples) in each class\n",
    "#     count0 = df_class0.count()\n",
    "#     count1 = df_class1.count()\n",
    "#     count2 = df_class2.count()\n",
    "\n",
    "#     print(\"Counts:\", count0, count1, count2)\n",
    "\n",
    "#     # Define a scaling factor (alpha) that will be applied to the majority class count.\n",
    "#     # Here, we're setting the target count for classes 1 and 2 as 75% of the count of class 0\n",
    "#     alpha = 0.75\n",
    "\n",
    "#     # Calculate the target count based on the scaling factor and class 0 count.\n",
    "#     target_count = alpha * count0\n",
    "#     # Compute sampling fractions for classes 1 and 2 to upsample them to the target_count.\n",
    "#     frac1 = target_count / count1\n",
    "#     frac2 = target_count / count2\n",
    "\n",
    "#     # Perform upsampling using random sampling with replacement for classes 1 and 2.\n",
    "#     # The fraction parameters determine how many times each class will be replicated.\n",
    "#     df_class1_upsampled = df_class1.sample(withReplacement=True, fraction=frac1, seed=42)\n",
    "#     df_class2_upsampled = df_class2.sample(withReplacement=True, fraction=frac2, seed=42)\n",
    "\n",
    "#     # Combine class 0 with the upsampled class 1 and class 2 to form a balanced DataFrame.\n",
    "#     balanced_df = df_class0.unionAll(df_class1_upsampled).unionAll(df_class2_upsampled)\n",
    "\n",
    "#     # Order the resulting DataFrame by time-related columns\n",
    "#     balanced_df = balanced_df.orderBy('YEAR', 'MONTH', 'DAY_OF_MONTH', 'sched_depart_hour_UTC', 'sched_depart_minute_UTC')\n",
    "\n",
    "#     # Group by the DEP_DELAY_BINNED column to show the new distribution of classes.\n",
    "#     balanced_df.groupBy(\"DEP_DELAY_BINNED\").count().show()\n",
    "\n",
    "#     return balanced_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "910768e3-ca87-4212-ba0f-8905de7ec794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upsampling(df):\n",
    "    # Filter the DataFrame by each class label for DEP_DELAY_BINNED:\n",
    "    df_class0 = df.filter(col(\"DEP_DELAY_BINNED\") == 0)\n",
    "    df_class1 = df.filter(col(\"DEP_DELAY_BINNED\") == 1)\n",
    "    df_class2 = df.filter(col(\"DEP_DELAY_BINNED\") == 2)\n",
    "\n",
    "    # Count the number of rows (samples) in each class\n",
    "    count0 = df_class0.count()\n",
    "    count1 = df_class1.count()\n",
    "    count2 = df_class2.count()\n",
    "\n",
    "    print(\"Counts:\", count0, count1, count2)\n",
    "    \n",
    "    # Check if any class has zero samples\n",
    "    if count0 == 0 or count1 == 0 or count2 == 0:\n",
    "        print(\"Warning: One or more classes have zero samples!\")\n",
    "        # Find the class with max count to use as reference\n",
    "        max_class_count = max(count0, count1, count2)\n",
    "        max_class_index = [count0, count1, count2].index(max_class_count)\n",
    "        print(f\"Using class {max_class_index} with {max_class_count} samples as reference\")\n",
    "        \n",
    "        # Define a fallback strategy\n",
    "        balanced_dfs = []\n",
    "        \n",
    "        # Add available classes to the result\n",
    "        if count0 > 0:\n",
    "            balanced_dfs.append(df_class0)\n",
    "        if count1 > 0:\n",
    "            balanced_dfs.append(df_class1)\n",
    "        if count2 > 0:\n",
    "            balanced_dfs.append(df_class2)\n",
    "            \n",
    "        # If there's only one class with data, just return it\n",
    "        if len(balanced_dfs) == 1:\n",
    "            print(\"Only one class has data, returning without upsampling\")\n",
    "            balanced_df = balanced_dfs[0]\n",
    "        else:\n",
    "            # Otherwise, upsample the available classes to match the largest\n",
    "            alpha = 0.75  # Same scaling factor as original\n",
    "            target_count = alpha * max_class_count\n",
    "            \n",
    "            # Upsample each available non-empty class (except the largest)\n",
    "            for i, count in enumerate([count0, count1, count2]):\n",
    "                if i == max_class_index or count == 0:\n",
    "                    continue  # Skip the largest class and empty classes\n",
    "                    \n",
    "                # Get the appropriate class DataFrame\n",
    "                class_df = [df_class0, df_class1, df_class2][i]\n",
    "                \n",
    "                # Calculate fraction for upsampling\n",
    "                frac = target_count / count\n",
    "                \n",
    "                # Upsample and add to balanced_dfs\n",
    "                upsampled_df = class_df.sample(withReplacement=True, fraction=frac, seed=42)\n",
    "                balanced_dfs.append(upsampled_df)\n",
    "            \n",
    "            # Combine all dataframes\n",
    "            balanced_df = reduce(DataFrame.unionAll, balanced_dfs)\n",
    "    else:\n",
    "        # Original logic when all classes have data\n",
    "        alpha = 0.75\n",
    "        target_count = alpha * count0\n",
    "        \n",
    "        # Compute sampling fractions for classes 1 and 2\n",
    "        frac1 = target_count / count1 if count1 > 0 else 0\n",
    "        frac2 = target_count / count2 if count2 > 0 else 0\n",
    "        \n",
    "        # Perform upsampling\n",
    "        df_class1_upsampled = df_class1.sample(withReplacement=True, fraction=frac1, seed=42) if count1 > 0 else df_class1\n",
    "        df_class2_upsampled = df_class2.sample(withReplacement=True, fraction=frac2, seed=42) if count2 > 0 else df_class2\n",
    "        \n",
    "        # Combine all classes\n",
    "        balanced_df = df_class0.unionAll(df_class1_upsampled).unionAll(df_class2_upsampled)\n",
    "\n",
    "    # Order the resulting DataFrame by time-related columns\n",
    "    balanced_df = balanced_df.orderBy('YEAR', 'MONTH', 'DAY_OF_MONTH', 'sched_depart_hour_UTC', 'sched_depart_minute_UTC')\n",
    "\n",
    "    # Group by the DEP_DELAY_BINNED column to show the new distribution of classes\n",
    "    balanced_df.groupBy(\"DEP_DELAY_BINNED\").count().show()\n",
    "\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b82627-7be5-43c1-882d-cde6f9ec9bd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def downsampling(df):\n",
    "    # Filter the DataFrame by each class label\n",
    "    df_class0 = df.filter(col(\"DEP_DELAY_BINNED\") == 0)\n",
    "    df_class1 = df.filter(col(\"DEP_DELAY_BINNED\") == 1)\n",
    "    df_class2 = df.filter(col(\"DEP_DELAY_BINNED\") == 2)\n",
    "    \n",
    "    # Count the number of instances for each class\n",
    "    count0 = df_class0.count()\n",
    "    count1 = df_class1.count()\n",
    "    count2 = df_class2.count()\n",
    "    \n",
    "    print(\"Counts:\", count0, count1, count2)\n",
    "    \n",
    "    # Determine the target count as the minimum number of instances among all classes\n",
    "    target_count = min(count0, count1, count2)\n",
    "    \n",
    "    # Compute sampling fractions for each class based on the target count\n",
    "    frac0 = target_count / count0 if count0 > target_count else 1.0\n",
    "    frac1 = target_count / count1 if count1 > target_count else 1.0\n",
    "    frac2 = target_count / count2 if count2 > target_count else 1.0\n",
    "    \n",
    "    # Downsample each class (without replacement) to match the target count\n",
    "    df_class0_sampled = df_class0.sample(withReplacement=False, fraction=frac0, seed=42)\n",
    "    df_class1_sampled = df_class1.sample(withReplacement=False, fraction=frac1, seed=42)\n",
    "    df_class2_sampled = df_class2.sample(withReplacement=False, fraction=frac2, seed=42)\n",
    "    \n",
    "    # Combine the downsampled classes to form a balanced DataFrame\n",
    "    balanced_df = df_class0_sampled.unionAll(df_class1_sampled).unionAll(df_class2_sampled)\n",
    "    \n",
    "    # Sort the resulting DataFrame by time attributes \n",
    "    balanced_df = balanced_df.orderBy('YEAR', 'MONTH', 'DAY_OF_MONTH', 'sched_depart_hour_UTC', 'sched_depart_minute_UTC')\n",
    "    \n",
    "    # Show the new class counts\n",
    "    balanced_df.groupBy(\"DEP_DELAY_BINNED\").count().show()\n",
    "    \n",
    "    return balanced_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1084153e-94a3-422e-b473-3ac53edda9c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Graph feature helper method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2502962-dde7-47c4-af73-1e2af2de65b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from graphframes import GraphFrame\n",
    "def graph_based_feature(df):\n",
    "    # calculate the number of flights that occur from one airport to another (this will be the weight of the edges)\n",
    "    flight_freq = df.groupBy(col('ORIGIN_AIRPORT_ID').alias('src'),col('DEST_AIRPORT_ID').alias('dst')).count().withColumnRenamed('count','flight_freq')\n",
    "\n",
    "    # create the vertices based on airport ids for origin and destination, respectively\n",
    "    vertices_origin = df.select(col('ORIGIN_AIRPORT_ID').alias('id'))\n",
    "    vertices_dest = df.select(col('DEST_AIRPORT_ID').alias('id'))\n",
    "\n",
    "    # union the two dataframes to have one list of all nodes (vertices)\n",
    "    vertices = vertices_origin.union(vertices_dest).dropDuplicates()\n",
    "\n",
    "    # edges will be the connection between the origin and destination airports weighted by how frequently they're flown\n",
    "    edges = df.select(col('ORIGIN_AIRPORT_ID').alias('src'), col('DEST_AIRPORT_ID').alias('dst'))\n",
    "    edges = edges.join(flight_freq, ['src','dst'])\n",
    "\n",
    "    # create graph and calculate pagerank (tol=1e-6 will stop the program when the ranks don't change by more than 1e-6)\n",
    "    g = GraphFrame(vertices, edges)\n",
    "    results = g.pageRank(tol=1e-3)\n",
    "\n",
    "    # get just the airport ids and their corresponding pageranks\n",
    "    airport_pr = results.vertices.select(col('id'), col('pagerank'))\n",
    "\n",
    "    # join the pageranks with the original dataframe and do it for both the origin and destinations airports\n",
    "    df_graph = df.join(airport_pr, df.ORIGIN_AIRPORT_ID == airport_pr.id).drop('id').withColumnRenamed('pagerank','origin_airport_pagerank')\n",
    "    df_graph = df_graph.join(airport_pr, df_graph.DEST_AIRPORT_ID == airport_pr.id).drop('id').withColumnRenamed('pagerank','dest_airport_pagerank')\n",
    "\n",
    "    return df_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "371e0d74-58db-4586-8195-1c62ae9d7566",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Partitioning/Checkpointing Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a770376e-924e-4944-b39d-93b2cdbd4a0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_balanced_time_ordered_partitions(df, num_partitions=200):\n",
    "    \"\"\"\n",
    "    Creates evenly sized partitions while maintaining chronological order.\n",
    "    \n",
    "    This approach:\n",
    "    1. Computes row numbers based on time ordering\n",
    "    2. Creates a synthetic partition key that distributes rows evenly\n",
    "    3. Repartitions by this key to get balanced partitions\n",
    "    4. Sorts within each partition to maintain overall ordering\n",
    "    \n",
    "    Returns: A DataFrame with balanced partitions in chronological order\n",
    "    \"\"\"\n",
    "    if 'sched_depart_date_time_UTC' in df.columns:\n",
    "        sort_list = ['YEAR', 'MONTH', 'DAY_OF_MONTH', 'sched_depart_date_time_UTC']\n",
    "    else:\n",
    "        sort_list = ['YEAR', 'MONTH', 'DAY_OF_MONTH', 'sched_depart_hour_UTC', 'sched_depart_minute_UTC']\n",
    "\n",
    "    # First, add a monotonically increasing ID based on time ordering\n",
    "    windowSpec = Window.orderBy(*sort_list)\n",
    "    # `sched_depart_minute_UTC`, `sched_depart_hour_UTC`\n",
    "    df_indexed = df.withColumn(\"row_num\", F.row_number().over(windowSpec))\n",
    "    \n",
    "    # Get the total count to calculate partition sizes\n",
    "    total_rows = df_indexed.count()\n",
    "    rows_per_partition = total_rows / num_partitions\n",
    "    \n",
    "    # Create a synthetic partition key that distributes rows evenly\n",
    "    df_partitioned = df_indexed.withColumn(\n",
    "        \"partition_id\", \n",
    "        F.floor(F.col(\"row_num\") / F.lit(rows_per_partition))\n",
    "    )\n",
    "    \n",
    "    # Repartition by this synthetic key to get balanced partitions\n",
    "    df_balanced = df_partitioned.repartition(num_partitions, \"partition_id\")\n",
    "    # Sort within each partition to maintain chronological order\n",
    "    df_balanced = df_balanced.sortWithinPartitions(*sort_list)\n",
    "    # Drop the temporary columns\n",
    "    df_balanced = df_balanced.drop(\"row_num\", \"partition_id\")\n",
    "    \n",
    "    return df_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4798ec8-258b-4a6a-b83b-3da6a44f1b4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def checkpoint_save(df, name, overwrite=True, clean_previous=True):\n",
    "    \"\"\"Save DataFrame to checkpoint location with standardized naming and cleanup\"\"\"\n",
    "    checkpoint_path = f\"{folder_path}/{name}\"\n",
    "    \n",
    "    # First check if we need to clean up previous checkpoint with same name\n",
    "    if clean_previous and spark._jsparkSession.catalog().tableExists(name):\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {name}\")\n",
    "        \n",
    "    # Force materialization before writing to ensure computation happens now\n",
    "    count = df.count()\n",
    "    print(f\"Checkpointing {count} rows to {name}\")\n",
    "    \n",
    "    # Repartition to balance data and write as table for efficient retrieval\n",
    "    # Use coalesce if the dataset has become smaller due to filtering\n",
    "    if df.rdd.getNumPartitions() > 200 and count < 10000000:\n",
    "        df = df.coalesce(200)\n",
    "    else:\n",
    "        df = df.repartition(200)\n",
    "        \n",
    "    df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(name)\n",
    "    \n",
    "    # Explicitly unpersist and run garbage collection\n",
    "    df.unpersist(blocking=True)\n",
    "    gc.collect()\n",
    "    \n",
    "    return checkpoint_path\n",
    "\n",
    "def checkpoint_load(name):\n",
    "    \"\"\"Load DataFrame from checkpoint location\"\"\"\n",
    "    print(f\"Loading checkpoint: {name}\")\n",
    "    return spark.table(name)\n",
    "\n",
    "def cleanup_checkpoints():\n",
    "    \"\"\"Delete all checkpoint tables created during processing\"\"\"\n",
    "    tables = [table.name for table in spark.catalog.listTables() \n",
    "              if table.name.startswith(\"checkpoint_\")]\n",
    "    \n",
    "    for table in tables:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {table}\")\n",
    "    \n",
    "    print(f\"Cleaned up {len(tables)} checkpoint tables\")\n",
    "    \n",
    "    # Also remove physical checkpoint directory\n",
    "    dbutils.fs.rm(folder_path, recurse=True)\n",
    "\n",
    "def clear_cache():\n",
    "    \"\"\"Clear all cached DataFrames and run garbage collection\"\"\"\n",
    "    # Clear Spark cache\n",
    "    spark.catalog.clearCache()\n",
    "    \n",
    "    # Force Python garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"Cache cleared and garbage collection performed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bb2212e-4737-46c3-9971-b1579d2f3cb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Custom Transformers (for feature engineering and imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd1de415-0982-421c-a422-c8084d3c8e04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# custom transformer for preprocessing/cleaning data\n",
    "class DataPreprocessor(Transformer, HasInputCols, HasOutputCols, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    \"\"\"\n",
    "    Custom transformer to preprocess flight delay data before feature engineering\n",
    "    \"\"\"\n",
    "    def __init__(self, inputCols=None, outputCols=None):\n",
    "        super(DataPreprocessor, self).__init__()\n",
    "        self._setDefault(inputCols=[], outputCols=[])\n",
    "        self.setParams(inputCols, outputCols)\n",
    "    \n",
    "    def setParams(self, inputCols=None, outputCols=None):\n",
    "        if inputCols is not None:\n",
    "            self.setInputCols(inputCols)\n",
    "        if outputCols is not None:\n",
    "            self.setOutputCols(outputCols)\n",
    "        return self\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        \"\"\"\n",
    "        Transforms the input DataFrame by applying preprocessing steps\n",
    "        \"\"\"\n",
    "        print('Running DataPreprocessor...')\n",
    "        df = dataset\n",
    "        print(f'Preprocessing {df.count()} rows')\n",
    "        # Drop records with missing target variable\n",
    "        df = df.dropna(subset=[\"DEP_DELAY\"])\n",
    "        \n",
    "        # Remove columns with high missing values (>30%)\n",
    "        # In a transformer, we would typically pre-calculate this or pass a parameter\n",
    "        # For simplicity, let's include only the columns to delete directly\n",
    "        cols_to_delete = self._get_high_missing_cols(df)\n",
    "        if cols_to_delete:\n",
    "            df = df.drop(*cols_to_delete)\n",
    "        \n",
    "        # Remove redundant features\n",
    "        redundant_features = ['ORIGIN', 'OP_CARRIER', 'ORIGIN_AIRPORT_SEQ_ID', 'ORIGIN_CITY_MARKET_ID',\n",
    "                             'ORIGIN_CITY_NAME', 'ORIGIN_STATE_NM', 'ORIGIN_WAC', 'ORIGIN_STATE_FIPS',\n",
    "                             'DEST_AIRPORT_SEQ_ID', 'DEST_CITY_MARKET_ID', 'DEST', 'DEST_CITY_NAME',\n",
    "                             'DEST_STATE_FIPS', 'DEST_STATE_NM', 'DEST_WAC', 'DEP_TIME', 'CRS_DEP_TIME',\n",
    "                             'DEP_DELAY_NEW', 'DEP_DELAY_GROUP', 'DEP_TIME_BLK', 'DIVERTED',\n",
    "                             'origin_airport_name', 'origin_station_name', 'origin_iata_code',\n",
    "                             'origin_icao', 'origin_region', 'origin_station_lon', 'origin_airport_lat',\n",
    "                             'origin_station_lat', 'origin_airport_lon', 'dest_airport_name',\n",
    "                             'dest_station_name', 'dest_iata_code', 'dest_icao', 'dest_region',\n",
    "                             'dest_station_lat', 'dest_station_lon', 'dest_airport_lat',\n",
    "                             'dest_airport_lon', 'sched_depart_date_time', 'NAME', 'REM',\n",
    "                             'BackupEquipment', '_row_desc', 'FL_DATE', 'REPORT_TYPE', 'SOURCE']\n",
    "        \n",
    "        existing_redundant = [col for col in redundant_features if col in df.columns]\n",
    "        if existing_redundant:\n",
    "            df = df.drop(*existing_redundant)\n",
    "        \n",
    "        # Remove unusable features (post-departure)\n",
    "        post_departure_features = [\"TAXI_OUT\", \"WHEELS_OFF\", \"WHEELS_ON\", \"TAXI_IN\", \n",
    "                                  \"CRS_ARR_TIME\", \"ARR_TIME\", \"ARR_DELAY\", \"ARR_DELAY_NEW\", \n",
    "                                  \"ARR_DEL15\", \"ARR_DELAY_GROUP\", \"ARR_TIME_BLK\", \"CANCELLED\", \n",
    "                                  \"CRS_ELAPSED_TIME\", \"ACTUAL_ELAPSED_TIME\", \"AIR_TIME\", \n",
    "                                  \"CARRIER_DELAY\", \"WEATHER_DELAY\", \"LATE_AIRCRAFT_DELAY\", \n",
    "                                  \"WEATHER_DELAY\", \"NAS_DELAY\", \"CARRIER_DELAY\", \"SECURITY_DELAY\", \n",
    "                                  \"TAIL_NUMBER\", \"FLIGHTS\"]\n",
    "        \n",
    "        existing_post_departure = [col for col in post_departure_features if col in df.columns]\n",
    "        if existing_post_departure:\n",
    "            df = df.drop(*existing_post_departure)\n",
    "        \n",
    "        # Convert data types\n",
    "        int_cols = ['QUARTER', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'OP_CARRIER_FL_NUM', \n",
    "                    'DEP_DEL15', 'YEAR', 'MONTH', 'OP_CARRIER_FL_NUM']\n",
    "        float_cols = ['DEP_DELAY', 'DISTANCE_GROUP', 'DISTANCE', 'origin_station_dis',\n",
    "                     'dest_station_dis', 'LATITUDE', 'LONGITUDE', 'ELEVATION',\n",
    "                     'HourlyAltimeterSetting', 'HourlyDewPointTemperature',\n",
    "                     'HourlyDryBulbTemperature', 'HourlyPrecipitation', \n",
    "                     'HourlyRelativeHumidity', 'HourlySeaLevelPressure',\n",
    "                     'HourlyStationPressure', 'HourlyVisibility',\n",
    "                     'HourlyWetBulbTemperature', 'HourlyWindDirection', 'HourlyWindSpeed']\n",
    "        \n",
    "        for col_name in int_cols:\n",
    "            if col_name in df.columns:\n",
    "                df = df.withColumn(col_name, F.col(col_name).cast(\"int\"))\n",
    "        \n",
    "        for col_name in float_cols:\n",
    "            if col_name in df.columns:\n",
    "                df = df.withColumn(col_name, F.col(col_name).cast(\"float\"))\n",
    "        \n",
    "        # Remove duplicate records\n",
    "        df = df.dropDuplicates()\n",
    "        \n",
    "        # Remove multicollinear features\n",
    "        colinear_cols = ['HourlyStationPressure', 'DISTANCE_GROUP', \n",
    "                         'HourlySeaLevelPressure', 'HourlyWetBulbTemperature',\n",
    "                         'HourlyDryBulbTemperature']\n",
    "        \n",
    "        existing_colinear = [col for col in colinear_cols if col in df.columns]\n",
    "        if existing_colinear:\n",
    "            df = df.drop(*existing_colinear)\n",
    "        \n",
    "        df = df.orderBy('YEAR', 'MONTH', 'DAY_OF_MONTH', 'sched_depart_date_time_UTC')\n",
    "        df = df.repartition(180)  # Adjust for data volume after preprocessing\n",
    "        checkpoint_save(df, \"checkpoint_data_preprocessed\")\n",
    "        return checkpoint_load(\"checkpoint_data_preprocessed\")\n",
    "    \n",
    "    def _get_high_missing_cols(self, df, threshold=30):\n",
    "        \"\"\"\n",
    "        Find columns with missing values above the threshold percentage\n",
    "        \"\"\"\n",
    "        # This would be better pre-calculated for a real transformer\n",
    "        # But we'll implement it for completeness\n",
    "        total_count = df.count()\n",
    "        if total_count == 0:\n",
    "            return []\n",
    "            \n",
    "        missing_counts = []\n",
    "        for col in df.columns:\n",
    "            null_count = df.where(F.col(col).isNull()).count()\n",
    "            missing_percent = (null_count / total_count) * 100\n",
    "            if missing_percent > threshold:\n",
    "                missing_counts.append(col)\n",
    "                \n",
    "        return missing_counts\n",
    "\n",
    "# Custom Transformer for datetime processing\n",
    "class DateTimeProcessor(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self):\n",
    "        super(DateTimeProcessor, self).__init__()\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        \"\"\"Process datetime columns to extract hour and minute features\"\"\"\n",
    "        print('Running DateTimeProcessor...')\n",
    "\n",
    "        datetime_columns = ['sched_depart_date_time_UTC', 'four_hours_prior_depart_UTC', \n",
    "                            'two_hours_prior_depart_UTC', 'DATE']\n",
    "        \n",
    "        result = dataset\n",
    "        for col_ in datetime_columns:\n",
    "            if col_ in dataset.columns:\n",
    "                result = result.withColumn(col_, F.to_timestamp(col_, \"yyyy-MM-dd'T'HH:mm:ss\"))\n",
    "        \n",
    "        # Extract hour and minute components\n",
    "        if 'sched_depart_date_time_UTC' in dataset.columns:\n",
    "            result = result.withColumn('sched_depart_hour_UTC', F.hour('sched_depart_date_time_UTC')) \\\n",
    "                           .withColumn('sched_depart_minute_UTC', F.minute('sched_depart_date_time_UTC'))\n",
    "        \n",
    "        if 'four_hours_prior_depart_UTC' in dataset.columns:\n",
    "            result = result.withColumn('four_hours_prior_depart_hour_UTC', F.hour('four_hours_prior_depart_UTC')) \\\n",
    "                           .withColumn('four_hours_prior_depart_minute_UTC', F.minute('four_hours_prior_depart_UTC'))\n",
    "        \n",
    "        if 'two_hours_prior_depart_UTC' in dataset.columns:\n",
    "            result = result.withColumn('two_hours_prior_depart_hour_UTC', F.hour('two_hours_prior_depart_UTC')) \\\n",
    "                           .withColumn('two_hours_prior_depart_minute_UTC', F.minute('two_hours_prior_depart_UTC'))\n",
    "        \n",
    "        if 'DATE' in dataset.columns:\n",
    "            result = result.withColumn('station_hour_UTC', F.hour('DATE')) \\\n",
    "                           .withColumn('station_minute_UTC', F.minute('DATE'))\n",
    "        \n",
    "        # Drop old datetime columns\n",
    "        result = result.drop(*datetime_columns)\n",
    "\n",
    "        return result\n",
    "\n",
    "# Custom Transformer for SkyConditions processing\n",
    "class SkyConditionsProcessor(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self):\n",
    "        super(SkyConditionsProcessor, self).__init__()\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        \"\"\"Process HourlySkyConditions column to extract cloud data\"\"\"\n",
    "        \n",
    "        print('Running SkyConditionsProcessor...')\n",
    "        if 'HourlySkyConditions' not in dataset.columns:\n",
    "            return dataset\n",
    "        \n",
    "        result = dataset.withColumn(\"split_values\", F.split(col(\"HourlySkyConditions\"), \" \")) \\\n",
    "                       .withColumn(\"HourlyCloudState\", F.expr(\"slice(split_values, greatest(size(split_values)-1, 1), 2)\")) \\\n",
    "                       .withColumn(\"HourlyCloudCoverage\", F.split(col(\"HourlyCloudState\")[0], \":\")[0]) \\\n",
    "                       .withColumn(\"HourlyCloudLayerAmount\", F.split(col(\"HourlyCloudState\")[0], \":\")[1].cast(\"int\")) \\\n",
    "                       .withColumn(\"HourlyCloudBaseHeight\", col(\"HourlyCloudState\")[1].cast(\"float\")*100) \\\n",
    "                       .drop(\"split_values\", \"HourlyCloudState\", \"HourlySkyConditions\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Custom Transformer for dropping outliers processing for pre-chosen columns\n",
    "class OutlierHandler(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self):\n",
    "        super(OutlierHandler, self).__init__()\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        \"\"\"Process pre-chosen columns to drop outliers\"\"\"\n",
    "        print('Running OutlierHandler...')\n",
    "        df = dataset\n",
    "\n",
    "        preserve_outlier_list = ['DEP_DELAY', 'origin_station_dis', 'dest_station_dis', 'HourlyPrecipitation', 'HourlyVisibility']\n",
    "\n",
    "        float_cols_outliers = [col for col, dtype in df.dtypes if dtype in ('float') and col not in preserve_outlier_list]\n",
    "\n",
    "        df_quantiles = df.approxQuantile(float_cols_outliers, [.25, .75], 0.01)\n",
    "        df_IQR = [df_quantiles[i][1]- df_quantiles[i][0] for i in range(len(df_quantiles))]\n",
    "        high_outlier = [df_quantiles[i][1] + 3*df_IQR[i] for i in range(len(df_quantiles))]\n",
    "        low_outlier = [df_quantiles[i][0] - 3*df_IQR[i] for i in range(len(df_quantiles))]\n",
    "\n",
    "        prev_size = df.count()\n",
    "        for i, column in enumerate(float_cols_outliers):\n",
    "            df = df.filter((df[column] >= low_outlier[i]) & (df[column] <= high_outlier[i]))\n",
    "            new_size = df.count()\n",
    "            print(f\"The current column is {column}, the number of rows dropped is {prev_size - new_size}, the current number of rows is {new_size}\")\n",
    "            prev_size = new_size\n",
    "            \n",
    "        return df\n",
    "\n",
    "# Custom Transformer for handling missing values\n",
    "class MissingValueHandler(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self):\n",
    "        super(MissingValueHandler, self).__init__()\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        \"\"\"Handle missing values in numerical and categorical columns\"\"\"\n",
    "        print('Running MissingValueHandler...')\n",
    "        result = dataset\n",
    "        \n",
    "        # Handle precipitation data - replace nulls with 0\n",
    "        if 'HourlyPrecipitation' in result.columns:\n",
    "            result = result.withColumn(\n",
    "                \"HourlyPrecipitation\",\n",
    "                F.when(F.col(\"HourlyPrecipitation\").isNull(), 0.0)\n",
    "                .otherwise(F.col(\"HourlyPrecipitation\"))\n",
    "            )\n",
    "\n",
    "        # Impute numerical columns with median\n",
    "        preserve_outlier_list = ['DEP_DELAY', 'origin_station_dis', 'dest_station_dis', 'HourlyPrecipitation', 'HourlyVisibility']\n",
    "\n",
    "        float_cols_outliers = [col for col, dtype in result.dtypes if dtype in ('float') and col not in preserve_outlier_list]\n",
    "\n",
    "        df_missing_vals = find_missing_values(result)\n",
    "\n",
    "        float_cols_missing = list(set(df_missing_vals[df_missing_vals[\"percent_missing\"] > 0].index).intersection(set(float_cols_outliers)) - set({\"DEP_DELAY\"}))\n",
    "\n",
    "        if float_cols_missing:\n",
    "            imputer = Imputer(strategy=\"median\", inputCols=float_cols_missing, outputCols=float_cols_missing)\n",
    "\n",
    "            imputer_model = imputer.fit(result)\n",
    "            \n",
    "            result = imputer_model.transform(result)\n",
    "\n",
    "        # Fill missing categorical columns with 'missing'\n",
    "        missing_cat_cols = ['HourlyCloudLayerAmount', 'HourlyCloudCoverage', 'TAIL_NUM']\n",
    "        result = result.fillna('missing', subset='TAIL_NUM')\n",
    "        # for col_name in missing_cat_cols:\n",
    "        #     if col_name in result.columns:\n",
    "        #         result = result.withColumn(\n",
    "        #             col_name,\n",
    "        #             F.when(F.col(col_name).isNull(), 'missing')\n",
    "        #             .otherwise(F.col(col_name))\n",
    "        #         )\n",
    "        \n",
    "        result = result.dropna()\n",
    "\n",
    "        return result\n",
    "\n",
    "# Custom Transformer for creating target bins\n",
    "class TargetBinner(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self):\n",
    "        super(TargetBinner, self).__init__()\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        \"\"\"Create delay bins from DEP_DELAY\"\"\"\n",
    "        print('Running TargetBinner...')\n",
    "        if 'DEP_DELAY' in dataset.columns:\n",
    "            return dataset.withColumn(\"DEP_DELAY_BINNED\",\n",
    "                F.when(F.col(\"DEP_DELAY\") < 0, 0)\n",
    "                .when((F.col(\"DEP_DELAY\") >= 0) & (F.col(\"DEP_DELAY\") <= 15), 1)\n",
    "                .when(F.col(\"DEP_DELAY\") > 15, 2)\n",
    "            )\n",
    "        return dataset\n",
    "\n",
    "# Custom Transformer for smote resampling\n",
    "class GraphFeatureProcessor(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self):\n",
    "        super(GraphFeatureProcessor, self).__init__()\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        print('Running GraphFeatureProcessor...')\n",
    "        df_final = graph_based_feature(dataset)\n",
    "        return df_final\n",
    "\n",
    "# # Custom Transformer for creating target encodings of high-cardinality categorical columns\n",
    "# class TargetEncoder(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "#     def __init__(self):\n",
    "#         super(TargetEncoder, self).__init__()\n",
    "    \n",
    "#     def _transform(self, dataset):\n",
    "#         \"\"\"Create target encodings for pre-chosen columns\"\"\"\n",
    "#         print('Running TargetEncoder...')\n",
    "#         cat_cols_to_target_encode = ['TAIL_NUM', 'OP_CARRIER_FL_NUM', 'ORIGIN_AIRPORT_ID', 'DEST_AIRPORT_ID', 'origin_station_id', 'dest_station_id', 'STATION']\n",
    "\n",
    "#         # remaining_cat_columns = ['OP_UNIQUE_CARRIER',\n",
    "#         #     'OP_CARRIER_AIRLINE_ID',\n",
    "#         #     'ORIGIN_STATE_ABR',\n",
    "#         #     'DEST_STATE_ABR',\n",
    "#         #     'YEAR',\n",
    "#         #     'origin_type',\n",
    "#         #     'dest_type',\n",
    "#         #     'HourlyCloudCoverage',\n",
    "#         #     'HourlyCloudLayerAmount']\n",
    "        \n",
    "#         targ_enc_col_names = []\n",
    "#         DEP_DELAY_mean = dataset.agg(F.mean(\"DEP_DELAY\")).collect()[0][0]\n",
    "#         for i, col in enumerate(cat_cols_to_target_encode):\n",
    "#             # Compute the Mean of DEP_DELAY per Category\n",
    "#             train_mean_df = dataset.groupBy(col).agg(F.mean(\"DEP_DELAY\").alias(f\"{col}_mean_delay\")).orderBy(F.desc(f\"{col}_mean_delay\"))\n",
    "#             # Accumulate New Column Names\n",
    "#             targ_enc_col_names.append(f\"{col}_mean_delay\")\n",
    "#             # Join the Aggregated Means to the DataFrames\n",
    "#             if i == 0:\n",
    "#                 dataset_joined = dataset.join(train_mean_df, on=col, how=\"left\")\n",
    "#             else:\n",
    "#                 dataset_joined = dataset_joined.join(train_mean_df, on=col, how=\"left\")\n",
    "            \n",
    "#             # dataset_joined = dataset_joined.drop(col)\n",
    "\n",
    "#         # Fill NA's with mean\n",
    "#         dataset_joined = dataset_joined.fillna(value=DEP_DELAY_mean, subset=targ_enc_col_names)\n",
    "\n",
    "#         return dataset_joined\n",
    "\n",
    "class TargetEncoder(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self):\n",
    "        super(TargetEncoder, self).__init__()\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        \"\"\"Create target encodings for pre-chosen columns more efficiently\"\"\"\n",
    "        print('Running TargetEncoder...')\n",
    "        cat_cols_to_target_encode = ['TAIL_NUM', 'OP_CARRIER_FL_NUM', 'ORIGIN_AIRPORT_ID', \n",
    "                                     'DEST_AIRPORT_ID', 'origin_station_id', \n",
    "                                     'dest_station_id', 'STATION']\n",
    "        \n",
    "        # Only process columns that exist in the dataset\n",
    "        existing_cols = [col for col in cat_cols_to_target_encode if col in dataset.columns]\n",
    "        \n",
    "        if not existing_cols:\n",
    "            return dataset\n",
    "            \n",
    "        # Get overall mean for filling NA values\n",
    "        DEP_DELAY_mean = dataset.agg(F.mean(\"DEP_DELAY\")).collect()[0][0]\n",
    "        \n",
    "        # Pre-calculate all encoding lookup tables at once and cache them\n",
    "        encoding_dfs = {}\n",
    "        for col in existing_cols:\n",
    "            encoding_df = dataset.groupBy(col).agg(\n",
    "                F.mean(\"DEP_DELAY\").alias(f\"{col}_mean_delay\")\n",
    "            ).orderBy(F.desc(f\"{col}_mean_delay\"))\n",
    "            \n",
    "            # Cache small lookup tables for faster joins\n",
    "            encoding_df = encoding_df.cache()\n",
    "            encoding_dfs[col] = encoding_df\n",
    "            \n",
    "        # Start with original dataset\n",
    "        result_df = dataset\n",
    "        \n",
    "        # Perform all joins efficiently\n",
    "        for col, encoding_df in encoding_dfs.items():\n",
    "            # Broadcast join for better performance\n",
    "            result_df = result_df.join(\n",
    "                F.broadcast(encoding_df), \n",
    "                on=col, \n",
    "                how=\"left\"\n",
    "            )\n",
    "            \n",
    "            # Unpersist the lookup table after use\n",
    "            encoding_df.unpersist()\n",
    "            \n",
    "        # Fill all NA values at once\n",
    "        all_mean_cols = [f\"{col}_mean_delay\" for col in existing_cols]\n",
    "        result_df = result_df.fillna(value=DEP_DELAY_mean, subset=all_mean_cols)\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "# Custom Transformer for cyclic encoding\n",
    "class CyclicEncoder(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self, inputCols=None, outputPrefix=\"\"):\n",
    "        super(CyclicEncoder, self).__init__()\n",
    "        self.inputCols = inputCols or []\n",
    "        self.outputPrefix = outputPrefix\n",
    "        self.ranges = {\n",
    "            'QUARTER': 4,\n",
    "            'DAY_OF_MONTH': 31,\n",
    "            'DAY_OF_WEEK': 7,\n",
    "            'MONTH': 12,\n",
    "            'sched_depart_hour_UTC': 24,\n",
    "            'sched_depart_minute_UTC': 60,\n",
    "            'four_hours_prior_depart_hour_UTC': 24,\n",
    "            'four_hours_prior_depart_minute_UTC': 60,\n",
    "            'two_hours_prior_depart_hour_UTC': 24,\n",
    "            'two_hours_prior_depart_minute_UTC': 60,\n",
    "            'station_hour_UTC': 24,\n",
    "            'station_minute_UTC': 60\n",
    "        }\n",
    "    \n",
    "    def setInputCols(self, value):\n",
    "        \"\"\"Set input columns to be encoded\"\"\"\n",
    "        self.inputCols = value\n",
    "        return self\n",
    "    \n",
    "    def getInputCols(self):\n",
    "        \"\"\"Get input columns to be encoded\"\"\"\n",
    "        return self.inputCols\n",
    "    \n",
    "    def getOutputCols(self):\n",
    "        \"\"\"Get output column names after encoding\"\"\"\n",
    "        output_cols = []\n",
    "        for col in self.inputCols:\n",
    "            if col in self.ranges:\n",
    "                output_cols.extend([f\"{col}_sin\", f\"{col}_cos\"])\n",
    "        return output_cols\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        \"\"\"Apply cyclic encoding to temporal features\"\"\"\n",
    "        print('Running CyclicEncoder...')\n",
    "        result = dataset\n",
    "        \n",
    "        # Apply cyclic encoding only to columns that exist in the dataset\n",
    "        available_cols = [col for col in self.inputCols if col in result.columns and col in self.ranges]\n",
    "        \n",
    "        for feature in available_cols:\n",
    "            max_val = self.ranges[feature]\n",
    "            result = result.withColumn(\n",
    "                f\"{feature}_sin\", \n",
    "                F.sin(2 * F.pi() * F.col(feature) / F.lit(max_val))\n",
    "            )\n",
    "            result = result.withColumn(\n",
    "                f\"{feature}_cos\", \n",
    "                F.cos(2 * F.pi() * F.col(feature) / F.lit(max_val))\n",
    "            )\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Custom Transformer for smote resampling\n",
    "class SMOTEResampler(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self):\n",
    "        super(SMOTEResampler, self).__init__()\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        print('Running SMOTEResampler...')\n",
    "        df_final = fast_pseudo_smote_preserve_all(dataset, \"DEP_DELAY\", feature_col=\"final_features\")\n",
    "        return df_final\n",
    "    \n",
    "# Custom Transformer for smote resampling\n",
    "class UpsampleResampler(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self):\n",
    "        super(UpsampleResampler, self).__init__()\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        print('Running UpsampleResampler...')\n",
    "        df_final = upsampling(dataset)\n",
    "        return df_final\n",
    "    \n",
    "# Custom Transformer for smote resampling\n",
    "class DownsampleResampler(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self):\n",
    "        super(DownsampleResampler, self).__init__()\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        print('Running DownsampleResampler...')\n",
    "        df_final = downsampling(dataset)\n",
    "        return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "126530b4-3d6c-49bf-a47d-cf81aa0773dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Parallelized Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ecf1b927-2ee7-44af-a6b4-d4eb4951bf44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ParallelTargetEncoder(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self):\n",
    "        super(ParallelTargetEncoder, self).__init__()\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        \"\"\"Parallel target encoding implementation for high-cardinality categorical features\"\"\"\n",
    "        print('Running ParallelTargetEncoder...')\n",
    "        cat_cols_to_target_encode = ['TAIL_NUM', 'OP_CARRIER_FL_NUM', 'ORIGIN_AIRPORT_ID', \n",
    "                                     'DEST_AIRPORT_ID', 'origin_station_id', \n",
    "                                     'dest_station_id', 'STATION']\n",
    "        \n",
    "        # Filter to only process columns that exist\n",
    "        existing_cols = [col for col in cat_cols_to_target_encode if col in dataset.columns]\n",
    "        \n",
    "        if not existing_cols:\n",
    "            return dataset\n",
    "        \n",
    "        # Pre-compute global mean for filling NA values\n",
    "        DEP_DELAY_mean = dataset.agg(F.mean(\"DEP_DELAY\")).collect()[0][0]\n",
    "        \n",
    "        # Set partition count based on cluster size\n",
    "        num_partitions = 200  # Adjusted for your 40-80 core cluster\n",
    "        \n",
    "        # Use a ThreadPool to compute encodings in parallel\n",
    "        def compute_encoding(col_name):\n",
    "            # Repartition specifically for this operation \n",
    "            encoding_df = dataset.repartition(min(60, dataset.rdd.getNumPartitions()), col_name) \\\n",
    "                .groupBy(col_name) \\\n",
    "                .agg(F.mean(\"DEP_DELAY\").alias(f\"{col_name}_mean_delay\"))\n",
    "            \n",
    "            # Cache the small lookup table\n",
    "            encoding_df = encoding_df.cache()\n",
    "            result_df = dataset.hint(\"broadcast\", encoding_df).join(\n",
    "                encoding_df, on=col_name, how=\"left\"\n",
    "            )\n",
    "            return result_df, col_name\n",
    "        \n",
    "        # Process in parallel batches to avoid overwhelming the driver\n",
    "        batch_size = 3  # Process 3 columns at a time\n",
    "        batched_cols = [existing_cols[i:i+batch_size] for i in range(0, len(existing_cols), batch_size)]\n",
    "        \n",
    "        result_df = dataset\n",
    "        \n",
    "        for batch in batched_cols:\n",
    "            # Process this batch in parallel\n",
    "            with ThreadPool(processes=min(len(batch), 3)) as pool:\n",
    "                partial_results = pool.map(compute_encoding, batch)\n",
    "            \n",
    "            # Merge the results\n",
    "            for partial_df, col_name in partial_results:\n",
    "                # We only need the new column from each partial result\n",
    "                mean_col = f\"{col_name}_mean_delay\"\n",
    "                result_df = result_df.join(\n",
    "                    partial_df.select(col_name, mean_col),\n",
    "                    on=col_name,\n",
    "                    how=\"left\"\n",
    "                )\n",
    "        \n",
    "        # Fill missing values in all mean columns at once\n",
    "        all_mean_cols = [f\"{col}_mean_delay\" for col in existing_cols]\n",
    "        result_df = result_df.fillna(DEP_DELAY_mean, subset=all_mean_cols)\n",
    "        \n",
    "        return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "69e3c864-6078-4d57-bfe8-726c470ce447",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ParallelGraphFeatureProcessor(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self):\n",
    "        super(ParallelGraphFeatureProcessor, self).__init__()\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        print('Running ParallelGraphFeatureProcessor...')\n",
    "        \n",
    "        # Make the dataset more resilient\n",
    "        dataset = dataset.repartition(250).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "        dataset.count()  # Force materialization\n",
    "        \n",
    "        try:\n",
    "            # Calculate the flight frequencies with better partition control\n",
    "            flight_freq = dataset.select(\n",
    "                col('ORIGIN_AIRPORT_ID').alias('src'),\n",
    "                col('DEST_AIRPORT_ID').alias('dst')\n",
    "            ).groupBy('src', 'dst').count().withColumnRenamed('count', 'flight_freq')\n",
    "            \n",
    "            # Break lineage with checkpoint to avoid recomputation\n",
    "            flight_freq = flight_freq.repartition(100)\n",
    "            flight_freq.checkpoint()\n",
    "            flight_freq.count()  # Force materialization\n",
    "            \n",
    "            # Create vertices with optimized operation\n",
    "            vertices_origin = dataset.select(col('ORIGIN_AIRPORT_ID').alias('id')).distinct()\n",
    "            vertices_dest = dataset.select(col('DEST_AIRPORT_ID').alias('id')).distinct()\n",
    "            \n",
    "            # Union and distinct with controlled partitioning\n",
    "            vertices = vertices_origin.unionAll(vertices_dest).distinct().repartition(20)\n",
    "            vertices.checkpoint()\n",
    "            vertices.count()  # Force materialization\n",
    "            \n",
    "            # Create edges with optimized partitioning\n",
    "            edges = dataset.select(\n",
    "                col('ORIGIN_AIRPORT_ID').alias('src'), \n",
    "                col('DEST_AIRPORT_ID').alias('dst')\n",
    "            ).distinct()\n",
    "            \n",
    "            # Join with controlled partition count\n",
    "            edges = edges.repartition(100, \"src\", \"dst\")\n",
    "            flight_freq = flight_freq.repartition(100, \"src\", \"dst\")\n",
    "            \n",
    "            edges = edges.join(flight_freq, ['src', 'dst'])\n",
    "            edges.checkpoint()\n",
    "            edges.count()  # Force materialization\n",
    "            \n",
    "            # GraphFrames creation with more resilient settings\n",
    "            from graphframes import GraphFrame\n",
    "            g = GraphFrame(vertices, edges)\n",
    "            \n",
    "            # Use maxIter instead of tol to ensure the algorithm terminates\n",
    "            # in a predictable amount of time\n",
    "            results = g.pageRank(\n",
    "                maxIter=20\n",
    "            )\n",
    "            \n",
    "            # Extract and checkpoint airport ranks\n",
    "            airport_pr = results.vertices.select(col('id'), col('pagerank'))\n",
    "            airport_pr = airport_pr.repartition(20)\n",
    "            airport_pr.checkpoint()\n",
    "            airport_pr.count()  # Force materialization\n",
    "            \n",
    "            # Broadcast the smaller PageRank table for efficient joins\n",
    "            broadcast_pr = F.broadcast(airport_pr)\n",
    "            \n",
    "            # Perform the joins more efficiently\n",
    "            df_graph = dataset.join(\n",
    "                broadcast_pr, \n",
    "                dataset.ORIGIN_AIRPORT_ID == broadcast_pr.id,\n",
    "                \"left\"\n",
    "            ).drop('id').withColumnRenamed('pagerank', 'origin_airport_pagerank')\n",
    "            \n",
    "            # Break lineage to avoid recomputing the expensive first join\n",
    "            df_graph = df_graph.repartition(250) \n",
    "            df_graph.checkpoint()\n",
    "            df_graph.count()  # Force materialization\n",
    "            \n",
    "            # Second join with broadcast\n",
    "            df_graph = df_graph.join(\n",
    "                broadcast_pr, \n",
    "                df_graph.DEST_AIRPORT_ID == broadcast_pr.id,\n",
    "                \"left\"\n",
    "            ).drop('id').withColumnRenamed('pagerank', 'dest_airport_pagerank')\n",
    "            \n",
    "            # Fill any nulls that might have been created in the join\n",
    "            df_graph = df_graph.fillna(0.0, subset=['origin_airport_pagerank', 'dest_airport_pagerank'])\n",
    "            \n",
    "            # Clean up the final result and make it ready for next stage\n",
    "            df_graph = df_graph.repartition(200)\n",
    "            \n",
    "            # Unpersist the original dataset to free memory\n",
    "            dataset.unpersist()\n",
    "            \n",
    "            return df_graph\n",
    "            \n",
    "        except Exception as e:\n",
    "            # If graph processing fails, fall back to a simple version\n",
    "            # that just assigns default values to avoid pipeline failures\n",
    "            dataset.unpersist()\n",
    "            print(f\"Graph processing failed with error: {e}\")\n",
    "            print(\"Falling back to default pagerank values\")\n",
    "            \n",
    "            # Add default pagerank columns\n",
    "            fallback_df = dataset.withColumn(\"origin_airport_pagerank\", F.lit(1.0))\n",
    "            fallback_df = fallback_df.withColumn(\"dest_airport_pagerank\", F.lit(1.0))\n",
    "            \n",
    "            return fallback_df.repartition(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "710ba63a-8a85-4f18-baea-56602790fe46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ParallelDataPreprocessor(Transformer, HasInputCols, HasOutputCols, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self, inputCols=None, outputCols=None):\n",
    "        super(ParallelDataPreprocessor, self).__init__()\n",
    "        self._setDefault(inputCols=[], outputCols=[])\n",
    "        self.setParams(inputCols, outputCols)\n",
    "    \n",
    "    def setParams(self, inputCols=None, outputCols=None):\n",
    "        if inputCols is not None:\n",
    "            self.setInputCols(inputCols)\n",
    "        if outputCols is not None:\n",
    "            self.setOutputCols(outputCols)\n",
    "        return self\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        \"\"\"Optimized transformer with parallel processing\"\"\"\n",
    "        print('Running ParallelDataPreprocessor...')\n",
    "        \n",
    "        # Set optimal partition count for processing\n",
    "        df = dataset.repartition(200)\n",
    "        \n",
    "        # Step 1: Drop records with missing target variable - can be done first for efficiency\n",
    "        df = df.dropna(subset=[\"DEP_DELAY\"])\n",
    "        \n",
    "        # Step 2-7: Group operations that can be done in parallel\n",
    "        # Identify columns to keep (inverse of columns to drop)\n",
    "        redundant_features = ['ORIGIN', 'OP_CARRIER', 'ORIGIN_AIRPORT_SEQ_ID', 'ORIGIN_CITY_MARKET_ID',\n",
    "                             'ORIGIN_CITY_NAME', 'ORIGIN_STATE_NM', 'ORIGIN_WAC', 'ORIGIN_STATE_FIPS',\n",
    "                             'DEST_AIRPORT_SEQ_ID', 'DEST_CITY_MARKET_ID', 'DEST', 'DEST_CITY_NAME',\n",
    "                             'DEST_STATE_FIPS', 'DEST_STATE_NM', 'DEST_WAC', 'DEP_TIME', 'CRS_DEP_TIME',\n",
    "                             'DEP_DELAY_NEW', 'DEP_DELAY_GROUP', 'DEP_TIME_BLK', 'DIVERTED',\n",
    "                             'origin_airport_name', 'origin_station_name', 'origin_iata_code',\n",
    "                             'origin_icao', 'origin_region', 'origin_station_lon', 'origin_airport_lat',\n",
    "                             'origin_station_lat', 'origin_airport_lon', 'dest_airport_name',\n",
    "                             'dest_station_name', 'dest_iata_code', 'dest_icao', 'dest_region',\n",
    "                             'dest_station_lat', 'dest_station_lon', 'dest_airport_lat',\n",
    "                             'dest_airport_lon', 'sched_depart_date_time', 'NAME', 'REM',\n",
    "                             'BackupEquipment', '_row_desc', 'FL_DATE', 'REPORT_TYPE', 'SOURCE']\n",
    "        \n",
    "        post_departure_features = [\"TAXI_OUT\", \"WHEELS_OFF\", \"WHEELS_ON\", \"TAXI_IN\", \n",
    "                                  \"CRS_ARR_TIME\", \"ARR_TIME\", \"ARR_DELAY\", \"ARR_DELAY_NEW\", \n",
    "                                  \"ARR_DEL15\", \"ARR_DELAY_GROUP\", \"ARR_TIME_BLK\", \"CANCELLED\", \n",
    "                                  \"CRS_ELAPSED_TIME\", \"ACTUAL_ELAPSED_TIME\", \"AIR_TIME\", \n",
    "                                  \"CARRIER_DELAY\", \"WEATHER_DELAY\", \"LATE_AIRCRAFT_DELAY\", \n",
    "                                  \"WEATHER_DELAY\", \"NAS_DELAY\", \"CARRIER_DELAY\", \"SECURITY_DELAY\", \n",
    "                                  \"TAIL_NUMBER\", \"FLIGHTS\"]\n",
    "        \n",
    "        colinear_cols = ['HourlyStationPressure', 'DISTANCE_GROUP', \n",
    "                         'HourlySeaLevelPressure', 'HourlyWetBulbTemperature',\n",
    "                         'HourlyDryBulbTemperature']\n",
    "        \n",
    "        # Combine all columns to drop\n",
    "        all_cols_to_drop = redundant_features + post_departure_features + colinear_cols\n",
    "        cols_to_drop = self._get_high_missing_cols(df) + [col for col in all_cols_to_drop if col in df.columns]\n",
    "        \n",
    "        # Keep only columns we want\n",
    "        keep_cols = [col for col in df.columns if col not in cols_to_drop]\n",
    "        df = df.select(*keep_cols)\n",
    "        \n",
    "        # Convert data types in bulk\n",
    "        int_cols = ['QUARTER', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'OP_CARRIER_FL_NUM', \n",
    "                    'DEP_DEL15', 'YEAR', 'MONTH', 'OP_CARRIER_FL_NUM']\n",
    "        float_cols = ['DEP_DELAY', 'DISTANCE_GROUP', 'DISTANCE', 'origin_station_dis',\n",
    "                     'dest_station_dis', 'LATITUDE', 'LONGITUDE', 'ELEVATION',\n",
    "                     'HourlyAltimeterSetting', 'HourlyDewPointTemperature',\n",
    "                     'HourlyDryBulbTemperature', 'HourlyPrecipitation', \n",
    "                     'HourlyRelativeHumidity', 'HourlySeaLevelPressure',\n",
    "                     'HourlyStationPressure', 'HourlyVisibility',\n",
    "                     'HourlyWetBulbTemperature', 'HourlyWindDirection', 'HourlyWindSpeed']\n",
    "        \n",
    "        # Use select with casting to apply all type conversions in one operation\n",
    "        select_exprs = []\n",
    "        for col_name in df.columns:\n",
    "            if col_name in int_cols:\n",
    "                select_exprs.append(F.col(col_name).cast(\"int\").alias(col_name))\n",
    "            elif col_name in float_cols:\n",
    "                select_exprs.append(F.col(col_name).cast(\"float\").alias(col_name))\n",
    "            else:\n",
    "                select_exprs.append(F.col(col_name))\n",
    "        \n",
    "        df = df.select(*select_exprs)\n",
    "        \n",
    "        # Drop duplicates and sort once at the end\n",
    "        df = df.dropDuplicates()\n",
    "        \n",
    "        # Sort and repartition for next stage\n",
    "        df = df.orderBy('YEAR', 'MONTH', 'DAY_OF_MONTH', 'sched_depart_date_time_UTC')\n",
    "        \n",
    "        checkpoint_save(df, \"checkpoint_preprocessed\")\n",
    "        clear_cache()\n",
    "        return checkpoint_load(\"checkpoint_preprocessed\")\n",
    "    \n",
    "    def _get_high_missing_cols(self, df, threshold=30):\n",
    "        \"\"\"\n",
    "        Find columns with missing values above the threshold percentage\n",
    "        \"\"\"\n",
    "        total_count = df.count()\n",
    "        if total_count == 0:\n",
    "            return []\n",
    "            \n",
    "        missing_counts = []\n",
    "        for col in df.columns:\n",
    "            null_count = df.where(F.col(col).isNull()).count()\n",
    "            missing_percent = (null_count / total_count) * 100\n",
    "            if missing_percent > threshold:\n",
    "                missing_counts.append(col)\n",
    "                \n",
    "        return missing_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "842da1f3-b755-4e87-8db4-be2fc1fc45af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "new custom cv method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87ea9fd1-2e2d-4a20-981d-df9b7aad0cba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _parallelFitTasks(est, train_df, evaluators, val_df, epm):\n",
    "    \"\"\"\n",
    "    Creates tasks for each parameter map (paramMap) so that they can be run in parallel threads.\n",
    "    Each task returns a tuple (paramIndex, metrics_dict), where:\n",
    "      - paramIndex is the index of the parameter map in the list epm.\n",
    "      - metrics_dict is a dictionary mapping each evaluators metric name to the computed score on the validation data.\n",
    "    \"\"\"\n",
    "     # Define an inner function 'singleTask' that takes a parameter map index.\n",
    "    def singleTask(paramIndex):\n",
    "        # Retrieve the parameter map for the current index from the list of parameter maps.\n",
    "        paramMap = epm[paramIndex]\n",
    "        # Create a copy of the estimator with the current parameter map applied, then fit the model on the training DataFrame.\n",
    "        model = est.copy(paramMap).fit(train_df)\n",
    "        # Use the trained model to make predictions on the validation DataFrame.\n",
    "        predictions = model.transform(val_df)\n",
    "        # Initialize an empty dictionary to store the metrics computed by each evaluator.\n",
    "        metrics_dict = {}\n",
    "        # Loop through the list of evaluators provided.\n",
    "        for evaluator in evaluators:\n",
    "            # Retrieve the metric name for the current evaluator (e.g., \"f1\", \"accuracy\").\n",
    "            metric_name = evaluator.getMetricName()\n",
    "             # Evaluate the predictions using the current evaluator and store the score in the dictionary.\n",
    "            metrics_dict[metric_name] = evaluator.evaluate(predictions)\n",
    "        # Return a tuple containing the parameter map index and the dictionary of computed metrics.\n",
    "        return paramIndex, metrics_dict\n",
    "    \n",
    "    # Create and return a list of lambda functions (tasks) for each parameter map index (for each hyperparameter combination)\n",
    "    return [lambda idx=i: singleTask(idx) for i in range(len(epm))]\n",
    "\n",
    "class CustomCrossValidator(Estimator, MLReadable, MLWritable):\n",
    "    \"\"\"\n",
    "    A custom cross-validator (inspired by Tim Lin's blog, https://www.timlrx.com/blog/creating-a-custom-cross-validation-function-in-pyspark) to perform\n",
    "    time-series or custom cross-validation in one DataFrame. It supports multiple evaluators to compute various metrics simultaneously.\n",
    "    \n",
    "    Steps:\n",
    "      1) Collect distinct fold IDs from 'fold_id'.\n",
    "      2) For each fold, filter rows where train_test == splitWord[0] for training,\n",
    "         and train_test == splitWord[1] for testing.\n",
    "      3) Train/evaluate each param map in parallel, accumulate metrics.\n",
    "      4) Pick the best param map (best hyperparameter combination), optionally retrain the final model on ALL data labeled as train.\n",
    "    \"\"\"\n",
    "    def __init__(self, estimator=None, estimatorParamMaps=None, evaluators=None,\n",
    "                 splitWord=('train','test'), cvCol='fold_id', seed=None, parallelism=1, verbose=0,\n",
    "                 retrainFullModel=False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "         - estimator: ML estimator (such as LogisticRegression, RandomForestClassifier)\n",
    "         - estimatorParamMaps: list of ParamMaps for hyperparameter search\n",
    "         - evaluators: evaluator or list of evaluators to compute metrics (such as MulticlassClassificationEvaluator)\n",
    "         - splitWord: tuple specifying the train/test labels (default ('train','test'))\n",
    "         - cvCol: name of the fold ID column in the DataFrame (default 'fold_id')\n",
    "         - seed: optional random seed\n",
    "         - parallelism: number of threads for parallel tasks\n",
    "         - verbose: 0 for no prints, 1 for printing fold info and metrics\n",
    "         - retrainFullModel: if True, retrain final model on ALL data labeled as train (default 'False')\n",
    "        \"\"\"\n",
    "        super().__init__() # allows you to call methods of a parent (or superclass) from a subclass\n",
    "        self.estimator = estimator\n",
    "        self.estimatorParamMaps = estimatorParamMaps\n",
    "         # Make sure evaluators is always a list (even if a single evaluator is passed)\n",
    "        self.evaluators = evaluators if isinstance(evaluators, list) else [evaluators]\n",
    "        self.splitWord = splitWord\n",
    "        self.cvCol = cvCol\n",
    "        self.seed = seed\n",
    "        self.parallelism = parallelism\n",
    "        self.verbose = verbose\n",
    "        self.retrainFullModel = retrainFullModel\n",
    "\n",
    "    def _fit(self, dataset):\n",
    "        est = self.estimator\n",
    "        epm = self.estimatorParamMaps\n",
    "        evaluators = self.evaluators\n",
    "        trainWord, testWord = self.splitWord\n",
    "        foldColName = self.cvCol\n",
    "        par = self.parallelism\n",
    "\n",
    "        # Make sure required parameters are provided\n",
    "        if not est or not epm or not evaluators:\n",
    "            raise ValueError(\"Must provide estimator, estimatorParamMaps, and evaluators.\")\n",
    "\n",
    "        # Get distinct fold IDs\n",
    "        distinctFolds = dataset.select(foldColName).distinct().collect()\n",
    "        foldIds = sorted(row[foldColName] for row in distinctFolds)\n",
    "        nFolds = len(foldIds)\n",
    "        numModels = len(epm)\n",
    "\n",
    "        # Initialize lists of dictionaries to hold metrics per param map\n",
    "        avg_metrics = [dict() for i in range(numModels)]\n",
    "        matrix_metrics = [[dict() for i in range(nFolds)] for i in range(numModels)]\n",
    "\n",
    "        # Create a thread pool for parallel processing\n",
    "        pool = ThreadPool(processes=min(par, numModels))\n",
    "\n",
    "        # Loop over each fold and process train/test splits\n",
    "        for i, fid in enumerate(foldIds):\n",
    "            train_df = dataset.filter((col(foldColName) == fid) & (col(\"train_test\") == trainWord)).cache()\n",
    "            val_df   = dataset.filter((col(foldColName) == fid) & (col(\"train_test\") == testWord)).cache()\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"fold {fid}: train={train_df.count()} rows, test={val_df.count()} rows\")\n",
    "\n",
    "            # Create parallel tasks to fit and evaluate each parameter map on this fold\n",
    "            tasks = _parallelFitTasks(est, train_df, evaluators, val_df, epm)\n",
    "            for j, metrics_dict in pool.imap_unordered(lambda f: f(), tasks):\n",
    "                # Store metrics for the current parameter map and fold\n",
    "                matrix_metrics[j][i] = metrics_dict\n",
    "                # Accumulate average metrics over folds for each evaluator metric\n",
    "                for evaluator in evaluators:\n",
    "                    mname = evaluator.getMetricName()\n",
    "                    avg_metrics[j][mname] = avg_metrics[j].get(mname, 0.0) + (metrics_dict[mname] / nFolds)\n",
    "\n",
    "            # Free up memory by unpersisting the DataFrames\n",
    "            train_df.unpersist()\n",
    "            val_df.unpersist()\n",
    "\n",
    "        # Choose best param map based on the primary evaluator's metric (the first evaluator you specify in `CustomCrossValidator()` )\n",
    "        primary_metric = evaluators[0].getMetricName()\n",
    "        if evaluators[0].isLargerBetter():\n",
    "            bestIndex = int(np.argmax([avg_metrics[j][primary_metric] for j in range(numModels)]))\n",
    "        else:\n",
    "            bestIndex = int(np.argmin([avg_metrics[j][primary_metric] for j in range(numModels)]))\n",
    "\n",
    "        # Extract best metric values and hyperparameter values from the best parameter map\n",
    "        best_metric_values = avg_metrics[bestIndex]\n",
    "        best_hyperparams = {param.name: value for param, value in epm[bestIndex].items()}\n",
    "\n",
    "        # Optionally retrain final model on ALL data labeled as train\n",
    "        if self.retrainFullModel:\n",
    "            full_train_df = dataset.filter(col(\"train_test\") == trainWord)\n",
    "            bestModel = est.copy(epm[bestIndex]).fit(full_train_df)\n",
    "        else:\n",
    "            bestModel = None\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Best Metric Values:\", best_metric_values)\n",
    "            print(\"Best Hyperparameter Values:\", best_hyperparams)\n",
    "\n",
    "        # Return a custom cross-validation model containing the best model and additional details\n",
    "        return CustomCrossValidatorModel(bestModel, matrix_metrics, avg_metrics, bestIndex, best_metric_values, best_hyperparams)\n",
    "    \n",
    "# The CustomCrossValidatorModel class is needed to wrap the best model along with the additional cross-validation results \n",
    "# (like average metrics, best hyperparameters, etc.), so that we can easily inspect the results and use the model\n",
    "# in our Spark ML pipelines.\n",
    "class CustomCrossValidatorModel(Model, MLReadable, MLWritable):\n",
    "    \"\"\"\n",
    "    Custom Cross Validator Model that holds:\n",
    "      - bestModel: the final model (if retrained)\n",
    "      - matrix_metrics: detailed metrics for each fold and each param map\n",
    "      - avgMetrics: average metrics across folds per param map (list of dictionaries)\n",
    "      - bestIndex: the index of the best parameter map\n",
    "      - bestMetricValues: the metric dictionary for the best parameter map\n",
    "      - bestHyperParams: the best hyperparameter values as a dictionary\n",
    "    \"\"\"\n",
    "    def __init__(self, bestModel, matrix_metrics, avgMetrics, bestIndex, bestMetricValues, bestHyperParams):\n",
    "        super(CustomCrossValidatorModel, self).__init__() # allows you to call methods of a parent (or superclass) from a subclass\n",
    "        self.bestModel = bestModel\n",
    "        self.matrix_metrics = matrix_metrics\n",
    "        self.avgMetrics = avgMetrics\n",
    "        self.bestIndex = bestIndex\n",
    "        self.bestMetricValues = bestMetricValues\n",
    "        self.bestHyperParams = bestHyperParams\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        # If no final model is available, raise an error\n",
    "        if self.bestModel is None:\n",
    "            raise ValueError(\"No best model available. Consider setting retrainFullModel=True.\")\n",
    "        # Otherwise, use the best model to transform (make predictions on) the dataset\n",
    "        return self.bestModel.transform(dataset)\n",
    "\n",
    "    def copy(self, extra=None):\n",
    "        # Create and return a copy of this model with all its attributes\n",
    "        return CustomCrossValidatorModel(self.bestModel, self.matrix_metrics, self.avgMetrics,\n",
    "                                           self.bestIndex, self.bestMetricValues, self.bestHyperParams)\n",
    "\n",
    "    def explainParams(self):\n",
    "        # Return a string summary of the key parameters (bestIndex and bestHyperParams) of this model\n",
    "        return \"CustomCrossValidatorModel with bestIndex={} and bestHyperParams={}\".format(self.bestIndex, self.bestHyperParams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e40f04b-c1e3-4ccf-9472-212d3f2e2316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "optimized time based folds (doesnt repeat certain unnecessary processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fe97c90-e7e6-4aa8-bfb7-5258b84cb6df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_time_based_folds_optimized(df, n_folds=3, train_ratio=0.8, margin=0, resampling_method=None):\n",
    "    \"\"\"\n",
    "    Optimized version of create_time_based_folds that focuses only on:\n",
    "    1. Assigning fold IDs and train/test labels\n",
    "    2. Target encoding (per fold to prevent leakage)\n",
    "    3. Feature scaling (per fold to prevent leakage)\n",
    "    4. Optional resampling (per fold)\n",
    "    \n",
    "    Parameters:\n",
    "      - df: PySpark DataFrame (already preprocessed with categorical and cyclic features)\n",
    "      - n_folds: Number of folds (blocks)\n",
    "      - train_ratio: Fraction of each fold used for training\n",
    "      - margin: Number of rows to skip between training and test within each fold\n",
    "      - resampling_method: Method for resampling ('SMOTE', 'UPSAMPLE', 'DOWNSAMPLE', None)\n",
    "    \n",
    "    Assumes df already contains:\n",
    "    - All relevant features\n",
    "    - 'cyclic_onehot_features' column (vector of one-hot encoded and cyclic features)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with fold_id, train_test labels, and properly processed features\n",
    "    \"\"\"\n",
    "    # First, balance the partitions while maintaining chronological order\n",
    "    df = create_balanced_time_ordered_partitions(df, num_partitions=200)\n",
    "\n",
    "    # 1. Assign row indices in chronological order\n",
    "    w = Window.orderBy(F.monotonically_increasing_id())\n",
    "    df_indexed = df.withColumn(\"row_index\", F.row_number().over(w) - 1)\n",
    "    \n",
    "    # 2. Compute fold size\n",
    "    total_count = df_indexed.count()\n",
    "    fold_size = total_count // n_folds\n",
    "    \n",
    "    # 3. Create UDF to assign fold_id and train_test labels\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"fold_id\", T.IntegerType(), False),\n",
    "        T.StructField(\"train_test\", T.StringType(), False)\n",
    "    ])\n",
    "    \n",
    "    @F.udf(returnType=schema)\n",
    "    def assign_fold_train_test(row_idx):\n",
    "        # Determine fold_id\n",
    "        fold_id = int(row_idx // fold_size) + 1\n",
    "        if fold_id > n_folds:\n",
    "            fold_id = n_folds\n",
    "        \n",
    "        # Position within fold\n",
    "        start_idx = (fold_id - 1) * fold_size\n",
    "        in_fold_pos = row_idx - start_idx\n",
    "        \n",
    "        # Handle last fold differently if it has leftover rows\n",
    "        current_fold_size = fold_size\n",
    "        if fold_id == n_folds:\n",
    "            leftover = total_count - (n_folds - 1) * fold_size\n",
    "            current_fold_size = leftover\n",
    "            in_fold_pos = row_idx - (n_folds - 1) * fold_size\n",
    "        \n",
    "        # Determine train/test/skip status\n",
    "        train_cutoff = int(train_ratio * current_fold_size)\n",
    "        if in_fold_pos < train_cutoff:\n",
    "            return (fold_id, \"train\")\n",
    "        elif in_fold_pos < train_cutoff + margin:\n",
    "            return (fold_id, \"skip\")\n",
    "        else:\n",
    "            return (fold_id, \"test\")\n",
    "    \n",
    "    # 4. Apply UDF to create fold structure\n",
    "    df_labeled = df_indexed.withColumn(\n",
    "        \"fold_struct\", assign_fold_train_test(F.col(\"row_index\"))\n",
    "    ).withColumn(\n",
    "        \"fold_id\", F.col(\"fold_struct.fold_id\")\n",
    "    ).withColumn(\n",
    "        \"train_test\", F.col(\"fold_struct.train_test\")\n",
    "    ).drop(\"row_index\", \"fold_struct\")\n",
    "\n",
    "    # After fold labeling\n",
    "    checkpoint_save(df_labeled, \"checkpoint_fold_labeled\")\n",
    "    df_labeled = checkpoint_load(\"checkpoint_fold_labeled\")\n",
    "    \n",
    "    # 5. Identify numeric features for scaling (exclude DEP_DELAY)\n",
    "    numeric_cols = [c for (c, t) in df_labeled.dtypes \n",
    "                   if t in ('float', 'double') and c != 'DEP_DELAY']\n",
    "    \n",
    "    # 6. Columns to target encode\n",
    "    cat_cols_to_target_encode = ['TAIL_NUM', 'OP_CARRIER_FL_NUM', 'ORIGIN_AIRPORT_ID', \n",
    "                              'DEST_AIRPORT_ID', 'origin_station_id', \n",
    "                              'dest_station_id', 'STATION']\n",
    "    \n",
    "    # Keep only columns that exist in the dataframe\n",
    "    cat_cols_to_target_encode = [c for c in cat_cols_to_target_encode \n",
    "                               if c in df_labeled.columns]\n",
    "    \n",
    "    # 7. Process each fold separately to prevent data leakage\n",
    "    fold_id_list = df_labeled.select(\"fold_id\").distinct().rdd.map(lambda r: r[0]).collect()\n",
    "    fold_id_list = sorted(fold_id_list)\n",
    "    \n",
    "    # Store processed folds for later union\n",
    "    processed_folds = []\n",
    "    \n",
    "    for fid in fold_id_list:\n",
    "        print(f\"Processing fold {fid}...\")\n",
    "        \n",
    "        # Extract train/test/skip parts\n",
    "        train_df = df_labeled.filter((F.col(\"fold_id\") == fid) & (F.col(\"train_test\") == \"train\"))\n",
    "        test_df = df_labeled.filter((F.col(\"fold_id\") == fid) & (F.col(\"train_test\") == \"test\"))\n",
    "        skip_df = df_labeled.filter((F.col(\"fold_id\") == fid) & (F.col(\"train_test\") == \"skip\"))\n",
    "        \n",
    "        # Skip empty folds\n",
    "        if train_df.count() == 0:\n",
    "            print(f\"Fold {fid} has empty training set. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # === TARGET ENCODING (per fold) ===\n",
    "        if cat_cols_to_target_encode:\n",
    "            print(f\"  Performing target encoding for fold {fid}...\")\n",
    "            \n",
    "            # Get mean target value for missing values\n",
    "            DEP_DELAY_mean = train_df.agg(F.mean(\"DEP_DELAY\")).collect()[0][0]\n",
    "            \n",
    "            # Calculate all target encodings from train set at once\n",
    "            encoding_dfs = {}\n",
    "            for col in cat_cols_to_target_encode:\n",
    "                if col in train_df.columns:\n",
    "                    # Compute mean delay per category\n",
    "                    encoding_df = train_df.groupBy(col).agg(\n",
    "                        F.mean(\"DEP_DELAY\").alias(f\"{col}_mean_delay\")\n",
    "                    )\n",
    "                    encoding_dfs[col] = encoding_df\n",
    "            \n",
    "            # Apply encodings to train, test, and skip sets\n",
    "            for col, encoding_df in encoding_dfs.items():\n",
    "                # Apply to train set\n",
    "                train_df = train_df.join(F.broadcast(encoding_df), on=col, how=\"left\")\n",
    "                \n",
    "                # Apply to test set if it exists\n",
    "                if test_df.count() > 0:\n",
    "                    test_df = test_df.join(F.broadcast(encoding_df), on=col, how=\"left\")\n",
    "                \n",
    "                # Apply to skip set if it exists\n",
    "                if skip_df.count() > 0:\n",
    "                    skip_df = skip_df.join(F.broadcast(encoding_df), on=col, how=\"left\")\n",
    "            \n",
    "            # Fill missing values with global mean\n",
    "            mean_cols = [f\"{col}_mean_delay\" for col in cat_cols_to_target_encode \n",
    "                       if col in train_df.columns]\n",
    "            \n",
    "            if mean_cols:\n",
    "                train_df = train_df.fillna(DEP_DELAY_mean, subset=mean_cols)\n",
    "                if test_df.count() > 0:\n",
    "                    test_df = test_df.fillna(DEP_DELAY_mean, subset=mean_cols)\n",
    "                if skip_df.count() > 0:\n",
    "                    skip_df = skip_df.fillna(DEP_DELAY_mean, subset=mean_cols)\n",
    "        \n",
    "        # === FEATURE SCALING (per fold) ===\n",
    "        if numeric_cols:\n",
    "            print(f\"  Scaling features for fold {fid}...\")\n",
    "            \n",
    "            # Add target-encoded features to numeric features for scaling\n",
    "            target_cols = [f\"{col}_mean_delay\" for col in cat_cols_to_target_encode \n",
    "                         if f\"{col}_mean_delay\" in train_df.columns]\n",
    "            all_numeric = numeric_cols + target_cols\n",
    "            \n",
    "            # Assemble numeric features\n",
    "            assembler = VectorAssembler(\n",
    "                inputCols=all_numeric,\n",
    "                outputCol=\"numeric_features\",\n",
    "                handleInvalid=\"skip\"\n",
    "            )\n",
    "            \n",
    "            train_assembled = assembler.transform(train_df)\n",
    "            test_assembled = assembler.transform(test_df) if test_df.count() > 0 else None\n",
    "            skip_assembled = assembler.transform(skip_df) if skip_df.count() > 0 else None\n",
    "            \n",
    "            # Scale using train data stats only\n",
    "            scaler = RobustScaler(\n",
    "                inputCol=\"numeric_features\",\n",
    "                outputCol=\"scaled_numeric_features\",\n",
    "                withCentering=False,\n",
    "                withScaling=True\n",
    "            )\n",
    "            \n",
    "            scaler_model = scaler.fit(train_assembled)\n",
    "            \n",
    "            # Apply to all sets\n",
    "            train_scaled = scaler_model.transform(train_assembled)\n",
    "            test_scaled = scaler_model.transform(test_assembled) if test_assembled else None\n",
    "            skip_scaled = scaler_model.transform(skip_assembled) if skip_assembled else None\n",
    "            \n",
    "            # Final feature assembly (combine scaled numeric with existing cyclic/onehot)\n",
    "            final_cols = [\"scaled_numeric_features\"]\n",
    "            if \"cyclic_onehot_features\" in train_scaled.columns:\n",
    "                final_cols.append(\"cyclic_onehot_features\")\n",
    "            \n",
    "            final_assembler = VectorAssembler(\n",
    "                inputCols=final_cols,\n",
    "                outputCol=\"final_features\",\n",
    "                handleInvalid=\"skip\"\n",
    "            )\n",
    "            \n",
    "            train_final = final_assembler.transform(train_scaled)\n",
    "            test_final = final_assembler.transform(test_scaled) if test_scaled else None\n",
    "            skip_final = final_assembler.transform(skip_scaled) if skip_scaled else None\n",
    "            \n",
    "            # === RESAMPLING (only for train) ===\n",
    "            if resampling_method and train_final:\n",
    "                print(f\"  Applying {resampling_method} resampling for fold {fid}...\")\n",
    "                \n",
    "                if resampling_method.upper() == \"SMOTE\":\n",
    "                    train_final = fast_pseudo_smote_preserve_all(\n",
    "                        train_final, \"DEP_DELAY_BINNED\", feature_col=\"final_features\"\n",
    "                    )\n",
    "                elif resampling_method.upper() == \"UPSAMPLE\":\n",
    "                    train_final = upsampling(train_final)\n",
    "                elif resampling_method.upper() == \"DOWNSAMPLE\":\n",
    "                    train_final = downsampling(train_final)\n",
    "            \n",
    "            # Combine all parts of this fold\n",
    "            fold_parts = []\n",
    "            if train_final is not None and train_final.count() > 0:\n",
    "                fold_parts.append(train_final)\n",
    "            if test_final is not None and test_final.count() > 0:\n",
    "                fold_parts.append(test_final)\n",
    "            if skip_final is not None and skip_final.count() > 0:\n",
    "                fold_parts.append(skip_final)\n",
    "            \n",
    "            if fold_parts:\n",
    "                fold_processed = reduce(DataFrame.unionByName, fold_parts)\n",
    "                checkpoint_save(fold_processed, f\"checkpoint_fold_{fid}\")\n",
    "                fold_processed = checkpoint_load(f\"checkpoint_fold_{fid}\")\n",
    "                processed_folds.append(fold_processed)\n",
    "    \n",
    "    # 8. Combine all processed folds\n",
    "    if not processed_folds:\n",
    "        print(\"No processed folds to return!\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Combining all processed folds...\")\n",
    "    result_df = reduce(DataFrame.unionByName, processed_folds)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b07445c-ee19-44fa-923c-1c5ce47894c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Method to run Custom Cross-validator to get best hyper parameters\n",
    "\n",
    "NOTE: takes final feature list if want to keep specific post-processed columns, custom cv parallelism number is set here\n",
    "\n",
    "Resample method options: 'smote', 'upsample', 'downsample'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e067e832-a520-4f6e-83ff-67f44212e615",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_cross_validation_mod(df_train, modelType, paramGrid, final_feature_list=None, resample_method=None):\n",
    "    \"\"\"\n",
    "    Run cross-validation to find the best hyperparameters for provided model/paramgrid\n",
    "    \n",
    "    Parameters:\n",
    "    - df_train: Training DataFrame (should be already preprocessed but NOT scaled)\n",
    "    - final_feature_list: List of columns to include in the final feature set (optional)\n",
    "    \n",
    "    Returns:\n",
    "    - best_params: Dictionary of best hyperparameters\n",
    "    - best_metrics: Dictionary of metrics for the best hyperparameters\n",
    "    \"\"\"\n",
    "    print(\"Preparing data for time-based cross-validation...\")\n",
    "    \n",
    "    # Apply non-scaling preprocessing\n",
    "    # This includes cleaning, handling missing values, feature extraction, but NOT scaling\n",
    "    print(\"Applying non-scaling preprocessing steps...\")\n",
    "    \n",
    "    # Create a modified pipeline excluding scaling steps\n",
    "    stages = [\n",
    "        DataPreprocessor(),\n",
    "        DateTimeProcessor(),                 \n",
    "        SkyConditionsProcessor(),\n",
    "        OutlierHandler(),            \n",
    "        MissingValueHandler(),               \n",
    "        TargetBinner(),\n",
    "        GraphFeatureProcessor()             \n",
    "    ]\n",
    "\n",
    "    # We'll still create the cyclic features as they don't rely on scaling\n",
    "    time_features = ['QUARTER', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'MONTH',\n",
    "                    'sched_depart_hour_UTC', 'sched_depart_minute_UTC',\n",
    "                    'four_hours_prior_depart_hour_UTC', 'four_hours_prior_depart_minute_UTC',\n",
    "                    'two_hours_prior_depart_hour_UTC', 'two_hours_prior_depart_minute_UTC',\n",
    "                    'station_hour_UTC', 'station_minute_UTC']\n",
    "    \n",
    "    # Filter to only include columns that exist in the dataframe\n",
    "    time_features = [col for col in time_features if col in df_train.columns]\n",
    "    \n",
    "    # Define and aadd cyclic encoder \n",
    "    cyclic_encoder = CyclicEncoder().setInputCols(time_features)\n",
    "    stages.append(cyclic_encoder)\n",
    "    \n",
    "    # Create and apply non-scaling pipeline\n",
    "    non_scaling_pipeline = Pipeline(stages=stages)\n",
    "    non_scaling_model = non_scaling_pipeline.fit(df_train)\n",
    "    preprocessed_df = non_scaling_model.transform(df_train)\n",
    "    \n",
    "    # checkpoint and reload data\n",
    "    checkpoint_save(preprocessed_df, \"checkpoint_cv_preprocessed\")\n",
    "    preprocessed_df = checkpoint_load(\"checkpoint_cv_preprocessed\")\n",
    "    \n",
    "    categorical_features = ['OP_UNIQUE_CARRIER', 'OP_CARRIER_AIRLINE_ID', 'ORIGIN_STATE_ABR', \n",
    "                           'DEST_STATE_ABR', 'origin_type', 'dest_type',  \n",
    "                           'HourlyCloudCoverage', 'HourlyCloudLayerAmount']\n",
    "    \n",
    "    # cat_cols_to_target_encode = ['TAIL_NUM', 'OP_CARRIER_FL_NUM', 'ORIGIN_AIRPORT_ID', 'DEST_AIRPORT_ID', 'origin_station_id', 'dest_station_id', 'STATION']\n",
    "\n",
    "    # Filter to only include columns that exist in the dataframe\n",
    "    categorical_features = [col for col in categorical_features if col in preprocessed_df.columns]\n",
    "\n",
    "    # Apply string indexing and one-hot encoding\n",
    "    indexed_cols = []\n",
    "    onehot_cols = []\n",
    "    category_preprocessors = []\n",
    "    \n",
    "    for cat_col in categorical_features:\n",
    "        indexer = StringIndexer(\n",
    "            inputCol=cat_col,\n",
    "            outputCol=f\"{cat_col}_indexed\",\n",
    "            handleInvalid=\"keep\"\n",
    "        )\n",
    "        indexer_model = indexer.fit(preprocessed_df)\n",
    "        category_preprocessors.append(indexer_model)\n",
    "        preprocessed_df = indexer_model.transform(preprocessed_df)\n",
    "        indexed_cols.append(f\"{cat_col}_indexed\")\n",
    "        \n",
    "        encoder = OneHotEncoder(\n",
    "            inputCol=f\"{cat_col}_indexed\",\n",
    "            outputCol=f\"{cat_col}_onehot\"\n",
    "        )\n",
    "        encoder_model = encoder.fit(preprocessed_df)\n",
    "        category_preprocessors.append(encoder_model)\n",
    "        preprocessed_df = encoder_model.transform(preprocessed_df)\n",
    "        onehot_cols.append(f\"{cat_col}_onehot\")\n",
    "    \n",
    "    checkpoint_save(preprocessed_df, \"checkpoint_cv_categorical\")\n",
    "    preprocessed_df = checkpoint_load(\"checkpoint_cv_categorical\")\n",
    "\n",
    "    # Keep final features in list if provided\n",
    "    if final_feature_list:\n",
    "        final_feature_list = [col for col in final_feature_list if col in preprocessed_df.columns]\n",
    "        preprocessed_df = preprocessed_df.select(*final_feature_list)\n",
    "    \n",
    "    # Assemble one-hot encoded features\n",
    "    if onehot_cols:\n",
    "        onehot_cols = [col for col in onehot_cols if col in preprocessed_df.columns]\n",
    "        onehot_assembler = VectorAssembler(\n",
    "            inputCols=onehot_cols,\n",
    "            outputCol=\"onehot_features\",\n",
    "            handleInvalid=\"skip\"\n",
    "        )\n",
    "        preprocessed_df = onehot_assembler.transform(preprocessed_df)\n",
    "    \n",
    "    # Assemble cyclic features if available\n",
    "    cyclic_cols = cyclic_encoder.getOutputCols()\n",
    "    \n",
    "    if cyclic_cols:\n",
    "        cyclic_cols = [col for col in cyclic_cols if col in preprocessed_df.columns]\n",
    "        cyclic_assembler = VectorAssembler(\n",
    "            inputCols=cyclic_cols,\n",
    "            outputCol=\"cyclic_features\",\n",
    "            handleInvalid=\"skip\"\n",
    "        )\n",
    "        preprocessed_df = cyclic_assembler.transform(preprocessed_df)\n",
    "    \n",
    "    # Combine cyclic and onehot features\n",
    "    combined_features = []\n",
    "    if \"cyclic_features\" in preprocessed_df.columns:\n",
    "        combined_features.append(\"cyclic_features\")\n",
    "    if \"onehot_features\" in preprocessed_df.columns:\n",
    "        combined_features.append(\"onehot_features\")\n",
    "    \n",
    "    if combined_features:\n",
    "        combined_assembler = VectorAssembler(\n",
    "            inputCols=combined_features,\n",
    "            outputCol=\"cyclic_onehot_features\",\n",
    "            handleInvalid=\"skip\"\n",
    "        )\n",
    "        preprocessed_df = combined_assembler.transform(preprocessed_df)\n",
    "        preprocessed_df.checkpoint()\n",
    "        preprocessed_df.count()  # Force materialization\n",
    "\n",
    "    else:\n",
    "        # If no cyclic or onehot features, create an empty vector as placeholder\n",
    "        preprocessed_df = preprocessed_df.withColumn(\n",
    "            \"cyclic_onehot_features\",\n",
    "            F.array().cast(\"vector\")\n",
    "        )\n",
    "    \n",
    "    # make sure dataset is chronologically ordered for time-based cross-validation\n",
    "    preprocessed_df = preprocessed_df.orderBy('YEAR', 'MONTH', 'DAY_OF_MONTH', 'sched_depart_hour_UTC', 'sched_depart_minute_UTC')\n",
    "\n",
    "    checkpoint_save(preprocessed_df, \"checkpoint_cv_combined_features\")\n",
    "    preprocessed_df = checkpoint_load(\"checkpoint_cv_combined_features\")\n",
    "    \n",
    "    # Create time-based folds (WITH scaling within each fold)\n",
    "    print(\"Creating time-based folds with per-fold scaling...\")\n",
    "    # Now use create_time_based_folds which will scale numerical features properly within each fold\n",
    "    # df_cv = create_time_based_folds(preprocessed_df, n_folds=2, train_ratio=0.8, resampling_method=resample_method)\n",
    "    df_cv = create_time_based_folds_optimized(preprocessed_df, n_folds=2, train_ratio=0.8, resampling_method=resample_method)\n",
    "    \n",
    "    # Define model for cross-validation\n",
    "    print(\"Setting up cross-validation...\")\n",
    "\n",
    "    # Adjust for MLP if needed\n",
    "    # Get a sample from the processed data for feature size\n",
    "    sample_features = df_cv.filter(F.col(\"final_features\").isNotNull()).select(\"final_features\").limit(1).collect()\n",
    "    \n",
    "    if sample_features:  # Make sure we have a valid sample\n",
    "        feature_size = len(sample_features[0][0])\n",
    "        print(f\"Adjusted feature vector size for CV: {feature_size}\")\n",
    "        \n",
    "        # Update MLP architecture if using MLP\n",
    "        if isinstance(modelType, MultilayerPerceptronClassifier):\n",
    "            current_layers = modelType.getLayers()\n",
    "            new_layers = [feature_size] + current_layers[1:]\n",
    "            modelType = modelType.setLayers(new_layers)\n",
    "            print(f\"Updated MLP layers for CV: {new_layers}\")\n",
    "\n",
    "    # Define evaluators\n",
    "    evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"DEP_DELAY_BINNED\", \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"f1\"\n",
    "    )\n",
    "    \n",
    "    evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"DEP_DELAY_BINNED\", \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"accuracy\"\n",
    "    )\n",
    "    \n",
    "    evaluator_precision = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"DEP_DELAY_BINNED\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"weightedPrecision\"\n",
    "    )\n",
    "    \n",
    "    evaluator_recall = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"DEP_DELAY_BINNED\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"weightedRecall\"\n",
    "    )\n",
    "    \n",
    "    # Use the original CustomCrossValidator since scaling already happens in create_time_based_folds\n",
    "    cv = CustomCrossValidator(\n",
    "        estimator=modelType,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluators=[evaluator_f1, evaluator_accuracy, evaluator_precision, evaluator_recall],\n",
    "        splitWord=('train', 'test'),\n",
    "        cvCol='fold_id',\n",
    "        parallelism=4,\n",
    "        verbose=1,\n",
    "        retrainFullModel=False\n",
    "    )\n",
    "\n",
    "    # Run cross-validation\n",
    "    print(\"Running cross-validation...\")\n",
    "    cvModel = cv.fit(df_cv)\n",
    "    \n",
    "    # Since a big chunk of preprocessing is identical, return part of pipeline to continue in train_model_mod()\n",
    "    preprocessing_pipeline = {\n",
    "    \"non_scaling_model\": non_scaling_model,        # Initial preprocessing\n",
    "    \"category_preprocessors\": category_preprocessors,  # Categorical variables\n",
    "    \"onehot_assembler\": onehot_assembler,          # One-hot encoding\n",
    "    \"cyclic_assembler\": cyclic_assembler,          # Cyclic features\n",
    "    \"combined_assembler\": combined_assembler       # Combined categorical/cyclic\n",
    "    }\n",
    "\n",
    "    # Return best hyperparameters and metrics\n",
    "    return cvModel.bestHyperParams, cvModel.bestMetricValues, preprocessing_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fedb64a-c18b-4d5c-9bb3-35dbf8114905",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Complete Training Model Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c2062a7-fbb7-4123-b3f7-3ae42a5eb52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_model_mod(df_train, modelType, paramGrid, df_test=None, use_cross_validation=True, final_feature_list=None, resample_method=None):\n",
    "    \"\"\"\n",
    "    Train a flight delay prediction model\n",
    "    \n",
    "    Parameters:\n",
    "    - df_train: Training DataFrame\n",
    "    - df_test: Testing DataFrame (optional)\n",
    "    - use_cross_validation: Whether to use cross-validation for hyperparameter tuning\n",
    "    - final_feature_list: List of final feature columns to use (optional)\n",
    "    \n",
    "    Returns:\n",
    "    - model: The trained pipeline model\n",
    "    - metrics: Evaluation metrics (if df_test is provided)\n",
    "    \"\"\"\n",
    "    # Note: data is chronologically sorted at end of first transformer (DataPreprocessor()) now\n",
    "\n",
    "    # Hyperparameter tuning with cross-validation (if requested)\n",
    "    best_params = {}\n",
    "    \n",
    "    if use_cross_validation:\n",
    "        print(\"Beginning cross-validation for hyperparameter tuning...\")\n",
    "        # Sample records from every month for hyperparameter tuning\n",
    "        df_sample = sample_with_time_windows(df_train, 12000000)\n",
    "        df_sample = create_balanced_time_ordered_partitions(df_sample, num_partitions=200)\n",
    "        print(f'Cross-validation using sample size: {df_sample.count()}, partition count: {df_sample.rdd.getNumPartitions()}')\n",
    "        best_params, best_metrics, pp_pipeline = run_cross_validation_mod(df_sample, modelType, paramGrid, final_feature_list=final_feature_list, resample_method=resample_method)\n",
    "        print(\"Best hyperparameters:\", best_params)\n",
    "        print(\"Best cross-validation metrics:\", best_metrics)\n",
    "\n",
    "    # Apply non-scaling preprocessing steps for the final model\n",
    "    print(\"Building and applying non-scaling preprocessing steps for final model...\")\n",
    "    \n",
    "    # Use same initial pipeline stages from cv since preprocessing of non-numeric is identical\n",
    "    print(\"Using preprocessing pipeline from cross-validation...\")\n",
    "    non_scaling_model = pp_pipeline[\"non_scaling_model\"]\n",
    "    \n",
    "    category_preprocessors = pp_pipeline[\"category_preprocessors\"]\n",
    "    onehot_assembler = pp_pipeline[\"onehot_assembler\"]\n",
    "    cyclic_assembler = pp_pipeline[\"cyclic_assembler\"]\n",
    "    combined_assembler = pp_pipeline[\"combined_assembler\"]\n",
    "    \n",
    "    # Apply non-scaling preprocessing to train and test\n",
    "    df_train_processed = non_scaling_model.transform(df_train)\n",
    "    checkpoint_save(df_train_processed, \"checkpoint_train_preprocessed\")\n",
    "    df_train_processed = checkpoint_load(\"checkpoint_train_preprocessed\")\n",
    "    df_test_processed = None\n",
    "    if df_test is not None:\n",
    "        df_test_processed = non_scaling_model.transform(df_test)\n",
    "        checkpoint_save(df_test_processed, \"checkpoint_test_preprocessed\")\n",
    "        df_test_processed = checkpoint_load(\"checkpoint_test_preprocessed\")\n",
    "    \n",
    "    # Prepare categorical features\n",
    "    # categorical_features = ['OP_UNIQUE_CARRIER', 'OP_CARRIER_AIRLINE_ID', 'TAIL_NUM', \n",
    "    #                        'OP_CARRIER_FL_NUM', 'ORIGIN_AIRPORT_ID', 'ORIGIN_STATE_ABR', \n",
    "    #                        'DEST_AIRPORT_ID', 'DEST_STATE_ABR', 'origin_station_id', \n",
    "    #                        'origin_type', 'dest_station_id', 'dest_type', 'STATION',  \n",
    "    #                        'HourlyCloudCoverage', 'HourlyCloudLayerAmount']\n",
    "    \n",
    "    categorical_features = ['OP_UNIQUE_CARRIER', 'OP_CARRIER_AIRLINE_ID', 'ORIGIN_STATE_ABR', \n",
    "                           'DEST_STATE_ABR', 'origin_type', 'dest_type',  \n",
    "                           'HourlyCloudCoverage', 'HourlyCloudLayerAmount']\n",
    "\n",
    "    # Filter to only include columns that exist in the dataframe\n",
    "    categorical_features = [col for col in categorical_features if col in df_train_processed.columns]\n",
    "    \n",
    "    # Create and fit categorical preprocessors\n",
    "    onehot_cols = []\n",
    "    \n",
    "    for cat_col in categorical_features:\n",
    "        onehot_cols.append(f\"{cat_col}_onehot\")\n",
    "    \n",
    "    # Apply categorical preprocessing to train data\n",
    "    for processor in category_preprocessors:\n",
    "            df_train_processed = processor.transform(df_train_processed)\n",
    "    checkpoint_save(df_train_processed, \"checkpoint_train_categorical\")\n",
    "    df_train_processed = checkpoint_load(\"checkpoint_train_categorical\")\n",
    "\n",
    "    # Apply categorical preprocessing to test data\n",
    "    if df_test_processed is not None:\n",
    "        for processor in category_preprocessors:\n",
    "            df_test_processed = processor.transform(df_test_processed)\n",
    "        checkpoint_save(df_test_processed, \"checkpoint_test_categorical\")\n",
    "        df_test_processed = checkpoint_load(\"checkpoint_test_categorical\")\n",
    "\n",
    "    # Keep final features in list if provided\n",
    "    if final_feature_list:\n",
    "        final_feature_list = [col for col in final_feature_list if col in df_train_processed.columns]\n",
    "        df_train_processed = df_train_processed.select(*final_feature_list)\n",
    "        if df_test_processed is not None:\n",
    "            df_test_processed = df_test_processed.select(*final_feature_list)\n",
    "\n",
    "    # Assemble one-hot encoded features\n",
    "    if onehot_cols:\n",
    "        df_train_processed = onehot_assembler.transform(df_train_processed)\n",
    "        if df_test_processed is not None:\n",
    "            df_test_processed = onehot_assembler.transform(df_test_processed)\n",
    "    \n",
    "    # Assemble cyclic features\n",
    "    cyclic_cols = cyclic_assembler.getInputCols()\n",
    "    if cyclic_cols:\n",
    "        df_train_processed = cyclic_assembler.transform(df_train_processed)\n",
    "        if df_test_processed is not None:\n",
    "            df_test_processed = cyclic_assembler.transform(df_test_processed)\n",
    "    \n",
    "    # Combine cyclic and onehot features\n",
    "    combined_features = [\"cyclic_features\", \"onehot_features\"]\n",
    "    \n",
    "    if combined_features:\n",
    "        df_train_pretarget = combined_assembler.transform(df_train_processed)\n",
    "        checkpoint_save(df_train_pretarget, \"checkpoint_train_pretarget_assembled\")\n",
    "        df_train_pretarget = checkpoint_load(\"checkpoint_train_pretarget_assembled\")\n",
    "        if df_test_processed is not None:\n",
    "            df_test_pretarget = combined_assembler.transform(df_test_processed)\n",
    "            checkpoint_save(df_test_pretarget, \"checkpoint_test_pretarget_assembled\")\n",
    "            df_test_pretarget = checkpoint_load(\"checkpoint_test_pretarget_assembled\")\n",
    "    else:\n",
    "        # If no cyclic or onehot features, create an empty vector as placeholder\n",
    "        df_train_pretarget = df_train_processed.withColumn(\n",
    "            \"cyclic_onehot_features\",\n",
    "            F.array().cast(\"vector\")\n",
    "        )\n",
    "        if df_test_processed is not None:\n",
    "            df_test_pretarget = df_test_processed.withColumn(\n",
    "                \"cyclic_onehot_features\",\n",
    "                F.array().cast(\"vector\")\n",
    "            )\n",
    "\n",
    "    # Process TargetEncoder features after cv\n",
    "    target_encoder = TargetEncoder()\n",
    "    df_train_processed = target_encoder.transform(df_train_pretarget)\n",
    "    checkpoint_save(df_train_processed, \"checkpoint_train_target_encoded\")\n",
    "    df_train_processed = checkpoint_load(\"checkpoint_train_target_encoded\")\n",
    "    if df_test_pretarget is not None:\n",
    "        df_test_processed = target_encoder.transform(df_test_pretarget)\n",
    "        checkpoint_save(df_test_processed, \"checkpoint_test_target_encoded\")\n",
    "        df_test_processed = checkpoint_load(\"checkpoint_test_target_encoded\")\n",
    "\n",
    "    float_cols = [c for (c, t) in df_train_processed.dtypes if (t == 'float') and (c != 'DEP_DELAY')] + ['TAIL_NUM_mean_delay', 'OP_CARRIER_FL_NUM_mean_delay', 'ORIGIN_AIRPORT_ID_mean_delay', 'DEST_AIRPORT_ID_mean_delay', 'origin_station_id_mean_delay', 'dest_station_id_mean_delay', 'STATION_mean_delay']\n",
    "\n",
    "    # Create a VectorAssembler for float features\n",
    "    float_assembler = VectorAssembler(\n",
    "        inputCols=float_cols,\n",
    "        outputCol=\"float_vector\",\n",
    "        handleInvalid=\"skip\"\n",
    "    )\n",
    "    \n",
    "    # Apply assembler to train and test\n",
    "    df_train_assembled = float_assembler.transform(df_train_processed)\n",
    "    df_test_assembled = None\n",
    "    if df_test_processed is not None:\n",
    "        df_test_assembled = float_assembler.transform(df_test_processed)\n",
    "    \n",
    "    checkpoint_save(df_train_assembled, \"checkpoint_train_assembled\")\n",
    "    df_train_assembled = checkpoint_load(\"checkpoint_train_assembled\")\n",
    "    if df_test_assembled is not None:\n",
    "        checkpoint_save(df_test_assembled, \"checkpoint_test_assembled\")\n",
    "        df_test_assembled = checkpoint_load(\"checkpoint_test_assembled\")\n",
    "\n",
    "    # Fit scaler on training data only\n",
    "    scaler = RobustScaler(\n",
    "        inputCol=\"float_vector\",\n",
    "        outputCol=\"scaled_float_vector\",\n",
    "        withCentering=False,\n",
    "        withScaling=True\n",
    "    )\n",
    "    \n",
    "    scaler_model = scaler.fit(df_train_assembled)\n",
    "    \n",
    "    # Apply scaler to both train and test\n",
    "    df_train_scaled = scaler_model.transform(df_train_assembled)\n",
    "    df_test_scaled = None\n",
    "    if df_test_assembled is not None:\n",
    "        df_test_scaled = scaler_model.transform(df_test_assembled)\n",
    "    \n",
    "    checkpoint_save(df_train_scaled, \"checkpoint_train_scaled\")\n",
    "    df_train_scaled = checkpoint_load(\"checkpoint_train_scaled\")\n",
    "    if df_test_scaled is not None:\n",
    "        checkpoint_save(df_test_scaled, \"checkpoint_test_scaled\")\n",
    "        df_test_scaled = checkpoint_load(\"checkpoint_test_scaled\")\n",
    "\n",
    "    # Final feature assembly for the training model\n",
    "    # Combine scaled float vector with cyclic_onehot_features\n",
    "    final_assembler = VectorAssembler(\n",
    "        inputCols=[\"scaled_float_vector\", \"cyclic_onehot_features\"],\n",
    "        outputCol=\"final_features\",\n",
    "        handleInvalid=\"skip\"\n",
    "    )\n",
    "    \n",
    "    # Apply final assembler\n",
    "    df_train_final = final_assembler.transform(df_train_scaled)\n",
    "    df_test_final = None\n",
    "    if df_test_scaled is not None:\n",
    "        df_test_final = final_assembler.transform(df_test_scaled)\n",
    "    \n",
    "    checkpoint_save(df_train_final, \"checkpoint_train_final\")\n",
    "    df_train_final = checkpoint_load(\"checkpoint_train_final\")\n",
    "    if df_test_final is not None:\n",
    "        checkpoint_save(df_test_final, \"checkpoint_test_final\")\n",
    "        df_test_final = checkpoint_load(\"checkpoint_test_final\")\n",
    "\n",
    "    if resample_method:\n",
    "        if resample_method.lower() == \"smote\":\n",
    "            resampler = SMOTEResampler()\n",
    "        elif resample_method.lower() == \"upsample\":\n",
    "            resampler = UpsampleResampler()\n",
    "        elif resample_method.lower() == \"downsample\":\n",
    "            resampler = DownsampleResampler()\n",
    "        \n",
    "        df_train_final = resampler.transform(df_train_final)\n",
    "        if df_test_final is not None:\n",
    "            df_test_final = resampler.transform(df_test_final)\n",
    "        \n",
    "        checkpoint_save(df_train_final, \"checkpoint_train_final_resampled\")\n",
    "        df_train_final = checkpoint_load(\"checkpoint_train_final_resampled\")\n",
    "        if df_test_final is not None:\n",
    "            checkpoint_save(df_test_final, \"checkpoint_test_final_resampled\")\n",
    "            df_test_final = checkpoint_load(\"checkpoint_test_final_resampled\")\n",
    "\n",
    "    # Define model with optimal hyperparameters\n",
    "    print(\"Creating final model...\")\n",
    "\n",
    "    # modelType.set(**best_params)\n",
    "    # Use Param objects to set the values\n",
    "    for paramMap in best_params.items():\n",
    "        print(f'current paramMap: {paramMap}')\n",
    "        # Get the parameter object by name\n",
    "        param = None\n",
    "        for p in modelType.params:\n",
    "            if p.name == paramMap[0]:\n",
    "                param = p\n",
    "                break\n",
    "        \n",
    "        if param is not None:\n",
    "            # Set the parameter value\n",
    "            modelType.set(param, paramMap[1])\n",
    "        else:\n",
    "            print(f\"Warning: Parameter {paramMap[0]} not found in model\")\n",
    "\n",
    "    # Grab a sample to determine the feature dimension\n",
    "    sample_features = df_train_final.select(\"final_features\").limit(1).collect()[0][0]\n",
    "    feature_size = len(sample_features)\n",
    "    print(f\"Adjusted feature vector size: {feature_size}\")\n",
    "    \n",
    "    # Dynamically update the MLP architecture if it's an MLP model\n",
    "    if isinstance(modelType, MultilayerPerceptronClassifier):\n",
    "        # Get the existing layers\n",
    "        current_layers = modelType.getLayers()\n",
    "        \n",
    "        # Update the first layer to match the feature size\n",
    "        new_layers = [feature_size] + current_layers[1:]\n",
    "        \n",
    "        # Set the updated layers\n",
    "        modelType = modelType.setLayers(new_layers)\n",
    "        print(f\"Updated MLP layers: {new_layers}\")\n",
    "\n",
    "    df_train_final = df_train_final.repartition(200)  # Balanced partitions for training\n",
    "    # Now train the model on the full training dataset\n",
    "    model = modelType.fit(df_train_final)\n",
    "    # Unpersist final training df\n",
    "    df_train_final.unpersist()\n",
    "\n",
    "    # Evaluate on test data (if provided)\n",
    "    metrics = None\n",
    "    if df_test_final is not None:\n",
    "        print(\"Evaluating model on test data...\")\n",
    "        predictions = model.transform(df_test_final)\n",
    "        # Unpersist final test df\n",
    "        df_test_final.unpersist()\n",
    "\n",
    "        evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "            labelCol=\"DEP_DELAY_BINNED\", \n",
    "            predictionCol=\"prediction\", \n",
    "            metricName=\"f1\"\n",
    "        )\n",
    "        \n",
    "        evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "            labelCol=\"DEP_DELAY_BINNED\", \n",
    "            predictionCol=\"prediction\", \n",
    "            metricName=\"accuracy\"\n",
    "        )\n",
    "        \n",
    "        evaluator_precision = MulticlassClassificationEvaluator(\n",
    "            labelCol=\"DEP_DELAY_BINNED\",\n",
    "            predictionCol=\"prediction\",\n",
    "            metricName=\"weightedPrecision\"\n",
    "        )\n",
    "        \n",
    "        evaluator_recall = MulticlassClassificationEvaluator(\n",
    "            labelCol=\"DEP_DELAY_BINNED\",\n",
    "            predictionCol=\"prediction\",\n",
    "            metricName=\"weightedRecall\"\n",
    "        )\n",
    "        \n",
    "        metrics = {\n",
    "            \"f1\": evaluator_f1.evaluate(predictions),\n",
    "            \"accuracy\": evaluator_accuracy.evaluate(predictions),\n",
    "            \"precision\": evaluator_precision.evaluate(predictions),\n",
    "            \"recall\": evaluator_recall.evaluate(predictions)\n",
    "        }\n",
    "        \n",
    "        print(\"Test metrics:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "        \n",
    "        print(\"Model Hyperparameters:\")\n",
    "        for param, value in model.extractParamMap().items():\n",
    "            print(f\"  {param.name}: {value}\")\n",
    "    \n",
    "    # Create final pipeline model that combines all the fitted preprocessing steps and the trained model\n",
    "    pipeline_model = PipelineModel(stages=[stage for stage in [\n",
    "        non_scaling_model,\n",
    "        *category_preprocessors,\n",
    "        onehot_assembler if 'onehot_assembler' in locals() else None,\n",
    "        cyclic_assembler if 'cyclic_assembler' in locals() else None,\n",
    "        combined_assembler if 'combined_assembler' in locals() else None,\n",
    "        target_encoder,\n",
    "        float_assembler,\n",
    "        scaler_model,\n",
    "        final_assembler,\n",
    "        resampler if 'resampler' in locals() else None,\n",
    "        model\n",
    "    ] if stage is not None])\n",
    "        \n",
    "    return pipeline_model, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01b57cd4-3f7b-45ba-b657-6e155b453fab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Full Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "147bdd75-f1b7-4c6a-9a9b-fa013b56307b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_flight_delay_prediction_workflow_mod(df, modelType, paramGrid, resample_method=None):\n",
    "    \"\"\"\n",
    "    Run the complete workflow for flight delay prediction\n",
    "    \n",
    "    This function demonstrates the entire process:\n",
    "    1. Loading data\n",
    "    2. Splitting into train/test\n",
    "    3. Training model with cross-validation\n",
    "    4. Evaluating model\n",
    "    5. Making predictions\n",
    "    \"\"\"\n",
    "    # First, balance the partitions while maintaining chronological order\n",
    "    df = create_balanced_time_ordered_partitions(df, num_partitions=200)\n",
    "\n",
    "    # Perform chronological train-test split (80/20)\n",
    "    print(\"Performing chronological train-test split...\")\n",
    "    # Add a sequential row index\n",
    "    windowSpec = Window.orderBy('YEAR', 'MONTH', 'DAY_OF_MONTH')\n",
    "    df = df.withColumn(\"row_index\", F.row_number().over(windowSpec))\n",
    "    \n",
    "    # Calculate the split point (80% for training)\n",
    "    total_rows = df.count()\n",
    "    train_rows = int(total_rows * 0.8)\n",
    "    \n",
    "    # Split the data\n",
    "    train_data = df.filter(df.row_index <= train_rows).drop(\"row_index\")\n",
    "    test_data = df.filter(df.row_index > train_rows).drop(\"row_index\")\n",
    "\n",
    "    print(f\"Training data size: {train_data.count()} rows\")\n",
    "    print(f\"Test data size: {test_data.count()} rows\")\n",
    "\n",
    "    # Define final input features for model (to be kept post-processing) (optional)\n",
    "    final_feature_list = None\n",
    "    \n",
    "    # Train the model with cross-validation\n",
    "    print(\"Begin model hyperparameter tuning and training...\")\n",
    "    pipeline_model, metrics = train_model_mod(train_data, df_test=test_data, modelType=modelType, paramGrid=paramGrid, use_cross_validation=True, final_feature_list=final_feature_list, resample_method=resample_method)\n",
    "    \n",
    "    # Make predictions on test data (example)\n",
    "    print(\"Making predictions on a sample of test data...\")\n",
    "    \n",
    "    sample_predictions = pipeline_model.transform(test_data.limit(5))\n",
    "    \n",
    "    # Display prediction results\n",
    "    print(\"Sample predictions:\")\n",
    "    sample_predictions.select(\"DEP_DELAY\", \"DEP_DELAY_BINNED\", \"prediction\", \"probability\").show()\n",
    "    \n",
    "    # Clear checkpoints at end of processing\n",
    "    dbutils.fs.rm(folder_path, recurse=True)\n",
    "    print(\"Workflow completed successfully!\")\n",
    "    return pipeline_model, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33fca313-2792-4659-a803-a372c6df37ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Testing/Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f310e2d2-e6f1-4715-9009-2ec791df0419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/mids-w261/OTPW_60M_Backup/_SUCCESS</td><td>_SUCCESS</td><td>0</td><td>1744495305000</td></tr><tr><td>dbfs:/mnt/mids-w261/OTPW_60M_Backup/_committed_6865934896646357313</td><td>_committed_6865934896646357313</td><td>524</td><td>1744495306000</td></tr><tr><td>dbfs:/mnt/mids-w261/OTPW_60M_Backup/_started_6865934896646357313</td><td>_started_6865934896646357313</td><td>0</td><td>1744495305000</td></tr><tr><td>dbfs:/mnt/mids-w261/OTPW_60M_Backup/part-00000-tid-6865934896646357313-d0d2f795-a1f4-4e99-baf6-57bbbfdcabaa-133-1.c000.snappy.parquet</td><td>part-00000-tid-6865934896646357313-d0d2f795-a1f4-4e99-baf6-57bbbfdcabaa-133-1.c000.snappy.parquet</td><td>1385705174</td><td>1744495305000</td></tr><tr><td>dbfs:/mnt/mids-w261/OTPW_60M_Backup/part-00001-tid-6865934896646357313-d0d2f795-a1f4-4e99-baf6-57bbbfdcabaa-134-1.c000.snappy.parquet</td><td>part-00001-tid-6865934896646357313-d0d2f795-a1f4-4e99-baf6-57bbbfdcabaa-134-1.c000.snappy.parquet</td><td>1356469888</td><td>1744496514000</td></tr><tr><td>dbfs:/mnt/mids-w261/OTPW_60M_Backup/part-00002-tid-6865934896646357313-d0d2f795-a1f4-4e99-baf6-57bbbfdcabaa-135-1.c000.snappy.parquet</td><td>part-00002-tid-6865934896646357313-d0d2f795-a1f4-4e99-baf6-57bbbfdcabaa-135-1.c000.snappy.parquet</td><td>1092710849</td><td>1744497469000</td></tr><tr><td>dbfs:/mnt/mids-w261/OTPW_60M_Backup/part-00003-tid-6865934896646357313-d0d2f795-a1f4-4e99-baf6-57bbbfdcabaa-136-1.c000.snappy.parquet</td><td>part-00003-tid-6865934896646357313-d0d2f795-a1f4-4e99-baf6-57bbbfdcabaa-136-1.c000.snappy.parquet</td><td>1045217237</td><td>1744498247000</td></tr><tr><td>dbfs:/mnt/mids-w261/OTPW_60M_Backup/part-00004-tid-6865934896646357313-d0d2f795-a1f4-4e99-baf6-57bbbfdcabaa-137-1.c000.snappy.parquet</td><td>part-00004-tid-6865934896646357313-d0d2f795-a1f4-4e99-baf6-57bbbfdcabaa-137-1.c000.snappy.parquet</td><td>1030144723</td><td>1744499059000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/mids-w261/OTPW_60M_Backup/_SUCCESS",
         "_SUCCESS",
         0,
         1744495305000
        ],
        [
         "dbfs:/mnt/mids-w261/OTPW_60M_Backup/_committed_6865934896646357313",
         "_committed_6865934896646357313",
         524,
         1744495306000
        ],
        [
         "dbfs:/mnt/mids-w261/OTPW_60M_Backup/_started_6865934896646357313",
         "_started_6865934896646357313",
         0,
         1744495305000
        ],
        [
         "dbfs:/mnt/mids-w261/OTPW_60M_Backup/part-00000-tid-6865934896646357313-d0d2f795-a1f4-4e99-baf6-57bbbfdcabaa-133-1.c000.snappy.parquet",
         "part-00000-tid-6865934896646357313-d0d2f795-a1f4-4e99-baf6-57bbbfdcabaa-133-1.c000.snappy.parquet",
         1385705174,
         1744495305000
        ],
        [
         "dbfs:/mnt/mids-w261/OTPW_60M_Backup/part-00001-tid-6865934896646357313-d0d2f795-a1f4-4e99-baf6-57bbbfdcabaa-134-1.c000.snappy.parquet",
         "part-00001-tid-6865934896646357313-d0d2f795-a1f4-4e99-baf6-57bbbfdcabaa-134-1.c000.snappy.parquet",
         1356469888,
         1744496514000
        ],
        [
         "dbfs:/mnt/mids-w261/OTPW_60M_Backup/part-00002-tid-6865934896646357313-d0d2f795-a1f4-4e99-baf6-57bbbfdcabaa-135-1.c000.snappy.parquet",
         "part-00002-tid-6865934896646357313-d0d2f795-a1f4-4e99-baf6-57bbbfdcabaa-135-1.c000.snappy.parquet",
         1092710849,
         1744497469000
        ],
        [
         "dbfs:/mnt/mids-w261/OTPW_60M_Backup/part-00003-tid-6865934896646357313-d0d2f795-a1f4-4e99-baf6-57bbbfdcabaa-136-1.c000.snappy.parquet",
         "part-00003-tid-6865934896646357313-d0d2f795-a1f4-4e99-baf6-57bbbfdcabaa-136-1.c000.snappy.parquet",
         1045217237,
         1744498247000
        ],
        [
         "dbfs:/mnt/mids-w261/OTPW_60M_Backup/part-00004-tid-6865934896646357313-d0d2f795-a1f4-4e99-baf6-57bbbfdcabaa-137-1.c000.snappy.parquet",
         "part-00004-tid-6865934896646357313-d0d2f795-a1f4-4e99-baf6-57bbbfdcabaa-137-1.c000.snappy.parquet",
         1030144723,
         1744499059000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Full OTPW Dataset\n",
    "directory = \"dbfs:/mnt/mids-w261/OTPW_60M_Backup\"\n",
    "display(dbutils.fs.ls(f\"{directory}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "362a435f-a384-4c3e-a323-a47d03af843b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "# df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/mnt/mids-w261/OTPW_3M_2015.csv\")\n",
    "# df = spark.read.option(\"header\", \"true\").csv(f\"dbfs:/mnt/mids-w261/OTPW_12M/OTPW_12M/OTPW_12M_2015.csv.gz\").limit(100)\n",
    "#Load 5 parts of 60M dataset\n",
    "df_60m_part_0 = spark.read.parquet(\"dbfs:/mnt/mids-w261/OTPW_60M_Backup/part-00000-tid-6865934896646357313-d0d2f795-a1f4-4e99-baf6-57bbbfdcabaa-133-1.c000.snappy.parquet\")\n",
    "df_60m_part_1 = spark.read.parquet(\"dbfs:/mnt/mids-w261/OTPW_60M_Backup/part-00001-tid-6865934896646357313-d0d2f795-a1f4-4e99-baf6-57bbbfdcabaa-134-1.c000.snappy.parquet\")\n",
    "df_60m_part_2 = spark.read.parquet(\"dbfs:/mnt/mids-w261/OTPW_60M_Backup/part-00002-tid-6865934896646357313-d0d2f795-a1f4-4e99-baf6-57bbbfdcabaa-135-1.c000.snappy.parquet\")\n",
    "df_60m_part_3 = spark.read.parquet(\"dbfs:/mnt/mids-w261/OTPW_60M_Backup/part-00003-tid-6865934896646357313-d0d2f795-a1f4-4e99-baf6-57bbbfdcabaa-136-1.c000.snappy.parquet\")\n",
    "df_60m_part_4 = spark.read.parquet(\"dbfs:/mnt/mids-w261/OTPW_60M_Backup/part-00004-tid-6865934896646357313-d0d2f795-a1f4-4e99-baf6-57bbbfdcabaa-137-1.c000.snappy.parquet\")\n",
    "df_list = [df_60m_part_0, df_60m_part_1, df_60m_part_2, df_60m_part_3, df_60m_part_4]\n",
    "df = reduce(DataFrame.union, df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb033910-ccfb-4f42-908d-db2e6ee040cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlp_1 = MultilayerPerceptronClassifier(\n",
    "    featuresCol=\"final_features\", # column name of features (vectorized)\n",
    "    labelCol=\"DEP_DELAY_BINNED\", # column name of label\n",
    "    layers=[198, 100, 50, 25, 10, 3],\n",
    "    maxIter=50, \n",
    "    seed=42)\n",
    "\n",
    "mlp_2 = MultilayerPerceptronClassifier(\n",
    "    featuresCol=\"final_features\", # column name of features (vectorized)\n",
    "    labelCol=\"DEP_DELAY_BINNED\", # column name of label\n",
    "    layers=[198, 150, 75, 50, 25, 10, 3],\n",
    "    maxIter=10, \n",
    "    seed=42)\n",
    "\n",
    "mlp_3 = MultilayerPerceptronClassifier(\n",
    "    featuresCol=\"final_features\", # column name of features (vectorized)\n",
    "    labelCol=\"DEP_DELAY_BINNED\", # column name of label\n",
    "    layers= [198, 50, 10, 3],\n",
    "    maxIter= 10,\n",
    "    blockSize= 64, \n",
    "    seed=42)\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"final_features\", # column name of features (vectorized)\n",
    "    labelCol=\"DEP_DELAY_BINNED\", # column name of label\n",
    "    family=\"multinomial\", # multinomial classification\n",
    "    maxIter=200, # max iterations\n",
    "    regParam=0, # regularization parameter\n",
    "    elasticNetParam=0.0, # regularization mix (1.0=L1, 0.0=L2)\n",
    "    \n",
    "    # performance optimizations\n",
    "    aggregationDepth=2, # depth for tree aggregation (higher values = more parallelism)\n",
    "    standardization=False, # standardize features for better convergence\n",
    "    tol=1e-6, # convergence tolerance\n",
    "    fitIntercept=True # include intercept term\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2f79be6-3a67-4b2a-b777-2208aae42afc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlp_1_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(mlp_1.stepSize, [0.05]) \\\n",
    "    .build()\n",
    "mlp_2_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(mlp_2.stepSize, [0.01, 0.05, 0.1]) \\\n",
    "    .build()\n",
    "mlp_3_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(mlp_3.stepSize, [0.01, 0.05, 0.1]) \\\n",
    "    .build()\n",
    "lr_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aebe371-6d73-457e-995c-3ad6d3645aeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define parameters to feed to workflow\n",
    "\n",
    "# Define model type\n",
    "lr = LogisticRegression(\n",
    "        featuresCol=\"final_features\", \n",
    "        labelCol=\"DEP_DELAY_BINNED\"\n",
    "    )\n",
    "\n",
    "# Define paramgrid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(lr.regParam, [0.001]) \\\n",
    "        .addGrid(lr.elasticNetParam, [0.0]) \\\n",
    "        .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89038f97-eaae-4647-87cc-72efe1a9b6ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting flight delay prediction workflow...\nPerforming chronological train-test split...\nTraining data size: 25338495 rows\nTest data size: 6334624 rows\nBegin model hyperparameter tuning and training...\nBeginning cross-validation for hyperparameter tuning...\nCross-validation using sample size: 11995803, partition count: 200\nPreparing data for time-based cross-validation...\nApplying non-scaling preprocessing steps...\nRunning DataPreprocessor...\nPreprocessing 1 rows\nCheckpointing 1 rows to checkpoint_data_preprocessed\nLoading checkpoint: checkpoint_data_preprocessed\nRunning DateTimeProcessor...\nRunning SkyConditionsProcessor...\nRunning OutlierHandler...\nThe current column is DISTANCE, the number of rows dropped is 0, the current number of rows is 1\nThe current column is LATITUDE, the number of rows dropped is 0, the current number of rows is 1\nThe current column is LONGITUDE, the number of rows dropped is 0, the current number of rows is 1\nThe current column is ELEVATION, the number of rows dropped is 0, the current number of rows is 1\nThe current column is HourlyAltimeterSetting, the number of rows dropped is 0, the current number of rows is 1\nThe current column is HourlyDewPointTemperature, the number of rows dropped is 0, the current number of rows is 1\nThe current column is HourlyRelativeHumidity, the number of rows dropped is 0, the current number of rows is 1\nThe current column is HourlyWindDirection, the number of rows dropped is 0, the current number of rows is 1\nThe current column is HourlyWindSpeed, the number of rows dropped is 0, the current number of rows is 1\nThe current column is HourlyCloudBaseHeight, the number of rows dropped is 0, the current number of rows is 1\nRunning MissingValueHandler...\nRunning TargetBinner...\nRunning GraphFeatureProcessor...\nRunning CyclicEncoder...\nRunning DataPreprocessor...\nPreprocessing 1 rows\nCheckpointing 1 rows to checkpoint_data_preprocessed\nLoading checkpoint: checkpoint_data_preprocessed\nRunning DateTimeProcessor...\nRunning SkyConditionsProcessor...\nRunning OutlierHandler...\nThe current column is DISTANCE, the number of rows dropped is 0, the current number of rows is 1\nThe current column is LATITUDE, the number of rows dropped is 0, the current number of rows is 1\nThe current column is LONGITUDE, the number of rows dropped is 0, the current number of rows is 1\nThe current column is ELEVATION, the number of rows dropped is 0, the current number of rows is 1\nThe current column is HourlyAltimeterSetting, the number of rows dropped is 0, the current number of rows is 1\nThe current column is HourlyDewPointTemperature, the number of rows dropped is 0, the current number of rows is 1\nThe current column is HourlyRelativeHumidity, the number of rows dropped is 0, the current number of rows is 1\nThe current column is HourlyWindDirection, the number of rows dropped is 0, the current number of rows is 1\nThe current column is HourlyWindSpeed, the number of rows dropped is 0, the current number of rows is 1\nThe current column is HourlyCloudBaseHeight, the number of rows dropped is 0, the current number of rows is 1\nRunning MissingValueHandler...\nRunning TargetBinner...\nRunning GraphFeatureProcessor...\nRunning CyclicEncoder...\nRunning DataPreprocessor...\nPreprocessing 1 rows\nCheckpointing 1 rows to checkpoint_data_preprocessed\nLoading checkpoint: checkpoint_data_preprocessed\nRunning DateTimeProcessor...\nRunning SkyConditionsProcessor...\nRunning OutlierHandler...\nThe current column is DISTANCE, the number of rows dropped is 0, the current number of rows is 1\nThe current column is LATITUDE, the number of rows dropped is 0, the current number of rows is 1\nThe current column is LONGITUDE, the number of rows dropped is 0, the current number of rows is 1\nThe current column is ELEVATION, the number of rows dropped is 0, the current number of rows is 1\nThe current column is HourlyAltimeterSetting, the number of rows dropped is 0, the current number of rows is 1\nThe current column is HourlyDewPointTemperature, the number of rows dropped is 0, the current number of rows is 1\nThe current column is HourlyRelativeHumidity, the number of rows dropped is 0, the current number of rows is 1\nThe current column is HourlyWindDirection, the number of rows dropped is 0, the current number of rows is 1\nThe current column is HourlyWindSpeed, the number of rows dropped is 0, the current number of rows is 1\nThe current column is HourlyCloudBaseHeight, the number of rows dropped is 0, the current number of rows is 1\nRunning MissingValueHandler...\nRunning TargetBinner...\nRunning GraphFeatureProcessor...\nRunning CyclicEncoder...\nRunning DataPreprocessor...\nPreprocessing 1 rows\nCheckpointing 1 rows to checkpoint_data_preprocessed\nLoading checkpoint: checkpoint_data_preprocessed\nRunning DateTimeProcessor...\nRunning SkyConditionsProcessor...\nRunning OutlierHandler...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a3221c57d84f61a0af3792e60c0ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d817a8ee9f44f0bd0a172f03e46d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DataPreprocessor...\nPreprocessing 11995803 rows\nCheckpointing 11822718 rows to checkpoint_data_preprocessed\nLoading checkpoint: checkpoint_data_preprocessed\nRunning DateTimeProcessor...\nRunning SkyConditionsProcessor...\nRunning OutlierHandler...\nThe current column is DISTANCE, the number of rows dropped is 19251, the current number of rows is 11803179\nThe current column is LATITUDE, the number of rows dropped is 12120, the current number of rows is 11791059\nThe current column is LONGITUDE, the number of rows dropped is 0, the current number of rows is 11791059\nThe current column is ELEVATION, the number of rows dropped is 866545, the current number of rows is 10924514\nThe current column is HourlyAltimeterSetting, the number of rows dropped is 455727, the current number of rows is 10468787\nThe current column is HourlyDewPointTemperature, the number of rows dropped is 8520, the current number of rows is 10460267\nThe current column is HourlyRelativeHumidity, the number of rows dropped is 159, the current number of rows is 10460108\nThe current column is HourlyWindDirection, the number of rows dropped is 482289, the current number of rows is 9977819\nThe current column is HourlyWindSpeed, the number of rows dropped is 2412, the current number of rows is 9975407\nThe current column is HourlyCloudBaseHeight, the number of rows dropped is 1439346, the current number of rows is 8536061\nRunning MissingValueHandler...\nRunning TargetBinner...\nRunning GraphFeatureProcessor...\nRunning CyclicEncoder...\nCheckpointing 5948997 rows to checkpoint_cv_preprocessed\nLoading checkpoint: checkpoint_cv_preprocessed\nCheckpointing 5948997 rows to checkpoint_cv_categorical\nLoading checkpoint: checkpoint_cv_categorical\nCheckpointing 5948997 rows to checkpoint_cv_combined_features\nLoading checkpoint: checkpoint_cv_combined_features\nCreating time-based folds with per-fold scaling...\nCheckpointing 5948997 rows to checkpoint_fold_labeled\nLoading checkpoint: checkpoint_fold_labeled\nProcessing fold 1...\n  Performing target encoding for fold 1...\n  Scaling features for fold 1...\n  Applying upsample resampling for fold 1...\nCounts: 1369725 580160 429713\n+----------------+-------+\n|DEP_DELAY_BINNED|  count|\n+----------------+-------+\n|               0|1369725|\n|               1|1026180|\n|               2|1026800|\n+----------------+-------+\n\nCheckpointing 4017605 rows to checkpoint_fold_1\nLoading checkpoint: checkpoint_fold_1\nProcessing fold 2...\n  Performing target encoding for fold 2...\n  Scaling features for fold 2...\n  Applying upsample resampling for fold 2...\nCounts: 1415271 563715 400613\n+----------------+-------+\n|DEP_DELAY_BINNED|  count|\n+----------------+-------+\n|               0|1415271|\n|               1|1060255|\n|               2|1060756|\n+----------------+-------+\n\nCheckpointing 4131182 rows to checkpoint_fold_2\nLoading checkpoint: checkpoint_fold_2\nCombining all processed folds...\nSetting up cross-validation...\nAdjusted feature vector size for CV: 203\nUpdated MLP layers for CV: [203, 100, 50, 25, 10, 3]\nRunning cross-validation...\nfold 1: train=3422705 rows, test=594900 rows\nfold 2: train=3536282 rows, test=594900 rows\nBest Metric Values: {'f1': 0.4458000549190395, 'accuracy': 0.5294150277357539, 'weightedPrecision': 0.3917219421593771, 'weightedRecall': 0.5294150277357539}\nBest Hyperparameter Values: {'stepSize': 0.05}\nBest hyperparameters: {'stepSize': 0.05}\nBest cross-validation metrics: {'f1': 0.4458000549190395, 'accuracy': 0.5294150277357539, 'weightedPrecision': 0.3917219421593771, 'weightedRecall': 0.5294150277357539}\nBuilding and applying non-scaling preprocessing steps for final model...\nUsing preprocessing pipeline from cross-validation...\nRunning DataPreprocessor...\nPreprocessing 25338495 rows\nCheckpointing 24972456 rows to checkpoint_data_preprocessed\nLoading checkpoint: checkpoint_data_preprocessed\nRunning DateTimeProcessor...\nRunning SkyConditionsProcessor...\nRunning OutlierHandler...\nThe current column is DISTANCE, the number of rows dropped is 40562, the current number of rows is 24931894\nThe current column is LATITUDE, the number of rows dropped is 25699, the current number of rows is 24906195\nThe current column is LONGITUDE, the number of rows dropped is 0, the current number of rows is 24906195\nThe current column is ELEVATION, the number of rows dropped is 1829011, the current number of rows is 23077184\nThe current column is HourlyAltimeterSetting, the number of rows dropped is 952763, the current number of rows is 22124421\nThe current column is HourlyDewPointTemperature, the number of rows dropped is 18386, the current number of rows is 22106035\nThe current column is HourlyRelativeHumidity, the number of rows dropped is 356, the current number of rows is 22105679\nThe current column is HourlyWindDirection, the number of rows dropped is 1019783, the current number of rows is 21085896\nThe current column is HourlyWindSpeed, the number of rows dropped is 5209, the current number of rows is 21080687\nThe current column is HourlyCloudBaseHeight, the number of rows dropped is 3051912, the current number of rows is 18028775\nRunning MissingValueHandler...\nRunning TargetBinner...\nRunning GraphFeatureProcessor...\nRunning CyclicEncoder...\nCheckpointing 12570156 rows to checkpoint_train_preprocessed\nLoading checkpoint: checkpoint_train_preprocessed\nRunning DataPreprocessor...\nPreprocessing 6334624 rows\nCheckpointing 6224874 rows to checkpoint_data_preprocessed\nLoading checkpoint: checkpoint_data_preprocessed\nRunning DateTimeProcessor...\nRunning SkyConditionsProcessor...\nRunning OutlierHandler...\nThe current column is DISTANCE, the number of rows dropped is 10799, the current number of rows is 6214060\nThe current column is LATITUDE, the number of rows dropped is 6656, the current number of rows is 6207404\nThe current column is LONGITUDE, the number of rows dropped is 0, the current number of rows is 6207404\nThe current column is ELEVATION, the number of rows dropped is 422973, the current number of rows is 5784431\nThe current column is HourlyAltimeterSetting, the number of rows dropped is 247627, the current number of rows is 5536804\nThe current column is HourlyDewPointTemperature, the number of rows dropped is 3820, the current number of rows is 5532984\nThe current column is HourlyRelativeHumidity, the number of rows dropped is 58, the current number of rows is 5532926\nThe current column is HourlyWindDirection, the number of rows dropped is 279490, the current number of rows is 5253436\nThe current column is HourlyWindSpeed, the number of rows dropped is 1769, the current number of rows is 5251667\nThe current column is HourlyCloudBaseHeight, the number of rows dropped is 705356, the current number of rows is 4546311\nRunning MissingValueHandler...\nRunning TargetBinner...\nRunning GraphFeatureProcessor...\nRunning CyclicEncoder...\nCheckpointing 3243843 rows to checkpoint_test_preprocessed\nLoading checkpoint: checkpoint_test_preprocessed\nCheckpointing 12570156 rows to checkpoint_train_categorical\nLoading checkpoint: checkpoint_train_categorical\nCheckpointing 3243843 rows to checkpoint_test_categorical\nLoading checkpoint: checkpoint_test_categorical\nCheckpointing 12570156 rows to checkpoint_train_pretarget_assembled\nLoading checkpoint: checkpoint_train_pretarget_assembled\nCheckpointing 3243843 rows to checkpoint_test_pretarget_assembled\nLoading checkpoint: checkpoint_test_pretarget_assembled\nRunning TargetEncoder...\nCheckpointing 12570156 rows to checkpoint_train_target_encoded\nLoading checkpoint: checkpoint_train_target_encoded\nRunning TargetEncoder...\nCheckpointing 3243843 rows to checkpoint_test_target_encoded\nLoading checkpoint: checkpoint_test_target_encoded\nCheckpointing 12570156 rows to checkpoint_train_assembled\nLoading checkpoint: checkpoint_train_assembled\nCheckpointing 3243843 rows to checkpoint_test_assembled\nLoading checkpoint: checkpoint_test_assembled\nCheckpointing 12570156 rows to checkpoint_train_scaled\nLoading checkpoint: checkpoint_train_scaled\nCheckpointing 3243843 rows to checkpoint_test_scaled\nLoading checkpoint: checkpoint_test_scaled\nCheckpointing 12570156 rows to checkpoint_train_final\nLoading checkpoint: checkpoint_train_final\nCheckpointing 3243843 rows to checkpoint_test_final\nLoading checkpoint: checkpoint_test_final\nRunning UpsampleResampler...\nCounts: 7336504 3021421 2212231\n+----------------+-------+\n|DEP_DELAY_BINNED|  count|\n+----------------+-------+\n|               0|7336504|\n|               1|5503574|\n|               2|5500975|\n+----------------+-------+\n\nRunning UpsampleResampler...\nCounts: 1901022 736028 606793\n+----------------+-------+\n|DEP_DELAY_BINNED|  count|\n+----------------+-------+\n|               0|1901022|\n|               1|1424258|\n|               2|1423178|\n+----------------+-------+\n\nCheckpointing 18341053 rows to checkpoint_train_final_resampled\nLoading checkpoint: checkpoint_train_final_resampled\nCheckpointing 4748458 rows to checkpoint_test_final_resampled\nLoading checkpoint: checkpoint_test_final_resampled\nCreating final model...\ncurrent paramMap: ('stepSize', 0.05)\nAdjusted feature vector size: 184\nUpdated MLP layers: [184, 100, 50, 25, 10, 3]\nEvaluating model on test data...\nTest metrics:\n  f1: 0.3228\n  accuracy: 0.3951\n  precision: 0.3057\n  recall: 0.3951\nModel Hyperparameters:\n  blockSize: 128\n  featuresCol: final_features\n  labelCol: DEP_DELAY_BINNED\n  maxIter: 50\n  predictionCol: prediction\n  probabilityCol: probability\n  rawPredictionCol: rawPrediction\n  seed: 42\n  solver: l-bfgs\n  stepSize: 0.05\n  tol: 1e-06\n  layers: [184, 100, 50, 25, 10, 3]\nMaking predictions on a sample of test data...\nRunning DataPreprocessor...\nPreprocessing 5 rows\nCheckpointing 5 rows to checkpoint_data_preprocessed\nLoading checkpoint: checkpoint_data_preprocessed\nRunning DateTimeProcessor...\nRunning SkyConditionsProcessor...\nRunning OutlierHandler...\nThe current column is DISTANCE, the number of rows dropped is 0, the current number of rows is 5\nThe current column is LATITUDE, the number of rows dropped is 1, the current number of rows is 4\nThe current column is LONGITUDE, the number of rows dropped is 0, the current number of rows is 4\nThe current column is ELEVATION, the number of rows dropped is 0, the current number of rows is 4\nThe current column is HourlyAltimeterSetting, the number of rows dropped is 0, the current number of rows is 4\nThe current column is HourlyDewPointTemperature, the number of rows dropped is 0, the current number of rows is 4\nThe current column is HourlyRelativeHumidity, the number of rows dropped is 1, the current number of rows is 3\nThe current column is HourlyWindDirection, the number of rows dropped is 0, the current number of rows is 3\nThe current column is HourlyWindSpeed, the number of rows dropped is 0, the current number of rows is 3\nThe current column is HourlyCloudBaseHeight, the number of rows dropped is 0, the current number of rows is 3\nRunning MissingValueHandler...\nRunning TargetBinner...\nRunning GraphFeatureProcessor...\nRunning CyclicEncoder...\nRunning TargetEncoder...\nRunning UpsampleResampler...\nCounts: 0 0 1\nWarning: One or more classes have zero samples!\nUsing class 2 with 1 samples as reference\nOnly one class has data, returning without upsampling\n+----------------+-----+\n|DEP_DELAY_BINNED|count|\n+----------------+-----+\n|               2|    1|\n+----------------+-----+\n\nSample predictions:\n+---------+----------------+----------+--------------------+\n|DEP_DELAY|DEP_DELAY_BINNED|prediction|         probability|\n+---------+----------------+----------+--------------------+\n|     58.0|               2|       2.0|[0.28443925135746...|\n+---------+----------------+----------+--------------------+\n\nWorkflow completed successfully!\n\nModel training and evaluation complete!\nFinal evaluation metrics: {'f1': 0.32283704989265183, 'accuracy': 0.3951055268889395, 'precision': 0.3057013391103145, 'recall': 0.3951055268889395}\n\nThe model can now be used to predict flight delays.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting flight delay prediction workflow...\")\n",
    "model_pipeline, metrics = run_flight_delay_prediction_workflow_mod(df, modelType=mlp_1, paramGrid=mlp_1_paramGrid, resample_method='upsample')\n",
    "\n",
    "print(\"\\nModel training and evaluation complete!\")\n",
    "print(\"Final evaluation metrics:\", metrics)\n",
    "print(\"\\nThe model can now be used to predict flight delays.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Phase 3 - ML Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}