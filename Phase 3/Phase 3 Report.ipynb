{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b3bca89-a026-4bcf-a38f-951599270c11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Predicting Flight Delays with Multiclass Classification and Machine Learning\n",
    "## Section 3 Team 1 Phase 3 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "637fb047-9ba9-4888-b124-82c5af9c6ed2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Team Members\n",
    "\n",
    "| Name              | Email                          | Picture                 |\n",
    "|------------------|--------------------------------|-------------------------|\n",
    "| Sebastian Rosales | sbsrosales11@berkeley.edu     | <img src=\"https://i.imgur.com/DraML4f.jpeg\" alt=\"Sebastian Rosales\" width=\"200\" />|\n",
    "| Kenneth Hahn     | hahnkenneth@berkeley.edu      | <img src=\"https://i.imgur.com/wKSmQey.png\" alt=\"Kenneth Hahn\" width=\"200\" />|\n",
    "| Benjamin He      | ben_he@berkeley.edu           | <img src=\"https://i.imgur.com/mD1dfkk.png\" alt=\"Benjamin He\" width=\"200\" />   |\n",
    "| Edgar Munoz      | edgarmunoz@berkeley.edu       | <img src=\"https://i.imgur.com/rIVa4Z9.jpeg\" alt=\"Edgar Munoz\" width=\"200\" />    |\n",
    "| Adam Perez       | adperez@berkeley.edu          | <img src=\"https://i.imgur.com/G4bthyO.jpeg\" alt=\"Adam Perez\" width=\"200\" /> |\n",
    "| Kent Bourgoing   | kent1bp@berkeley.edu          | <img src=\"https://i.imgur.com/HWSSgBp.jpeg\" alt=\"Kent Bourgoing\" width=\"200\" />|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f57b75c0-ec49-44bd-846d-bc546c3b3a19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Leader Plan\n",
    "Each phase of this project has designated leaders who coordinate the tasks, monitor progress, and ensure smooth collaboration among all team members. The table below summarizes the phase-by-phase leadership structure. While all team members contribute to each phase, the assigned leaders take primary responsibility for scheduling meetings, resolving blockers, and consolidating deliverables.\n",
    "\n",
    "| **Week 10 (Phase 1)** | **Week 11-12 (Phase 2)**                    | **Week 13-14 (Phase 3)**                      |\n",
    "|--------------------------------|---------------------------------|----------------------------------|\n",
    "|  Kent Bourgoing & Benjamin He | Sebastian Rosales & Adam Perez   | Kenneth Hahn & Edgar Munoz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad182ade-faf1-4704-a503-6c3085c342af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Credit Assignment Plan\n",
    "\n",
    "Each phase of the project includes clearly assigned roles and tasks to ensure fair workload distribution and accountability. The tables below outline who is responsible for each task, along with the task description, start and end dates, and estimated time required for completion.\n",
    "\n",
    "To keep things consistent and easy to read, each bullet point follows this format:\n",
    "\n",
    "`(Start Date – End Date, Estimated Hours): Task Description`\n",
    "\n",
    "For example:  \n",
    "`- (3/19/25–3/22/25, 3hrs): Applied Phase 1 preprocessing for the 12-month OTPW version.`\n",
    "\n",
    "This structure helps the team stay organized and ensures transparency in how time and responsibilities are managed throughout the project.\n",
    "\n",
    "\n",
    "### Phase 3 (Week 13-14)\n",
    "\n",
    "\n",
    "| **Team Member**       | **Primary Role/Responsibility**                                 | **Task Description**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | **Estimated Person-Hours** |\n",
    "|-----------------------|---------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------|\n",
    "| **Sebastian Rosales** | ML Model Exploration and Fine-Tunning   | - *(4/4/25 – 4/13/25, 20 hours)*: Experimented with three MLP architectures (three hidden layers, four hidden layers, and five hidden layers) using a small scale dataset and the 60 month dataset.<br>- *(4/4/25 – 4/13/25, 3 hours)*: Evaluated multiple MLP models using cross-validation on the training set (2015–2018) and assessed generalization using a blind test set (2019).<br>- *(4/11/25 – 4/13/25, 2 hours)*: Selected the optimal model(s) based on evaluation metrics and documented experiment run times and cluster configurations for reproducibility.<br>- *(4/13/25 – 4/14/25, 3 hours)*: Prepare presentation slides. <br>- *(4/14/25 – 4/18/25, 3 hours)*:  Prepare phase 3 report. | 31 hours                   |\n",
    "| **Kenneth Hahn**      | Feature Engineering   | - *(4/4/25 – 4/11/25, 5 hours)*:  Developed new predictive features, including at least one time-based feature (e.g., RFM) and one graph-based feature (e.g., airline PageRank).<br> - *(4/4/25 – 4/11/25, 4 hours)* :  Conducted experiments to assess the impact of these features on model performance and documented the findings.<br> - *(4/4/25 – 4/11/25, 3 hours)*:  Reviewed how different imputation techniques might improve performance.<br> - *(4/4/25 – 4/11/25, 4 hours)*:  Implemented Principal Component Analysis (PCA) to reduce our extremely large vector space.<br> - *(4/4/25 – 4/13/25, 4 hours)*:  Evaluated multiple models (e.g., Random Forests, Gradient Boosted Decision Trees, and MLP) using cross-validation on the training set (2015–2018) and a blind test set (2019).<br> - *(4/11/25 – 4/13/25, 1 hour)*:  Selected the optimal models based on performance metrics and logged experiment times and cluster configurations.<br> - *(4/13/25 – 4/14/25, 3 hours)* :  Prepared presentation slides.<br> - *(4/14/25 – 4/18/25, 3 hours)*:  Prepared Phase 3 report. | 27 hours                   |\n",
    "| **Benjamin He**       | ML Pipeline  | - *(4/4/25 – 4/13/25, 22 hours)*: Improve on pipeline to be more optimal and that takes raw data from ingestion through to final predictions. This pipeline is enhanced to be more modular and scalable to work with much larger 5-year dataset.<br>- *(4/11/25 – 4/14/25, 1 hour)*: Create a diagram that illustrates the entire ML pipeline, including data preprocessing, feature engineering, model training, hyperparameter tuning, and evaluation steps.<br>- *(4/4/25 – 4/13/25, 3 hours)*: Evaluate multiple models (e.g., Random Forests, Gradient Boosted Decision Trees, and MLP) using cross-validation on the training set (2015–2018) and a blind test set (2019).<br>- *(4/11/25 – 4/13/25, 1 hour)*: Select the optimal models based on performance metrics and log experiment times and cluster configurations.<br>- *(4/13/25 – 4/14/25, 1 hours)*: Prepare presentation slides.<br>- *(4/14/25 – 4/18/25, 3 hours)*: Prepare Phase 3 report. | 31 hours                   |\n",
    "| **Edgar Munoz**       | SMOTE with ML Model Exploration and Fine-Tunning    | - *(4/4/25 – 4/11/25, 10 hours)*: Investigate and implement SMOTE or any other similar method to mitigate class imbalance in our dataset. If effective, optimize the method for large-scale use and robust performance on unseen datasets.<br>- *(4/4/25 – 4/13/25, 8 hours)*: Evaluate multiple models (e.g., Random Forests, Gradient Boosted Decision Trees, and MLP) using cross-validation on the training set (2015–2018) and a blind test set (2019).<br>- *(4/11/25 – 4/13/25, 2 hours)*: Select the optimal models based on performance metrics and log experiment times and cluster configurations.<br>- *(4/13/25 – 4/14/25, 3 hours)*: Prepare presentation slides.<br>- *(4/14/25 – 4/18/25, 3 hours)*: Prepare Phase 3 report. | 26 hours                   |\n",
    "| **Adam Perez**        | Feature Engineering   | - *(4/4/25 – 4/11/25, 5 hours)*:  Developed new predictive features, including at least one time-based feature (e.g., RFM) and one graph-based feature (e.g., airline PageRank).<br> - *(4/4/25 – 4/11/25, 4 hours)*:  Conducted experiments to assess the impact of these features on model performance and documented the findings.<br> - *(4/4/25 – 4/11/25, 3 hours)* :  Reviewed how different imputation techniques might improve performance.<br> - *(4/4/25 – 4/11/25, 4 hours)*:  Implemented Principal Component Analysis (PCA) to reduce our extremely large vector space.<br> - *(4/4/25 – 4/13/25, 4 hours)*:  Evaluated multiple models (e.g., Random Forests, Gradient Boosted Decision Trees, and MLP) using cross-validation on the training set (2015–2018) and a blind test set (2019).<br> - *(4/11/25 – 4/13/25, 1 hour)*:  Selected the optimal models based on performance metrics and logged experiment times and cluster configurations.<br> - *(4/13/25 – 4/14/25, 3 hours)* :  Prepared presentation slides.<br> - *(4/14/25 – 4/18/25, 3 hours)*:  Prepared Phase 3 report. | 27 hours |\n",
    "| **Kent Bourgoing**    | Cross-Validation/Hyperparamter Tuning with ML Model Exploration  | - *(4/4/25 – 4/9/25, 4 hours)*: Implemented Random Search and Bayesian Optimization (using the Optuna library) for hyperparameter tuning within our custom cross-validation framework. <br> - *(4/4/25 – 4/11/25, 2 hours)*: Experimented with Grid Search, Random Search, and Bayesian Optimization by comparing their metric performance and runtime efficiency using Blocked Cross-Validation with a Logistic Regression model on the 2015 Q1 dataset. Evaluated the strengths and weaknesses of each method and identified the most effective one for our use case. <br> - *(4/9/25 – 4/13/25, 6 hours)*: Improved the custom cross-validation framework by adding support for scaling airport PageRank features, performing categorical target encoding on high-cardinality variables, and integrating resampling methods (including upsampling, downsampling, and SMOTE). Ensured all transformations were applied only to the training split within each fold to prevent data leakage. <br> - *(4/11/25 – 4/14/25, 6 hours)*: Trained and evaluated a Logistic Regression model with over-sampling and a Random Forest model with over-sampling using 2015–2018 as the training data and 2019 as the test set. <br> - *(4/14/25 – 4/17/25, 6 hour)*: Rescaled the probability outputs of all nine models (Logistic Regression, Random Forest, and MLP models) from our Phase 3 presentation using the equation \\\\(P_{adjusted}(i) = \\frac{P(i)/r_i}{\\sum_jP(j)/r_j}\\\\) where \\\\(r_i\\\\) is the resampling factor for class \\\\(i\\\\). This adjustment was made to address the overprediction of class 2, an issue highlighted by our professor during feedback. Although the rescaling slightly reduced the F1 score (our primary metric), it improved other metrics such as accuracy and recall. The confusion matrix also confirmed a significant reduction in overpredicted class 2 instances compared to the unadjusted outputs. However, it was later discovered that the formula used was incorrect. We then applied the proper recalibration formula  \\\\(P_{adjusted}(c) = \\frac{p'}{p' + \\frac{1 - p'}{w_c}}\\\\), which ultimately improved our F1 score. <br> - *(4/13/25 – 4/14/25, 3 hours)*: Prepare presentation slides.<br> - *(4/14/25 – 4/18/25, 3 hours)*: Prepare Phase 3 report.<br> | 30 hours   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76a20f5b-9e24-45e1-834d-40060404a528",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Abstract\n",
    "Flight delays are a pervasive issue within the commercial aviation sector, adversely affecting operational logistics and passenger satisfaction. Most of us have experienced it firsthand: rushing through a crowded airport only to arrive at the gate and discover that our flight has been delayed. It’s frustrating—and unfortunately, incredibly common. Given the intricate interdependencies that characterize modern air travel—coordinating over 45,000 flights and 2.9 million passengers daily—accurate delay prediction has the potential to deliver substantial operational and customer service benefits. \n",
    "\n",
    "In this project, we develop a flight delay prediction model using a large-scale, multimodal dataset comprising transportation and meteorological data sourced from the U.S. Department of Transportation and the National Oceanic and Atmospheric Administration (NOAA), respectively. Our methodology integrates extensive feature engineering, graph-based airport centrality metrics via PageRank, and cyclic temporal encoding within a distributed computation framework built on Databricks. We framed the task as a three-class classification problem—predicting whether a flight will depart early, on-time, or delayed—while addressing class imbalance through over- and under-sampling techniques, as well as SMOTE. Hyperparameter optimization strategies including grid search, random search, and Bayesian optimization (via Optuna) are evaluated for efficiency and effectiveness. While we experimented with many models and architectures, our final model, a multilayer perceptron (MLP) classifier trained with oversampling and principal component analysis (PCA), achieved the highest test F1 score of 54.6%, with strong performance for the majority “Early” class (Precision = 66.2%, Recall = 80.3%) and moderate success for minority classes (\"On-time\": Precision = 37.1%, Recall = 18.7%; \"Delayed\": Precision = 34.2%, Recall = 30.9%). The pipeline design emphasizes reproducibility and robust cross-validation, employing leakage prevention measures such as fold-specific transformations and target encoding only within training partitions. These results demonstrate the viability of predictive modeling for flight delays and highlight the challenges posed by class imbalance and temporal variability but it’s a promising start to solving a complex and operationally significant challenge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f97cab6a-1eba-49b0-8325-9e35b5262ce8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Project Description \n",
    "![ProjectOverview](https://i.imgur.com/STbWhpy.png)\n",
    "\n",
    "This report presents the development of a flight delay prediction model grounded in the application of machine learning to large-scale, multimodal transportation and meteorological data. The analysis proceeds in several stages, each addressing a critical component of the modeling process and the inherent complexities of predicting flight delays and the broader implications of enhancing travel reliability. \n",
    "\n",
    "We begin by outlining the technical and logistical complexities associated with flight delay prediction, framing the problem within the context of real-world air traffic coordination and data availability. This section also introduces the research objectives and classification framework used throughout the analysis.\n",
    "\n",
    "Subsequently, we detail the data sources that power our predictive model, specifically the On-Time Performance and Weather (OTPW) dataset. This dataset combines structured flight records from the U.S. Department of Transportation with environmental indicators from the National Oceanic and Atmospheric Administration (NOAA). Our exploratory data analysis (EDA) emphasizes domain-aware feature inspection, data quality assessment, and the identification of relevant variables and temporal patterns.\n",
    "\n",
    "The next section discusses the construction of a modular preprocessing pipeline, including feature filtering, imputation, categorical encoding, and robust scaling. Given the temporally ordered nature of flight data, we implement Blocked Time Series Cross-Validation to prevent data leakage and ensure rigorous evaluation. We also introduce a custom cross-validation class that automates fold creation, model evaluation, and metric aggregation in a reproducible and scalable manner.\n",
    "\n",
    "We then present a comprehensive modeling pipeline, evaluating a variety of classifiers ranging from linear models to deep learning architectures. This includes efforts to mitigate class imbalance through over- and under-sampling techniques and SMOTE, as well as incorporating advanced features such as airport centrality metrics derived via PageRank and cyclic temporal encoding. Dimensionality reduction techniques, particularly PCA, are also explored for their impact on model stability and performance.\n",
    "\n",
    "Finally, the report concludes with an in-depth analysis of model performance, including precision-recall tradeoffs across the three target classes, a discussion of error distributions, and an assessment of hyperparameter optimization strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebe194e5-7db7-4c47-a552-6bd7d6f13211",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Problem Context and Business Implications\n",
    "The air travel system is a highly interconnected and dynamic environment, delays in one part of the network can quickly cascade, disrupting flight operations system-wide. Improving the ability to predict these delays not only enhances operational efficiency for airlines—by enabling smarter resource allocation and more proactive scheduling—but also significantly improves the passenger experience by reducing uncertainty and allowing for better travel planning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56a6e58e-1573-449e-9228-f26389d4cdcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "The dataset we utilized for the purposes of this project was compiled by the join of three distinct datasets, the Flights dataset published by [Bureau of Transportation Statistics](https://www.transtats.bts.gov/Fields.asp?gnoyr_VQ=FGJ), the Weather dataset published by [the National Centers for Environmental Information](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ncdc:C00679), and the Airport Stations with Neighbors dataset published by the US Department of Transportation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc441b3d-3cfd-4bbc-b621-edb47c3b8a07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Dataset Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b084934e-b628-48a1-80b3-43e55a492071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Flights Dataset\n",
    "\n",
    "For the purposes of this exploratory data analysis we utilized the 1-year version dataset which consists of flight data for the entire 2019 calendar year. We found that this dataset contains 109 columns with 14,844,074 rows. This dataset provides us with important information with regards to the date and time of each flight, their carrier airline information, aircraft IDs, locations, destinations, traveled distances, and our target variable, departure delays.\n",
    "\n",
    "To begin, we conducted a missing value analysis which consisted of calculating the percentage of missing values for each column. Overall, we found that 48 columns had a missing value percentage above 95% which essentially renders them unsuitable for our project. On the contrary, we found that 40 columns had a missing value percentage below 1%. The missing value percentages for the remaining 21 columns are shown below:\n",
    "\n",
    "![FlightMissingValues](https://i.imgur.com/0hmCcwz.png)\n",
    "\n",
    "From the figure above we can see that the missing value percentages for most of the remaining columns are still very low, sitting at around the 2% mark. Therefore, these columns are still useful for our project. However, we can also see that there are 5 columns with missing value percentages above the 80% mark. These are the ``CARRIER_DELAY``, ``WEATHER_DELAY``, ``NAS_DELAY``, ``SECURITY_DELAY``, and ``LATE_AIRCRAFT_DELAY`` columns which correspond to a further description of the cause for delay. Due to their high missing value percentage these columns are not very useful for our project. Additionally, these are not values we can use to predict delays since they are only recorded once a delay has already occurred. \n",
    "\n",
    "To further our analysis, we compared delay averages for the time based columns.\n",
    "\n",
    "![AverageDelaybyTime](https://i.imgur.com/ddN5Ao3.png)\n",
    "\n",
    "The figure shown above displays how far the average delay by each measure deviates from the overall average delay, which is about 11 minutes. The measures shown in the figure are by quarter, month, day of the month, and day of the week. Each measure, providing more detail for the previous ones. For example, when looking at the quarterly chart in the top left, we may think that traveling in Q1 will result in a smaller delay. However, once we look at the monthly chart we can see that traveling in April or May can actually be better than traveling in February. Additionally, we can further break it down to decide which days of the month or even days of the week are better to travel in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3df9e61a-64d0-438b-8675-2e3b71a0dfb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Weather Dataset\n",
    "\n",
    "Similarly to the flights dataset, we utilized the 1-year version of the weather dataset which consists of data for the entire 2019 calendar year. The weather dataset consisted of 124 columns and 131,937,550 rows. This dataset provides us climatic and geographic values for weather stations at the hourly, daily, and monthly level. The climatic values include temperature, precipitation, pressure, humidity, wind conditions, sky conditions, altimeter settings, and visibility. The geographic values include latitude, longitude, elevation, state and country. \n",
    "\n",
    "Conducting our missing value analysis we found that 97 columns had missing value percentages above the 95% mark. This included colums such as ``MonthlyDewpointTemperature``, ``MonthlyWetBulb``, ``MonthlyTotalSnowfall``, ``MonthlyGreatestSnowDepth`` to name a few. Given the large percentage of missing values these columns are likely not useful for our project. On the other end, we found that 9 columns had less than 1% of values missing. This included columns such as ``LATITUDE``, ``LONGITUDE``, ``ELEVATION``, ``NAME``, ``STATION`` to name a few. Below we display the 18 columns remaining.\n",
    "\n",
    "![MissingValuesWeather](https://i.imgur.com/TPiVp8W.png)\n",
    "\n",
    "From the figure above we can see that we have a high range of missing value percentages. We have 7 columns with missing value percentages below 35%, which means that we can more reliably apply imputation techniques without severely affecting their underlying relationships. On the other end, we see missing value percentages that range from around 45% up to nearly 95%. We likely won't be able to reliably impute these columns and will have to drop them, unless we find that missing values are representative of the lack of occurance for an event. For example, we found that for ``HourlyPrecipitation`` a missing value means that there was 0 precipitation observed, enabling us to replace missing values with zeros. \n",
    "\n",
    "\n",
    "We also conducted a sanity check on our dataset by visualizing our time features with the average hourly temperatures. \n",
    "\n",
    "![WeatherChart](https://i.imgur.com/KBkvwJ6.png)\n",
    "\n",
    "As expected we can see that the average hourly temperature during the middle quarters and months tend to be higher than usual and the winter/spring months tend to be colder than usual. Overall, this figure serves as a quick sanity check and proof of the data integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdf4ffdc-b1cd-4117-bd30-ce6bc1886f3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Stations Dataset\n",
    "\n",
    "The stations dataset we are utilizing provides airport metadata that can be useful when analyzing location and distance from other airports. This dataset has 5,004,169 rows and 12 columns. It includes features such as airport ID, latitude, longitude, and distance to neighbors.\n",
    "\n",
    "When conducting the same missing values analysis as with previous datasets we found that there were no missing values. \n",
    "\n",
    "In addition to the missing values analysis, we also decided to look at the overall distribution of average distances from neighbors. \n",
    "\n",
    "![StationsChart](https://i.imgur.com/4wDmqt5.png)\n",
    "\n",
    "The distribution seems to be right skewed since most of the average distances are around the 1000 to 1500 mile range, while some range up to 4000 miles. This could be an interesting feature to include in our predictions since it is possible that greater delays occur when traveling to further airports.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79c7ba17-8f81-4d03-a120-7a068ab5343c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### OTPW Dataset\n",
    "\n",
    "For the first quarter of 2015, our joined dataset contains __1,401,363 records__ and __216 different features__. However, our 1-year joined dataset spans the entirity of the 2015 calendar year and contains __5,811,854 records__ and __216 different features__. Our 60 month dataset spanning 2015-2019 contains __31,197,330 records__ and __214 different features__. Because our goal is to predict departure delays before they occur, the dataset was joined by first calculating the flight departure time and subtracting four hours from it, calling this the `four_hours_prior_depart_UTC` column. It joins this new column with the nearest weather `DATE` column, which is a datetime feature for the hourly weather report. Not only that, but the datasets for the weather and flights are also joined by the closest weather station and departing airport, based on the minimum distance between the two, utilizing the latitude and longitude coordinates for both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30636c6b-3d0c-459b-b18f-6834014f747e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Data Dictionary  \n",
    "\n",
    "\n",
    "Our final data dictionary of the remaining columns is displayed below, after narrowing down our features based on the missing values threshold of 30% and further processing. \n",
    "\n",
    "|Feature|DataType|Explanation|  \n",
    "|-|-|-|\n",
    "|DEP_DELAY (LABEL)|INT|The number of min that a flight is delayed.|\n",
    "|DEP_DEL15 (LABEL)|INT|An indicator variable whether the flight was delayed >= 15 min.|\n",
    "|QUARTER|INT|The fiscal quarter when the flight took place.|\n",
    "|DAY_OF_MONTH|INT|The day of month of the flight.|\n",
    "|DAY_OF_WEEK|INT|The day of the week of the flight.|\n",
    "|OP_UNIQUE_CARRIER|STR|The unique identifier of the airline.|\n",
    "|TAIL_NUM|STR|The id of the airplane.|\n",
    "|OP_CARRIER_FL_NUM|INT|The flight number.|\n",
    "|ORIGIN_AIRPORT_ID|STR|The unique airport id of the origin airport.|\n",
    "|ORIGIN_STATE_ABR|STR|The 2-letter abbreviation for the US state in which the origin airport resides.|\n",
    "|DEST_AIRPORT_ID|STR|The unique airport id of the destination airport.|\n",
    "|DEST_STATE_ABR|STR|The 2-letter abbreviation for the US state in which the destination airport resides.|\n",
    "|DISTANCE|INT|Distance between airports (miles).|\n",
    "|YEAR|INT|year of flight.|\n",
    "|MONTH|INT|month of flight.|\n",
    "|origin_station_id|STR|unique identifier for the weather station nearest to the origin.|\n",
    "|origin_station_dis|FLOAT|distance btwn the origin airport and the closest weather station (miles).|\n",
    "|origin_type|STR|whether origin airport is a small, medium, or large airport|\n",
    "|dest_station_id|STR|unique identifier for the weather station nearest to the destination.|\n",
    "|dest_station_dis|FLOAT|distance btwn the destination airport and the closest weather station (miles).|\n",
    "|dest_type|STR|whether dest airport is a small, medium, or large airport|\n",
    "|sched_depart_date_time_UTC|STR|datetime of the scheduled departured time in UTC.|\n",
    "|four_hours_prior_depart_UTC|STR|datetime of the scheduled departure time in UTC minus four hours.|\n",
    "|two_hours_prior_depart_UTC|STR|datetime of the scheduled departure time in UTC minus two hours.|\n",
    "|STATION|STR|unique id of the nearest weather station.|\n",
    "|DATE|STR|the datetime taken from the nearest weather station (taken 4 hours UTC before scheduled departure time)\n",
    "|LATITUDE|FLOAT|latitude of the nearest weather station to the origin airport.|\n",
    "|LONGITUDE|FLOAT|longitude of the nearest weather station to the origin airport.|\n",
    "|HourlyAltimeterSetting|FLOAT|atmospheric pressure at sea level (in Hg).|\n",
    "|HourlyPrecipitation|FLOAT|amount of precipitation (in).|\n",
    "|HourlyPressureChange|FLOAT|differences in pressure over the past 3 hours (in Hg).|\n",
    "|HourlySkyConditions|STR|A report of each cloud layer in the format ccc:ll-xxx where ccc is coverage, ll is layer amount in eighths of sky covered by cloud, and xxx is cloud base height at lowest point of observation (in hundreds of feet 50 = 5000 ft).|\n",
    "|HourlyStationPressure|FLOAT|atmospheric pressure observed at the weather station (in Hg).|\n",
    "|HourlyVisibility|FLOAT|horizontal distance an object can be seen (miles).|\n",
    "|HourlyWetBulbTemperature|FLOAT|wet bulb temperature (degrees F).|\n",
    "|HourlyWindDirection|FLOAT|direction of wind with 360 = true north and 180 = south. 000 = calm wind.|\n",
    "|HourlyWindSpeed|FLOAT|wind speed (mph).|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "447d3051-59de-4eca-8b58-8004ba4d5e4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Summary Statistics and Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1518c4c-619f-465e-a91a-c3c0f69a03e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 3-Month \n",
    "\n",
    "![3MonthSummaryTable](https://i.imgur.com/uyJH8sm.png)\n",
    "\n",
    "The table shown above displays the summary statistics for our 3-Month OTPW joined dataset. One thing to note is that all the variables are on different scales. This doesn't come as a surprise given that we are measuring a variety of different metrics such as temperature, distance, pressure, temperature, etc. Therefore, moving forward it is crucial that we standardize our dataset to avoid poor model performance. From the summary statistics, we can also see we have features such as ``HourlyPrecipitation`` and our target variable ``DEP_DELAY`` that are strongly skewed. Their distributions are shown below:\n",
    "\n",
    "![3MonthDistribution](https://i.imgur.com/TDwVfOv.png)\n",
    "\n",
    "In addition to the summary statistics we also analyzed the correlations between all of our numeric features. \n",
    "\n",
    "![3MonthCorrelation](https://i.imgur.com/gdc0Ya5.png)\n",
    "\n",
    "In the correlation matrix above, we noticed a strong positive correlation between `DISTANCE` and `DISTANCE_GROUP` which makes sense because `DISTANCE_GROUP` is calculated from binning `DISTANCE` into 250-mile ranges. There was also strong negative correlation between `HourlyStationPressure` and `ELEVATION` which also makes sense as there is an inverse physical relationship between elevation (from sea level) and atmospheric pressure. Furthermore, there was a strong positive correlation between `HourlyAltimeterSetting` and `HourlySeaLevelPressure` because both features essentially measure atmospheric pressure at sea level. The last strong correlation we noticed was between `HourlyDewPointTemperature`, `HourlyDryBulbTemperature`, and `HourlyWetBulbTemperature` likely due to all three features being measures of atmospheric temperature and moisture content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4ea262d-8be3-4f9e-aea4-95f12f9d25b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 12-Month\n",
    "![12MonthSummaryTable](https://i.imgur.com/yHWSzhx.png)\n",
    "\n",
    "The table shown above displays the summary statistics for the 12-Month OTPW joined dataset. The summary statistics seem to be very similar to the 3-Month OTPW version. \n",
    "\n",
    "![12MonthDistributions](https://i.imgur.com/PItCXtQ.png)\n",
    "\n",
    "The figure above displays the distribution for the ``HourlyPrecipitation`` and ``DEP_DELAY`` columns. From the figure, we can see there are a few differences from the 3-month dataset, in particular we can see that the mean of ``DEP_DELAY`` dropped by about 1 minute which suggests that seasonality plays a role in departure delays. We can also see that the maximum ``HourlyPrecipitation`` increased from 1.69 to 5.76, demonstrating that seasonality affects weather patterns. Despite the slight differences, the 3-month OTPW subset seems to be somewhat representative of the underlying distributions across an entire year.\n",
    "\n",
    "Similarly to the 3-Month OTPW, we generated a correlation matrix for our numeric features.\n",
    "\n",
    "![12MonthCorr](https://i.imgur.com/HNNzF8S.png)\n",
    "\n",
    "The correlation matrix shown above maintains most of the colinearity we saw from the 3-month version. However, some notable differences are that `LONGITUDE` lost almost all of its correlation to `HourlyDewPointTemperature`, `HourlyDryBulbTemperature`, and `HourlyWetBulbTemperature`. This is likely because we introduced more seasons with the year round dataset, which naturally introduces more temperature ranges. Similarly, `LATITUDE` lost some correlation to `HourlyDewPointTemperature`, `HourlyDryBulbTemperature`, and `HourlyWetBulbTemperature`, but not all. This is likely because latitude tends to be more indicative of temperature than longitude since, generally speaking, the closer a location is to the equator the warmer it is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41600152-9198-4b98-94ad-9d6f1d72bedf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Target Variable Analysis\n",
    "\n",
    "##### Class Definition\n",
    "\n",
    "Given that we are interested in solving a classification problem we binned all of our departure delay times into classes. Ultimately, we decided that we wanted to focus on a 3-class problem that categorizes departure delay times into \"EARLY\", \"ON-TIME\", and \"DELAYED\". To define these classes we decided that any delay that is less than 0 minutes is \"EARLY\", any delay that is between 0 and 15 minutes (inclusive) is \"ON-TIME\", and any delay that is greater than 15 minutes is \"DELAYED\" following the definition from the Department of Transportation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "391b99be-df65-4165-a8e8-5b5495b1997b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### 3-Month Class Distribution\n",
    "\n",
    "Once we determined the classes we decided to look at our class distribution to further understand our problem.\n",
    "\n",
    "![binned dept delay](https://i.imgur.com/604w6a4.png)\n",
    "\n",
    "From the class distribution above, we observe a significant class imbalance, particularly between early flights and the other two classes. Since we assume the labels in the dataset are accurately recorded, this class distribution likely reflects real-world flight delay patterns. Hence, we aim to define a model that will accurately represent this distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6433d23b-e92e-497a-98e4-22335ba90a72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 12-Month Class Distribution\n",
    "\n",
    "We generated the same class distribution figure to discover if there are any important differences between our class distributions going from the 3 month dataset to the 12 month dataset. \n",
    "\n",
    "![12MonthClassDist](https://i.imgur.com/XdNrwNc.png)\n",
    "\n",
    "From the class distribution above, we still observe a very similar class imbalance as we did before. The \"EARLY\" class is still by far the most common class representing 57.2% of all flights. The \"ON-TIME\" class is the second most common occurrence representing 25.1% of flights and \"DELAYED\" is the rarest occurrence representing 17.8% of flights. These splits suggest that our 3-Month class distribution is fairly representative of the 12-Month class distribution. Additionally, we confirmed that combining the \"EARLY\" and \"ON-TIME\" classes would lead to an even greater class imbalance that would represent a significant challenge when training our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3800367b-084c-4bfb-a189-3057ef93c970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Class Occurance by Categorical Features\n",
    "\n",
    "In addition to the analysis conducted above, we were interested on whether certain categorical features can enable us to efficiently predict flight delay classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "239e4954-ac77-47c6-9010-94745b51f99f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Tail Number\n",
    "\n",
    "**3-Month**\n",
    "\n",
    "![3MonthTailNumber](https://i.imgur.com/FQbIl5U.png)\n",
    "\n",
    "The figure above shows the top 30 highest percentages (minimum 5 flights) of occurance for each class by tail number. From the figure, we can see that `TAIL_NUM` might be a good indicator of whether a flight will be early given that we have at least 30 tail numbers that had a early classification percentage of about 80%. Additionally, we can see that 6 tail numbers had delayed flights above 60% of the time. This suggests that there might be some aircraft with specific tail numbers that are less reliable.\n",
    "\n",
    "**12-Month** \n",
    "\n",
    "![12MonthTailNumber](https://i.imgur.com/JPBVZbC.png)\n",
    "\n",
    "The figure above is the same as plot as before but with the 12 month dataset. We can see that these plots still have very similar shapes, however some of the tail numbers at the top have changed. For example, we can see that the tail number \"N177DN\" went from being the fourth highest delayed percentage to first. This could indicate that either this aircraft became more unreliable as the year went on or perhaps there are additional factors, such as weather, that affect the reliabilty of these aircraft. Overall, we believe this categorical feature can be indicative of when a flight will be delayed, early, or on time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19ee478e-a933-4b79-a31d-3d6a350535a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Origin Airport ID\n",
    "\n",
    "**3-Month**\n",
    "\n",
    "![3MonthAirportID](https://i.imgur.com/aN2uo9I.png)\n",
    "\n",
    "The figure above shows the top 30 highest percentages (minimum 5 flights) of occurance for each class by Origin Airport ID. From the figure, we can see that `ORIGIN_AIRPORT_ID` might be a good indicator of whether a flight will be early given that we have at least 30 tail numbers that had an early classification percentage of above the 75% mark. However, unlike tail numbers this feature doesn't seem to be a strong indicator of the likelihood for delays since the highest delayed percentage is about 40% and most of the top 30 delayed percentages are close to the 25% mark. \n",
    "\n",
    "**12-Month** \n",
    "\n",
    "![12MonthAirportID](https://i.imgur.com/k7qGCki.png)\n",
    "\n",
    "The figure shown above is the same as before but with the 12 month dataset. We can see that both plots have basically the same shape. `ORIGIN_AIRPORT_ID` still seems to be a good indicator of when a flight will be early, but when it comes to the delayed or on time percentages it becomes harder to reliably determine if a flight will be delayed simply based on the airport."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc8b83a7-7b55-40a3-b83c-a35e5e177000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "269cd528-be3a-4c84-a279-8dfd38724a70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Temporal Features  \n",
    "\n",
    "For initial feature engineering, we first handled our temporal features. The full list of temporal features included the following: `QUARTER`, `DAY_OF_WEEK`, `DAY_OF_MONTH`, `MONTH`, `sched_depart_date_time_UTC`, `four_hours_prior_depart_UTC`, `two_hours_prior_depart_UTC`, and `DATE` (a datetime variable for the weather station data nearest to the respective origin airport). Because we have all date features as separate columns, we realistically only need the hour and the minute from the various datetime columns (`sched_depart_date_time_UTC`, `four_hours_prior_depart_UTC`, `two_hours_prior_depart_UTC`, and `DATE`). As a result, we first split these datetimes into an hour and a minute column for each datetime variable.  \n",
    "\n",
    "The benefit of temporal features is that all of the variables listed above happen in cycles, e.g. minutes happen in cycles of 60, months happen in cycles of 12, etc. We can further capture this periodicity by utilizing cyclic encoding and creating two different sine and cosine features relative to the time period. This is beneficial because it allows our model to understand the relative closeness of time on a continuous spectrum. For example, the hours 23 and 0 to a machine learning model would seem very spaced apart, when in reality, they are only 1 hour apart. It is also beneficial because this transformation inherently scales our time features to the same scale, ranging from [-1,1]. This prevents any specific temporal feature from dominating in the models, specifically when we hope to implement the use of neural networks. The equation for this transformation is shown below:\n",
    "\n",
    "$$t_{sin} = \\sin(\\frac{2\\pi{}t}{max(t)})$$\n",
    "$$t_{cos} = \\cos(\\frac{2\\pi{}t}{max(t)})$$  \n",
    "\n",
    "We chose this method, as opposed to one-hot encoding the date and time variables, in order to preserve the cyclical pattern of these features, where a one hot encoding may lead our model to believe that each of the different columns are independent of one another with no relationship. On the otherhand, if we kept the columns as is, we hypothesized that our model may not fit the data as well due to the discontinuities in these cycles, as mentioned earlier. By performing this nonlinear transformation, our more complex models such as the Multilayer Perceptron Classifier should be able to pick up on the pattern of time-series data to better find the relationship with departure delays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aad9f5d6-7e3c-43b4-b2ff-991ec2490fde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Decoding HourlySkyConditions  \n",
    "\n",
    "HourlySkyConditions is a feature provided from the weather dataset that overall encapsulates the conditions of the cloud coverage for a given hour. It can have up to three different cloud layers, each with format ccc:ll-xx, where ccc is a three letter indicator for the cloud coverage (CLR=clear, OVC=overcast, etc.), ll is the layer amount, or the amount of sky that is covered by clouds (measured in eighths of the sky covered), and xx is a numeric value that represents the base elevation of the lowest layer of clouds (measured in hundreds of feet).   \n",
    "\n",
    "To assist our model, we decided to separate the information on the cloud layers by having a separate feature for the cloud coverage, layer amount, and cloud base height. Based on the data dictionary provided by the National Oceanic and Atmospheric Administration Repository, out of the three cloud layers \"the full state of the sky can best be determined by the contraction given for the last layer.\" As a result, we extracted the last layer from each value, and then split the string into three separate features, `HourlyCloudCoverage` (str), `HourlyLayerAmount` (str), `HourlyCloudBaseHeight` (int).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49b409d4-b31c-4b9d-90b3-a065668940f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Imputation of Missing Values  \n",
    "\n",
    "With the prior features transformed, we are left with the following features that have missing values in descending order for the 60 month OTPW dataset:\n",
    "\n",
    "![missing values table](https://i.imgur.com/yfxdZcO.png)  \n",
    "\n",
    "The below preprocessing is how we approached missing values for our features:\n",
    "\n",
    "- `HourlyPrecipitation`: We filled in the null values with 0, as defined in the data dictionary for the weather dataset.  \n",
    "\n",
    "For the remaining features, we found that removing null values was the best option. After dropping all null values, we end up losing `9.65%` of data; however, this still left us with 28 million rows of flight and weather information. We believed that this was more than sufficient data for our final model and would allow us to better utilize our time with exploration of different models and further feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8d733f6-52b1-467f-8336-fdc02c4f9fe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Outlier Removal\n",
    "\n",
    "With all null values handled, we performed an outlier removal based on the following columns, `HourlyWindDirection`, `LONGITUDE`, `LATITUDE`, `HourlyRelativeHumidity`, `ELEVATION`, `DISTANCE`, `HourlyDewPointTemperature`, `HourlyCloudBaseHeight`, `HourlyWindSpeed`, and `HourlyAltimeterSetting`. To conduct the outlier removal we implemented a custom formula given that we were only interested in removing extreme outliers:\n",
    "\n",
    "$$\n",
    "IQR = Q_1 - Q_3 \n",
    "$$\n",
    "\n",
    "$$\n",
    "Lower Outlier = Q_1 - (3 \\cdot IQR)\n",
    "$$\n",
    "\n",
    "$$\n",
    "Higher Outlier = Q_3 + (3 \\cdot IQR)\n",
    "$$\n",
    "\n",
    "This formula enabled us to identify extreme outliers to filter. Ultimately, after performing this on the 60 month dataset we had roughly 23 million rows remaining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1930a87-9781-4979-95aa-be1e1e0d40ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Categorical and Numeric Feature Engineering  \n",
    "\n",
    "With all features now rid of missing values and outliers, we went about manipulating our categorical and numeric features in order to be useable for machine learning models.  \n",
    "\n",
    "#### Categorical Features  \n",
    "\n",
    "We first analyed our categorical features. The below table shows all of our categorical variables along with the number of unique categories (`num_unique`), the most frequent category (`most_freq_cat`), and the count of how many times the most frequent category shows up (`most_freq_count`). In total, there are 13591 unique categories when summed up, which would lead to an extremely sparse vector space. For this phase, instead of one-hot encoding all of our categorical features, leading to high cardinality, we first split these features into two different buckets: highly cardinal features and low cardinal features. We defined any feature as being highly cardinal as any feature with more than 100 unique categories: `ORIGIN_AIRPORT_ID`, `TAIL_NUM`, `OP_CARRIER_FL_NUM`, `DEST_AIRPORT_ID`, `origin_station_id`, `dest_station_id`, `STATION`. The remainder were considered as low cardinal features.\n",
    "\n",
    "![category table](https://i.imgur.com/aob6QJ5.png)  \n",
    "\n",
    "We then set aside `ORIGIN_AIRPORT_ID` and `DEST_AIRPORT_ID` to be used for our graph based feature (refer to `Graph Based Feature` section). For the remaining highly cardinal features, we utilized a method called [target encoding](https://towardsdatascience.com/dealing-with-categorical-variables-by-using-target-encoder-a0f1733a4c69/), where we found the mean `DEP_DELAY` (departure delay time) for each unique category and mapped this mean time back to the category in our train and test data. This allows us to have a numeric representation for our categorical features, without increasing the cardinality of our final dataset. With this in mind, since we are utilizing the target variable, there is a high risk of data leakage, so we ensured that we calculated the mean delay time per category **only on the training data** and transformed this information for the training and test sets (see more on `Data Leakage` section). \n",
    "\n",
    "For the remainder of our categorical features, we still used one-hot encoding. This leads to 149 additional features once we one-hot encoded. Although the feature space is still very sparse, we deemed this as acceptible still due to the vast amount of data (23 million rows). We will also incorporate PCA as a dimensionality reduction technique to understand whether a smaller feature space will lead to improved results.\n",
    "\n",
    "#### Numeric Features  \n",
    "\n",
    "For our numeric features, we decided on utilizing a RobustScalar in order to scale our values. A robust scalar utilizes the median and the interquartile range to reduce the scale of each of our numeric observations. We chose to implement a RobustScalar, as opposed to a StandardScalar due to our observation of outliers for many of our numeric columns from Phase 1 exploratory data analysis. By choosing to utilize the median and interquartile range for scaling, we ensure that our distributions are not significantly shifted by the presence of these outliers. We do understand there is a limitation in scaling data such as geographic numeric features (Latitude, Longitude, and Elevation); however, we do not want these geographic features to dominate over others, impacting performance for models that utilize gradient descent. As a result, we will scale them similarly. With these transformations, we have **221 different features**, significantly reducing the cardinality of our data from phase 2, where we had over 13,000 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "455e6f55-cefe-458c-8b58-33b935acd038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Graph Based Feature\n",
    "\n",
    "As mentioned in the previous section on categorical features, we are utilizing `ORIGIN_AIRPORT_ID` and `DEST_AIRPORT_ID` as our graph based features. We constructed the graph using the origin airport and the destination airport columns in order to have a directed graph, where the nodes consisted of all the airports and the edges showed the flights from origin to destination. We then calculated the number of flights that occurred from one airport as a way to weight our edges. By doing so, our graph is now a directed and weighted graph that we can then calculate the PageRank on. By calculating the PageRank, using the GraphFrames library, we can now have a score for each airport that's based on the connections of outgoing and incoming flights, but also it will account for the frequency of the flights as well. In this manner, we believe that the PageRank will allow our model to understand more about the airport information based on how \"popular\" the airport is and make a better determination of whether it will be delayed or not.\n",
    "\n",
    "Below is a visualization of a small subsample of our graph on the 60 month dataset:\n",
    "\n",
    "![graph](https://i.imgur.com/QxpPRxh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "493e4776-07d4-44eb-917d-d89a8e9dc6a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### LASSO Feature Selection (Carryover From Phase 2)\n",
    "\n",
    "**NOTE:** This section is carried over from the previous phase. We found that Lasso feature selection did not lead to significantly improved results, so we did not carry it over for phase 3; however, we believe it is still fruitful to leave this information in this report to delineate what we have already attempted.\n",
    "\n",
    "Now our data is mostly prepared for modeling. One experiment that we ran for this phase is testing the difference between Lasso selected features vs. keeping our whole vector space. In order to do so, we utilized Logistic Regression with lasso regularization strength of 0.001 to see which coefficients remained. Out of 13269 features, the regularization kept 38 weights, listed below.  \n",
    "\n",
    "- Numeric Features: `DISTANCE`, `LONGITUDE`, `HourlyRelativeHumidity`, `HourlyVisiblity`, `HourlyWindSpeed`\n",
    "- Temporal Features: `QUARTER_sin`, `MONTH_cos`, `sched_depart_hour_UTC_cos`, `four_hours_prior_depart_hour_UTC_sin`, `station_hour_UTC_sin`\n",
    "- Airlines Carriers: `WN`, `DL`, `OO`, `EV`, `UA`, `AS`, `NK`, `HA`\n",
    "- Origin Airport IDs: `10397`, `11298`, `10821`, `11259`\n",
    "- Origin Airport State Abbreviations: `IL`, `GA`, `HI`, `MD`, `NY` \n",
    "- Origin Weather Station ID: `72219013874`, `72259003927`, `72406093721`, `72258013960`\n",
    "- Origin Airport Type: `large_airport`\n",
    "- HourlyCloudCoverage: `OVC`\n",
    "- HourlyCloudLayerAmount: `8`  \n",
    "\n",
    "Out of the categories and features chosen, it seems that the regularization tended to favor categorical features that related with the **origin** airport as opposed to the destination airport, which aligns with intuition since the departure delays should predominantely be affected by weather conditions or other factors occurring at the origin airport. Adding onto our intuition, it seems that the remaining other categorical features, large airports tend to be more correlated with delays along with overcast and cloudy days, as seen by how the HourlyCloudCoverage of `OVC` corresponds with an overcast day and an HourlyCloudLayerAmount of `8` suggests that eight eighths of the sky was covered by clouds during that hour. These observations also align with our intuition on how airline departure delays are impacted and we will utilize these remaining features to see if they perform similarly or better than if we were to keep our whole feature space in later sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffc02c09-52f9-497c-93e7-9ba99627ec0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "Although Lasso regularization did not improve our results on the 12M dataset, we will now explore Principal Components Analysis (PCA) as an alternative dimensionality reduction technique. PCA works by identifying a lower-dimensional feature space where each new dimension (or principal component) is orthogonal to the others and captures the maximum variance of the original data, thereby preserving as much information as possible. In this experiment, we will use 110 principal components, effectively reducing our feature space by half. The [visualization](https://builtin.com/data-science/step-step-explanation-principal-component-analysis) below illustrates the PCA process in a two-dimensional space, where the red lines represent the projection of the original data onto the new principal component:\n",
    "\n",
    "![PCA demonstration](https://i.imgur.com/TSSllSF.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f86de4e5-7fc4-41d9-8d71-fea22f0421ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "In Phase 2, we built a custom blocked cross‑validation routine that respects the time order of our flight data by splitting the first three quarters of 2015 into non‑overlapping blocks with optional margins to prevent leakage. Within each fold, we fit a RobustScaler on the training subset and applied it to the test subset. Our `CustomCrossValidator` then performed a parallel grid search over all hyperparameter combinations, aggregated accuracy, F1, precision, and recall across folds, and selected the top-performing configuration. Finally, we retrained the model on the full three‑quarter dataset using these hyperparameters and evaluated it on the held‑out fourth quarter.\n",
    "\n",
    "In Phase 3, we extended our cross-validation to cover all four years (2015–2018) by sampling ~500,000 records from each year to create four balanced folds. This approach captures seasonal and annual variability while keeping computation manageable.\n",
    "\n",
    "![Blocked Cross-Validation](https://i.imgur.com/FVqCvaE.png)  \n",
    "\n",
    "### Grid vs Random vs Bayesian Optimization  \n",
    "\n",
    "To identify the most effective and efficient tuning strategy, we ran an experiment comparing grid search, random search, and Bayesian optimization on a simple logistic regression model using the 2015 Q1 three‑month subset. For each method, we measured hyperparameter quality via average F1, accuracy, precision, and recall over our custom blocked cross‑validation folds, and we tracked the total runtime.\n",
    "\n",
    "As part of our experiment, we first applied grid search by defining a 3×3 grid of hyperparameter values, resulting in nine total combinations. Because our `CustomCrossValidator` can train all combinations in parallel across folds, it completed all nine evaluations in about 16 minutes.  We then tested random search by sampling nine parameter sets randomly from the same hyperparameter ranges. When we ran it for the same number of trials, it also took roughly 16 minutes. However, random search does not guarantee that every combination is explored, which might lead to missing out on the optimal settings if the search space isn’t well-defined.\n",
    "\n",
    "Bayesian optimization with the Optuna library builds a probabilistic model of the hyperparameter landscape and chooses each new trial based on the outcomes of previous trials. We ran 50 sequential trials; each trial required about 10 minutes, since it must wait for the previous trial’s results before proposing the next set of parameters. Although each trial tended to yield slightly better metric scores, the total runtime was substantially longer than for grid or random search.\n",
    "\n",
    "In our project, we opted to continue using Grid Search because of its simplicity and efficiency. It enables us to leverage parallel processing, making it a more practical choice compared to the slower Bayesian Search, despite Bayesian Search’s potential for yielding slightly better metric scores in a sequential, trial-by-trial manner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e7f3c00-df57-4f16-9577-7a1472ab05f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Leakage\n",
    "\n",
    "Data leakage occurs when information that would not be available at prediction time is inadvertently included in the training data, leading to overly optimistic performance estimates. For example, using a flight’s actual arrival time to predict whether it will be delayed introduces future information that would not be known at the time of scheduling.\n",
    "\n",
    "In our pipeline, we carefully avoid data leakage by applying proper preprocessing steps before any model training, whether during cross-validation or when training on the full 2015–2018 dataset. The following safeguards are implemented during cross-validation and are consistently applied when training on the full dataset:\n",
    "\n",
    "- **Excluding future or post‑flight variables**  \n",
    "  We drop any columns that record events after takeoff (such as `wheels_off`, `wheels_on`), so no information from later in the flight can influence model fitting or validation.\n",
    "\n",
    "- **Excluding the target variable**  \n",
    "  We never include `DEP_DELAY` among the input features, eliminating any direct leakage of the flight’s actual delay into the predictors.\n",
    "\n",
    "- **Blocked Cross‑Validation instead of Rolling**  \n",
    "  Unlike rolling cross-validation, which reuses past data and can cause overlap between training and validation sets, blocked cross-validation uses non-overlapping, time-ordered folds. Each training set strictly precedes its validation set, preventing future data from leaking into the model during training.\n",
    "\n",
    "- **Robust scaling within each fold**  \n",
    "  For numeric features like `DISTANCE`, `ELEVATION`, or hourly wind speed, we fit a `RobustScaler` only on each fold’s training subset, then apply those parameters to the test subsets. This keeps quantile estimates blind to future data.\n",
    "\n",
    "- **Scaling airport PageRank**  \n",
    "  We compute PageRank on a directed, weighted graph of origin→destination flights using only training data in each fold. The resulting scores are then transformed on the test fold without refitting, preventing leakage of network connectivity.\n",
    "\n",
    "- **Categorical target encoding on high‑cardinality features**  \n",
    "  For each fold, we compute the average `DEP_DELAY` for every category (such as `TAIL_NUM` or `STATION`) using only the training subset. We then perform a left join to merge the calculated means back onto the train and test splits. Any category not encountered during training will result in a null value. To handle these nulls, we replace them with the overall mean `DEP_DELAY` of that fold’s training data. Once every record has a numeric `<feature>_mean_delay` value, we include those columns in our `float_vector` and apply the same fold‑specific `RobustScaler`. This ensures that unseen categories do not introduce leakage and that the encoded features remain on the same scale as the rest of our inputs.\n",
    "\n",
    "- **Resampling on training data only**  \n",
    "  To address class imbalance, our custom cross‑validation class can apply upsampling, downsampling, or SMOTE strictly on each fold’s training portion. Test splits remain untouched, ensuring they remain a truly unseen evaluation set.\n",
    "\n",
    "By partitioning data into non‑overlapping, time‑ordered folds, fitting each transformation and resampling step only on the training splits, and dropping any features that reflect future events, our pipeline follows best practices to prevent leakage and avoid cardinal sins in model evaluation. We make sure to follow these same steps when training on the full 2015–2018 dataset as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "615a6a48-a39c-4421-b9f9-0b2a272cb37f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Modeling Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3812bdb3-bfd7-44ee-b449-72e2a1a953f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Pipeline Implementation Flow Chart\n",
    "\n",
    "![ML Pipeline Implementation Structure](https://i.imgur.com/3FF1OYx.png)\n",
    "\n",
    "[Link to ML pipeline code](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/4472038552166031?o=4021782157704243)\n",
    "\n",
    "In Phase 2 we had separate pipelines for each model because the models were hard-coded into the pipeline. In Phase 3, we were able to abstract the models out of the pipeline so that we could simply pass the model of choice into the pipeline method, which is why the modeling pipelines are represented by a single flowchart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a2360bc-0f26-4b72-922b-633c9d97b906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Input Feature Families\n",
    "This is the final set of features we utilized for training and testing our classification models. As described in the LASSO Feature Selection section (carried over from Phase 2), some features were further removed for specific tests using LASSO to help select most relevant features only in Phase 2. In Phase 3, we used the full raw input feature space shown in the below table, which excludes certain raw columns that we dropped and using additional features derived from these input columns as described in the EDA and Feature Engineering sections.\n",
    "\n",
    "| Feature Family | Features Included | Count |\n",
    "|----------------|-------------------|--------|\n",
    "| **Temporal** | `DAY_OF_MONTH`, `DAY_OF_WEEK`, `MONTH`, `QUARTER`, `sched_depart_hour_UTC`, `sched_depart_minute_UTC`,<br>`four_hours_prior_depart_hour_UTC`, `four_hours_prior_depart_minute_UTC`,<br>`two_hours_prior_depart_hour_UTC`, `two_hours_prior_depart_minute_UTC`,<br>`station_hour_UTC`, `station_minute_UTC` | **12** |\n",
    "| **Categorical** | `OP_UNIQUE_CARRIER`, `OP_CARRIER_AIRLINE_ID`, `TAIL_NUM`, `OP_CARRIER_FL_NUM`,<br>`ORIGIN_AIRPORT_ID`, `ORIGIN_STATE_ABR`, `DEST_AIRPORT_ID`, `DEST_STATE_ABR`,<br>`origin_station_id`, `origin_type`, `dest_station_id`, `dest_type`,<br>`STATION`, `HourlyCloudCoverage`, `HourlyCloudLayerAmount` | **15** |\n",
    "| **Numeric** | `DEP_DELAY`, `DISTANCE`, `DISTANCE_GROUP`, `ELEVATION`,<br>`HourlyAltimeterSetting`, `HourlyDewPointTemperature`, `HourlyDryBulbTemperature`,<br>`HourlyPrecipitation`, `HourlyRelativeHumidity`, `HourlyVisibility`, `HourlyWindDirection`,<br>`HourlyWindSpeed`, `HourlyCloudBaseHeight`, `HourlySeaLevelPressure`, `HourlyStationPressure`,<br>`HourlyWetBulbTemperature`, `LATITUDE`, `LONGITUDE`, `origin_station_dis`, `dest_station_dis` | **20** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dd34bc5-5d71-4ceb-b0b1-38e3a5d4d593",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loss Functions\n",
    "Note: Some unicode symbols used with LaTeX equations for formatting clarity\n",
    "\n",
    "#### Logistic Regression with ElasticNet Regularization\n",
    "\n",
    "The loss function used for logistic regression is binary cross-entropy (log loss) with ElasticNet regularization. The formula is:\n",
    "\n",
    "$$\n",
    "L(w) = -\\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] + \\lambda \\left( \\alpha \\|w\\|_1 + (1 - \\alpha) \\|w\\|_2^2 \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `ŷᵢ` is the predicted probability from the sigmoid function\n",
    "- `λ` is the regularization strength (`regParam`)\n",
    "- `α` is the ElasticNet mixing parameter (`elasticNetParam`)\n",
    "  - α = 1 → LASSO (L1 only)\n",
    "  - α = 0 → Ridge (L2 only)\n",
    "\n",
    "---\n",
    "\n",
    "#### Random Forest / Decision Tree Classifier (Gini Impurity)\n",
    "\n",
    "Random Forests do not use a global loss function like logistic regression. Each decision tree is trained to minimize **Gini impurity** at each split:\n",
    "\n",
    "$$\n",
    "G(t) = \\sum_{c=1}^C p(c \\mid t) \\left(1 - p(c \\mid t)\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `G(t)` is the Gini impurity at node `t`\n",
    "- `p(c | t)` is the proportion of samples of class `c` at node `t`\n",
    "\n",
    "There was no L1 or L2 regularization. Model complexity was controlled through the following hyperparameters:\n",
    "- Number of trees\n",
    "- Maximum depth\n",
    "- Feature subsampling\n",
    "\n",
    "---\n",
    "#### Multilayer Perceptron Classifier (MLP)\n",
    "\n",
    "PySpark’s Multilayer Perceptron Classifier is a feedforward neural network trained using backpropagation and **cross-entropy loss**. It is optimized with either **L-BFGS** or **mini-batch gradient descent**.\n",
    "\n",
    "The loss function for a multi-class classification task is the **categorical cross-entropy**:\n",
    "\n",
    "$$\n",
    "L(W) = -\\sum_{i=1}^n \\sum_{k=1}^K y_{ik} \\log(\\hat{y}_{ik})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `yᵢₖ` = 1 if sample *i* belongs to class *k*, otherwise 0 (one-hot encoded ground truth)\n",
    "- `ŷᵢₖ` = predicted probability of class *k* for sample *i*\n",
    "- `K` = number of classes\n",
    "- `W` = all weights in the network\n",
    "\n",
    "**Key Notes:**\n",
    "- This loss is used for both binary and multi-class classification (binary is just \\( K = 2 \\))\n",
    "- There is **no explicit regularization** term like in logistic regression. In our experiments, overfitting was controlled by the following hyperparameters that we considered:\n",
    "  - Network architecture (number of layers and units)\n",
    "  - Maximum number of iterations\n",
    "  - Step size (learning rate)\n",
    "\n",
    "The output layer uses the **softmax** activation function to produce class probabilities:\n",
    "$$\n",
    "\\hat{y}_{ik} = \\frac{e^{z_{ik}}}{\\sum_{j=1}^{K} e^{z_{ij}}}\n",
    "$$\n",
    "\n",
    "- `zᵢₖ` = raw score (logit) for class *k* before softmax activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c12bc7c5-86aa-400b-9e40-f8bc7473ad7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cluster Configuration\n",
    "* Workers: 5-10\n",
    "* Worker Specs: 160-320 GB memory, 40-80 total cores (across all workers)\n",
    "* Driver: 128 GB memory, 32 cores\n",
    "* Runtime: Databricks Runtime ML 15.4 (Scala 2.12, CPU)\n",
    "* Photon Enabled: Yes\n",
    "* VM Types: m6g.xlarge, m6gd.2xlarge\n",
    "\n",
    "### Pipeline Wall Times\n",
    "\n",
    "**Full Pipeline Runtimes with Best-performing Hyperparameters on 5-year dataset (data cleaning, processing, feature engineering, cross-validation, final model training, final model evaluation)**\n",
    "* Decision Tree (All features, maxDepth = 3): ~20 hours\n",
    "* Random Forest (All features, numTrees=20, maxDepth=15): ~26 hours\n",
    "* Logistic Regression (All features, regParam=0.001, elasticNetParam=0.0): ~17 hours\n",
    "* MLP (All features, layers=[189, 100, 50, 25, 10, 3]): ~30 hours\n",
    "\n",
    "**Model-only Runtimes (final model training, final model evaluation, 80/20 train-test split)**\n",
    "* Decision Tree (All features, maxDepth = 3): train: ~18 minutes, test: ~16 minutes\n",
    "* Random Forest (All features, numTrees=20, maxDepth=15): train: ~5 hours 39 minutes, test: ~1 hour, 29 minutes\n",
    "* Logistic Regression: train: ~24 min, test: ~13 minutes\n",
    "* MLP (All features, layers=[189, 100, 50, 25, 10, 3]): train: ~50 minutes, test: ~30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33cc9a62-0aac-4318-870a-070c50a8b166",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Experiments Conducted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73c98224-738f-4ef9-a3c1-5560f2f24ec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Metrics Used\n",
    "\n",
    "In our flight delay prediction project (predicting early, on-time, and delayed flights), we use the following metrics:\n",
    "\n",
    "- **Accuracy**  \n",
    "  Accuracy measures the proportion of correct predictions over all predictions. In a multiclass setting, it is defined as:  \n",
    "  $$\n",
    "  Accuracy = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\n",
    "  $$\n",
    "\n",
    "- **Weighted Precision**  \n",
    "  Weighted Precision calculates the precision for each class and averages them, weighted by the number of instances in each class. For each class \\(i\\):  \n",
    "  $$\n",
    "  Precision_i = \\frac{TP_i}{TP_i + FP_i}\n",
    "  $$  \n",
    "  The overall weighted precision is:  \n",
    "  $$\n",
    "  Weighted\\ Precision = \\frac{\\sum_{i=1}^{C} n_i \\cdot Precision_i}{\\sum_{i=1}^{C} n_i}\n",
    "  $$\n",
    "  where \\(n_i\\) is the number of instances in class \\(i\\) and \\(C\\) is the total number of classes.\n",
    "\n",
    "- **Weighted Recall**  \n",
    "  Weighted Recall computes the recall (sensitivity) for each class and averages them with weights based on class frequencies. For each class \\(i\\):  \n",
    "  $$\n",
    "  Recall_i = \\frac{TP_i}{TP_i + FN_i}\n",
    "  $$  \n",
    "  The overall weighted recall is:  \n",
    "  $$\n",
    "  Weighted\\ Recall = \\frac{\\sum_{i=1}^{C} n_i \\cdot Recall_i}{\\sum_{i=1}^{C} n_i}\n",
    "  $$\n",
    "\n",
    "- **F1 Score**  \n",
    "  The F1 Score is the harmonic mean of precision and recall. For each class \\(i\\):  \n",
    "  $$\n",
    "  F1_i = 2 \\times \\frac{Precision_i \\cdot Recall_i}{Precision_i + Recall_i}\n",
    "  $$  \n",
    "  The weighted F1 score is calculated as:  \n",
    "  $$\n",
    "  Weighted\\ F1 = \\frac{\\sum_{i=1}^{C} n_i \\cdot F1_i}{\\sum_{i=1}^{C} n_i}\n",
    "  $$\n",
    "\n",
    "We focus on the F1 Score during grid search cross-validation because it balances precision and recall, ensuring our model performs reliably across all classes (early, on-time, delayed).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa5d38f3-99a9-4a03-b24c-8539ca72e036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Phase 2 Experiments\n",
    "Number of experiments:\n",
    "* Decision Tree: 4\n",
    "* Logistic Regression: 4\n",
    "* Random Forest: 4\n",
    "* Random Forest with LASSO features: 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba6348ac-d5f1-44ea-888a-7ebabda6276a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Best-performing Experiments:\n",
    "\n",
    "| Model                               | Input Feature Families Used         | Train F1 | Train Precision | Train Recall | Train Accuracy | Test F1 | Test Precision | Test Recall | Test Accuracy |\n",
    "|-------------------------------------|-------------------------------------|----------|------------------|--------------|----------------|---------|----------------|-------------|----------------|\n",
    "| Decision Tree (Baseline)            | Temporal, Categorical, Numeric      | 0.470    | 0.445            | 0.570        | 0.570          | 0.505   | 0.460          | 0.586       | 0.586          |\n",
    "| **Logistic Regression**             | Temporal, Categorical, Numeric       | **0.520**| **0.525**        | **0.582**    | **0.582**      | **0.537**| **0.523**      | **0.561**   | **0.561**      |\n",
    "| Random Forest                       | Temporal, Categorical, Numeric | 0.4240   | 0.4492           | 0.5762       | 0.5762         | 0.4461  | 0.3562         | 0.5968      | 0.5968         |\n",
    "| Random Forest (with Lasso Features) | Temporal, Categorical, Numeric (LASSO-selected subset)  | 0.4490   | 0.4935           | 0.5715       | 0.5715         | 0.4597  | 0.4297         | 0.5958      | 0.5958         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78109d11-4f93-423e-bd98-bf14adf41cba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Phase 3 Experiments\n",
    "\n",
    "As explored in the EDA section, a big challenge for our model training comes from the class imbalance in our dataset. In an attempt to improve model performance we heavily focused on using various resampling techniques that will allow us to balance  our classes and enable our model to have equivalent training examples across all classes. Additionally, as requested by our client, we have explored deep learning models through the experimentation of various distinct Multilayer Perceptron model architectures. \n",
    "\n",
    "In order to identify the best Multilayer Perceptron architectures we first conducted small scale experimentations. This experimentation consisted of training and evaluating multiple different architectures with a reduced dataset based on the OTPW 12-month dataset from Phase 2. However, given our goal of exploring resampling techniques we decided to over-sample our 12-month dataset to balance out our class distributions. The over-sampling technique we used was a simple duplication method where the minority classes are scaled up to match the majority class counts. However, before performing the resampling we decided to utilize a sequential train and test split that consisted of designating the first 80% of 2015 for training and the last 20% of 2015 for testing. Once our split was complete we proceeded with our resampling method on the training dataset and ended up with all classes having roughly 1.8 million rows. \n",
    "\n",
    "With our small scale experimentation dataset set up we proceeded to test distinct MLP architectures to identify potential candidates for our 60 month experimentation. The first experiment we conducted was on an MLP architecture consisting of 6 layers, an input layer with 189 features, a hidden layer with 100 neurons, a second hidden layer with 50 neurons, a third hidden layer with 25 neurons, a fourth hidden layer with 10 neurons, and an output layer for 3 classes. The initial results are listed below: \n",
    "\n",
    "| Model                               | Input Feature Families Used         | Train F1 | Train Precision | Train Recall | Train Accuracy | Test F1 | Test Precision | Test Recall | Test Accuracy |\n",
    "|-------------------------------------|-------------------------------------|----------|------------------|--------------|----------------|---------|----------------|-------------|----------------|\n",
    "| MLP 1 (6 layer)           | Temporal, Categorical, Numeric      |  0.414   | 0.430            | 0.443        | 0.443        | 0.484   | 0.504          | 0.489       | 0.489          |\n",
    "\n",
    "![Uncalibrated_MLP_1](https://i.imgur.com/cB1FvCv.png)\n",
    "![MLP_1_Training_History](https://i.imgur.com/WcDuzS8.png)\n",
    "\n",
    "These initial results displayed that this MLP architecture is very capable of predicting the early class, but severely struggles with predicting the on-time or delayed classes. We can also see that it is heavily predicting the delayed class when the actual class was early and in terms of training we can see that the loss began to converge around the 1.05 mark. However, the prediction results were not correct metrics to evaluate our model given that we had not yet performed a recalibration of our results. Given our over-sampling technique, a model recalibration enables us to accurately reflect the underlying class distribution that our model is unaware of at the time of training with the over-sampled dataset. To recalibrate our model we attempted a calibration method that consists of adjusting the predicted probabilities with the upsampling or downsampling rate factor. The formula and results are shown below, where _b_ is the original class ratio, _b'_ is the resampled class ratio, _p'_ is the model's class probability prediction, and _c_ is the corresponding class:\n",
    "\n",
    "$$\n",
    "w_c = \\frac{b}{1-b}\\cdot \\frac{1- b'}{b'}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_{adjusted}(c) = \\frac{p'}{p' + \\frac{1 - p'}{w_c}}\n",
    "$$\n",
    "\n",
    "\n",
    "| Model                               | Input Feature Families Used         | Train F1 | Train Precision | Train Recall | Train Accuracy | Test F1 | Test Precision | Test Recall | Test Accuracy |\n",
    "|-------------------------------------|-------------------------------------|----------|------------------|--------------|----------------|---------|----------------|-------------|----------------|\n",
    "| MLP 1 (6 layer)           | Temporal, Categorical, Numeric      |  0.4084 | 0.4336          | 0.4276       | 0.4276        | 0.5116   | 0.5003          | 0.5318       | 0.5318          |\n",
    "\n",
    "![Calibrated_MLP_1](https://i.imgur.com/69fM78r.png)\n",
    "\n",
    "Now with our recalibrated model, we can see that we are still struggling with predicting the delayed and on-time classes given that our model predicts early most of the time. However, we can also see that with our recalibration method the model has a class prediction distribution closer to what we would expect given the original class distribution. Next, we decided to experiment with a more complex model.\n",
    "\n",
    "In addition to the 6 layer MLP architecture, we explored a 7 layer architecture which now consists of a hidden layer with 150 neurons, a second hidden layer with 75 neurons, a third hidden layer with 50 neurons, a 4th hidden layer with 25 neurons, and a 5th hidden layer with 10 neurons. The calibrated results are shown below:\n",
    "\n",
    "| Model                               | Input Feature Families Used         | Train F1 | Train Precision | Train Recall | Train Accuracy | Test F1 | Test Precision | Test Recall | Test Accuracy |\n",
    "|-------------------------------------|-------------------------------------|----------|------------------|--------------|----------------|---------|----------------|-------------|----------------|\n",
    "| MLP 2 (7 layer)           | Temporal, Categorical, Numeric      |0.4058   |   0.4392       | 0.4251   |0.4251      | 0.5121  |0.5038     |0.5351      | 0.5351       |\n",
    "\n",
    "![Calibrated_MLP_2](https://i.imgur.com/3ZgdVZ8.png)\n",
    "![MLP_2_Training_History](https://i.imgur.com/jULabnL.png)\n",
    "\n",
    "One interesting thing to note is that the loss for this model hadn't converged yet in the 50 epoch training we conducted, indicating that perhaps this model needs more epochs to improve, but based on these results we can see that the added model complexity didn't improve our results. The test F1 score didn't change much and we still see that the model mostly predicts the early class, since it is not very capable at distinguishing the on-time or delayed classes. Hence, why we decided to test a less complex model. Perhaps a less complex model would be more capable of distinguishing our 3 classes. The architecture we tested was a 5 layer model with 3 hidden layers, the first having 150 neurons, the second having 75 neurons, and the third having 20 neurons. The calibrated results for this model are shown below:\n",
    "\n",
    "| Model                               | Input Feature Families Used         | Train F1 | Train Precision | Train Recall | Train Accuracy | Test F1 | Test Precision | Test Recall | Test Accuracy |\n",
    "|-------------------------------------|-------------------------------------|----------|------------------|--------------|----------------|---------|----------------|-------------|----------------|\n",
    "| MLP 3 (5 layer)           | Temporal, Categorical, Numeric      | 0.4312   | 0.4515           | 0.4445        | 0.4445      | 0.5221   | 0.5152         | 0.5337       | 0.5337         |\n",
    "\n",
    "![Calibrated_MLP_3](https://i.imgur.com/ffCIUAK.png)\n",
    "![MLP_2_Training_History](https://i.imgur.com/msPPTmH.png)\n",
    "\n",
    "In terms of training, we can see that this experiment is similar to the first where the loss converges around the 1.04 mark. Based on these results we can see that this model is a slight improvement over the previous models since it's test f1 score is about 1% higher. Most notably, this model was more capable of predicting the on-time and delayed classes since it has the highest count of correct predictions for both classes, 74,164 and 26,683 respectively. This seems to suggest that this model could provide us with the best results when working with the 60 month dataset.\n",
    "\n",
    "Based on all of these results we decided it would be worthwhile to test out all of these architectures with the 60 month dataset. Our intuition tells us that the more complex models could perform better with larger amounts of training data, but with these small scale tests we also noticed that the best performing model was a simpler 5 layer model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "889d77d3-feab-4277-9ce5-076611e7ec35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Best-performing Small-Scale Experiments:\n",
    "\n",
    "| Model                               | Input Feature Families Used         | Train F1 | Train Precision | Train Recall | Train Accuracy | Test F1 | Test Precision | Test Recall | Test Accuracy |\n",
    "|-------------------------------------|-------------------------------------|----------|------------------|--------------|----------------|---------|----------------|-------------|----------------|\n",
    "|  MLP 1 (6 layer)           | Temporal, Categorical, Numeric      |  0.4084 | 0.4336          | 0.4276       | 0.4276        | 0.5116   | 0.5003          | 0.5318       | 0.5318           |\n",
    "|  MLP 2 (7 layer)           | Temporal, Categorical, Numeric      |0.4058   |   0.4392       | 0.4251   |0.4251      | 0.5121  |0.5038     |0.5351      | 0.5351           |\n",
    "| **MLP 3 (5 layer)**           | Temporal, Categorical, Numeric      | **0.4312**   | **0.4515**           | **0.4445**        | **0.4445**      | **0.5221**   | **0.5152**         | **0.5337**       | **0.5337**          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a171a621-e3ee-48b9-92fe-96c15e77648a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Phase 3 60 Month Experiments:\n",
    "* Logistic Regression: 2\n",
    "* Random Forest: 2\n",
    "* MLP 1: 4\n",
    "* MLP 2: 4\n",
    "* MLP 3: 4\n",
    "* MLP 1 with PCA: 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "800cef0a-fabf-44a9-b5af-712a3b849323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Results\n",
    "\n",
    "We evaluated the performance of deep learning models, particularly Multilayer Perceptrons (MLPs), alongside classic classifiers (Logistic Regression and Random Forest) for predicting flight delay categories (\"Early\", \"On-Time\", \"Delayed\"). All models were trained and evaluated using the 60-month dataset introduced in Phase 3, with significant emphasis on class imbalance mitigation using over-sampling, under-sampling, and probability calibration. Our training data consisted of records belonging to the 2015-2018 calendar years, and our blind test consisted of records from the 2019 calendar year. \n",
    "\n",
    "We measured model performance using accuracy, weighted F1, precision, and recall, as well as class-level precision and recall for our evaluation test metrics.\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "#### Oversampling Variant\n",
    "\n",
    "The Logistic Regression model trained with oversampling showed moderate performance. While the uncalibrated test F1 was 0.5416 with 0.5445 precision, calibration improved the class distribution representation, increasing F1 to 0.5391 and accuracy to 0.5945.\n",
    "![lr o](https://imgur.com/F0aQb6N.png)\n",
    "\n",
    "#### Undersampling Variant\n",
    "\n",
    "Undersampling led to slightly higher test F1 of 0.4927 uncalibrated and 0.4952 after calibration. Calibration improved accuracy to 0.5843.\n",
    "\n",
    "![lr u](https://imgur.com/MBG3uYL.png)\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "#### Oversampling Variant\n",
    "\n",
    "The Random Forest model benefited from the resampled training data and showed improved generalization. The uncalibrated F1 score on the test set was 0.5439, and calibration further raised the F1 to 0.5244 and accuracy to 0.5999.\n",
    "\n",
    "![rf o](https://imgur.com/Ny7DSAo.png)\n",
    "\n",
    "#### Undersampling Variant\n",
    "\n",
    "This model had more balanced performance across classes. After calibration, the test F1 reached 0.4899 with accuracy 0.5956.\n",
    "\n",
    "![rf u](https://imgur.com/wIIo6Og.png)\n",
    "\n",
    "### MLP models\n",
    "#### MLP Model 1 – Oversampling\n",
    "\n",
    "MLP 1 (6-layer) performed better after calibration, reaching F1 = 0.5460 and accuracy = 0.5869.\n",
    "\n",
    "![mlp1 o](https://imgur.com/j7QBPMY.png)\n",
    "\n",
    "#### MLP Model 1 – Undersampling\n",
    "\n",
    "This variant achieved F1 = 0.5180, a notable jump post-calibration. Accuracy rose to 0.5851.\n",
    "\n",
    "![mlp1 u](https://imgur.com/aDMX6ij.png)\n",
    "\n",
    "#### MLP Model 2 – Oversampling\n",
    "\n",
    "Although this 7-layer model achieved modest F1 performance initially, calibration improved it to 0.5157 with accuracy = 0.5847.\n",
    "\n",
    "![mlp2 o](https://imgur.com/oUIkwak.png)\n",
    "\n",
    "#### MLP Model 2 – Undersampling\n",
    "\n",
    "Post-calibration, MLP 2 reached F1 = 0.5017, showing balanced gains with class-wise precision improving significantly.\n",
    "\n",
    "![mlp2 u](https://imgur.com/3Fllbeq.png)\n",
    "\n",
    "#### MLP Model 3 – Oversampling\n",
    "\n",
    "This 5-layer model delivered the best F1 performance of all MLPs at 0.5463 (calibrated), with a strong accuracy = 0.5758.\n",
    "\n",
    "![mlp3 o](https://imgur.com/md6uvDD.png)\n",
    "\n",
    "#### MLP Model 3 – Undersampling\n",
    "\n",
    "The undersampled MLP 3 achieved an F1 of 0.5167, and like others, calibration improved class-level prediction consistency.\n",
    "\n",
    "![mlp3 u](https://imgur.com/8huMXSQ.png)\n",
    "\n",
    "#### MLP with PCA\n",
    "\n",
    "PCA did not significantly boost MLP performance. The calibrated version reached F1 = 0.4736, with accuracy = 0.5858, lower than other MLPs without dimensionality reduction.\n",
    "\n",
    "![mlp o pca](https://imgur.com/0gAWpC6.png)\n",
    "\n",
    "### Discussion of Results\n",
    "\n",
    "The results highlight the need to address class imbalance when building multiclass classifiers for real-world problems like flight delay prediction, where early departures dominate. Oversampling and probability calibration consistently helped all models, especially in improving performance for the underrepresented On-Time and Delayed classes.\n",
    "\n",
    "Logistic Regression performed well for a linear model. With oversampling and calibration, it reached 59.45% accuracy and an F1 of 0.5391. Still, it struggled to recall the On-Time class, showing limited ability to handle class-level skew.\n",
    "\n",
    "Random Forest also improved with oversampling. It showed strong precision and recall for Early flights but had trouble separating On-Time and Delayed categories. Calibration helped a bit with class 2 recall but didn’t fully resolve the issue.\n",
    "\n",
    "MLPs captured deeper patterns and delivered the most balanced outcomes. MLP Model 3 with oversampling had the top F1 score at 0.5463. It also showed more consistent performance across all classes. MLP Model 2 did not perform as well, possibly due to an inefficient architecture or limited convergence.\n",
    "\n",
    "PCA did not improve MLP performance. The high-dimensional features from our engineering process seemed to contain valuable signals that PCA could not retain.\n",
    "\n",
    "Overall, no model perfectly balanced all class metrics. MLP 3 with oversampling offered the best trade-off between accuracy and fairness. Future improvements might explore model ensembling, threshold tuning, or better sampling-calibration methods to further reduce bias and enhance prediction stability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b771606-fbd7-4668-a6a6-0778362b5379",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40b5118e-f860-4917-8ee6-41ca16355c9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This report presented a comprehensive and scalable approach to flight delay prediction which we framed as a multiclass classification task, distinguishing between early, on-time, and delayed departures. Our analysis underscored the challenges of class imbalance, with the majority class (“Early”) comprising over half of the dataset. Despite many iterations of experiments involving class rebalancing, resampling strategies, and the inclusion of temporal and graph-derived airport features, model performance remained skewed toward the majority class.\n",
    "\n",
    "Among the models evaluated, a six-layer MLP with oversampling was the best-performing architecture, achieving a test F1 score of 56.2%. However, precision and recall for the minority “On-Time” and “Delayed” classes remained limited, highlighting the difficulty of distinguishing borderline departure cases using historical and environmental predictors alone. These findings suggest that while our current machine learning model can provide valuable early signals for flight delays, further increased in practical performance may require incorporation of real-time data streams (e.g., live weather radar, maintenance logs) and dynamic sequence modeling.\n",
    "\n",
    "A major contribution of this project is the development of a modular and data leakage-resistant machine learning pipeline capable of operating efficiently at scale on our distributed infrastructure. We built more than just a model—we built a flexible, distributed pipeline with a modular design that helped us to quickly and iteratively tackle issues. That infrastructure allowed us to iterate quickly on multiple hyperparameter tuning and time-fold cross validation strategies to scale efficiently and maintain efficacy in our results. Through the hyperparameter tuning experiments we have learned important tradeoffs between runtime efficiency and predictive performance, ultimately observing that grid search yielded practical advantages under parallelized settings. Additionally, we've carefully been able to identify possible points of data leakage and address them within our time-fold cross validation to ensure information from the test set or future data did not inadvertently influence our model results.\n",
    "\n",
    "In future iterations of this project, we'd most likely investigate more complex neural architectures and expand beyond static tabular features to include dynamic time series signals. Ultimately, despite prediction challenges for on-time and delayed classes, even moderately accurate early classifications from this model still have the potential to improve crew scheduling, gate planning, and customer communication. This results of the project we have highlighted in this report serve as a promising start to solving the complex and operationally significant flight delay challenge that plagues both travelers and airlines today.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbcf5a41-652c-4c60-9fdc-3cb9bb018394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## References\n",
    "\n",
    "**Phase 2**\n",
    "- [Phase 2 General EDA and Preprocessing](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/3612473680112384?o=4021782157704243#command/7426226296521711)\n",
    "- [Phase 2 EDA (Sebastian)](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/905158478260765?o=4021782157704243#command/6227772604401223)\n",
    "- [\"Creating a Custom Cross-Validation Function in PySpark\" by Timothy Lin (2018)](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/905158478260765?o=4021782157704243#command/6227772604401223)\n",
    "- [Phase 2 Logistic Regression Model Pipeline Code](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/2354472643154985?o=4021782157704243)\n",
    "- [Phase 2 Random Forest Classifier Model Pipeline Code](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/3581936500212876?o=4021782157704243)\n",
    "- [Phase 2 Decision Tree Classifier Model Pipeline Code](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/3581936500212903?o=4021782157704243)\n",
    "- [Phase 2 Presentation Slides](https://docs.google.com/presentation/d/1swzeNylBPO9uYtd4cWX26Oc7LCIjKtm_gcFPYL_Zyj8/edit?usp=sharing)\n",
    "\n",
    "**Phase 3**\n",
    "- [Phase 3 - Data Preprocessing (Sebastian)](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/1017866335725575?o=4021782157704243)\n",
    "- [Phase 3 - OverSampling and Experimentation](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/4211782756012278?o=4021782157704243)\n",
    "- [Phase 3 - 60 Month CV and Training](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/1536813829685184?o=4021782157704243)\n",
    "- [Phase 3 - Model Calibrations (with new formula)](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/4472038552165774?o=4021782157704243)\n",
    "    - [\"Recalibration after Negative Sampling\" by Johannes Haupt](https://johaupt.github.io/blog/downsampling_recalibration.html)\n",
    "- [Phase 3 - End-to-end Model Pipeline Code](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/4472038552166031?o=4021782157704243)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Phase 3 Report",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}